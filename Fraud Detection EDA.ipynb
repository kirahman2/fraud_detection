{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_train = train_transaction.merge(train_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.head() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.head() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.info(); # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.info(); # KEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# train_transaction_sum = train_transaction.duplicated().sum()\n",
    "# train_identity_sum = train_identity.duplicated().sum()\n",
    "# columns = train_transaction.columns\n",
    "# print('Train transaction duplicates: {}\\nTrain identity duplicates: {} \\n'.format(train_transaction_sum, train_identity_sum))\n",
    "# # print('Train feature columns:\\n', list(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 220)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many transactions are is the dataset?\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96501\n",
       "1    0.03499\n",
       "Name: isFraud, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the fraud rate of the data set?\n",
    "fraud_rate = df_train.isFraud.value_counts() / 590540\n",
    "fraud_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>...</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V316</th>\n",
       "      <th>V317</th>\n",
       "      <th>V318</th>\n",
       "      <th>V319</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5.905400e+05</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>5.905400e+05</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "      <td>590540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.282270e+06</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>7.372311e+06</td>\n",
       "      <td>135.027176</td>\n",
       "      <td>165.493264</td>\n",
       "      <td>65.854435</td>\n",
       "      <td>14.092458</td>\n",
       "      <td>15.269734</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>4.092185</td>\n",
       "      <td>...</td>\n",
       "      <td>39.173114</td>\n",
       "      <td>21.305592</td>\n",
       "      <td>43.226087</td>\n",
       "      <td>26.749372</td>\n",
       "      <td>109.816313</td>\n",
       "      <td>247.601710</td>\n",
       "      <td>162.150103</td>\n",
       "      <td>18.372102</td>\n",
       "      <td>42.072278</td>\n",
       "      <td>28.326009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.704744e+05</td>\n",
       "      <td>0.183755</td>\n",
       "      <td>4.617224e+06</td>\n",
       "      <td>239.162522</td>\n",
       "      <td>93.696289</td>\n",
       "      <td>3.546653</td>\n",
       "      <td>133.569018</td>\n",
       "      <td>154.668899</td>\n",
       "      <td>0.150536</td>\n",
       "      <td>68.848459</td>\n",
       "      <td>...</td>\n",
       "      <td>172.126681</td>\n",
       "      <td>95.804974</td>\n",
       "      <td>173.443984</td>\n",
       "      <td>116.734202</td>\n",
       "      <td>2270.010192</td>\n",
       "      <td>3980.002546</td>\n",
       "      <td>2793.315350</td>\n",
       "      <td>332.301482</td>\n",
       "      <td>473.494534</td>\n",
       "      <td>382.049311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>2.987000e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.640000e+04</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>3.134635e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.027058e+06</td>\n",
       "      <td>43.321000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3.282270e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.306528e+06</td>\n",
       "      <td>68.769000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>3.429904e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.124662e+07</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>3.577539e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.581113e+07</td>\n",
       "      <td>31937.391000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>4685.000000</td>\n",
       "      <td>5691.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2253.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>55125.000000</td>\n",
       "      <td>4817.470215</td>\n",
       "      <td>7519.870117</td>\n",
       "      <td>4817.470215</td>\n",
       "      <td>93736.000000</td>\n",
       "      <td>134021.000000</td>\n",
       "      <td>98476.000000</td>\n",
       "      <td>104060.000000</td>\n",
       "      <td>104060.000000</td>\n",
       "      <td>104060.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TransactionID        isFraud  TransactionDT  TransactionAmt  \\\n",
       "count   5.905400e+05  590540.000000   5.905400e+05   590540.000000   \n",
       "mean    3.282270e+06       0.034990   7.372311e+06      135.027176   \n",
       "std     1.704744e+05       0.183755   4.617224e+06      239.162522   \n",
       "min     2.987000e+06       0.000000   8.640000e+04        0.251000   \n",
       "25%     3.134635e+06       0.000000   3.027058e+06       43.321000   \n",
       "50%     3.282270e+06       0.000000   7.306528e+06       68.769000   \n",
       "75%     3.429904e+06       0.000000   1.124662e+07      125.000000   \n",
       "max     3.577539e+06       1.000000   1.581113e+07    31937.391000   \n",
       "\n",
       "               addr1          addr2             C1             C2  \\\n",
       "count  590540.000000  590540.000000  590540.000000  590540.000000   \n",
       "mean      165.493264      65.854435      14.092458      15.269734   \n",
       "std        93.696289       3.546653     133.569018     154.668899   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%        79.000000      65.000000       1.000000       1.000000   \n",
       "50%       151.000000      65.000000       1.000000       1.000000   \n",
       "75%       250.000000      65.000000       3.000000       3.000000   \n",
       "max       332.000000      74.000000    4685.000000    5691.000000   \n",
       "\n",
       "                  C3             C4  ...           V312           V313  \\\n",
       "count  590540.000000  590540.000000  ...  590540.000000  590540.000000   \n",
       "mean        0.005644       4.092185  ...      39.173114      21.305592   \n",
       "std         0.150536      68.848459  ...     172.126681      95.804974   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "max        26.000000    2253.000000  ...   55125.000000    4817.470215   \n",
       "\n",
       "                V314           V315           V316           V317  \\\n",
       "count  590540.000000  590540.000000  590540.000000  590540.000000   \n",
       "mean       43.226087      26.749372     109.816313     247.601710   \n",
       "std       173.443984     116.734202    2270.010192    3980.002546   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max      7519.870117    4817.470215   93736.000000  134021.000000   \n",
       "\n",
       "                V318           V319           V320           V321  \n",
       "count  590540.000000  590540.000000  590540.000000  590540.000000  \n",
       "mean      162.150103      18.372102      42.072278      28.326009  \n",
       "std      2793.315350     332.301482     473.494534     382.049311  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       0.000000  \n",
       "max     98476.000000  104060.000000  104060.000000  104060.000000  \n",
       "\n",
       "[8 rows x 207 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the statistical overview of the data set?\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>...</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V316</th>\n",
       "      <th>V317</th>\n",
       "      <th>V318</th>\n",
       "      <th>V319</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isFraud</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.281810e+06</td>\n",
       "      <td>7.360791e+06</td>\n",
       "      <td>134.511665</td>\n",
       "      <td>163.642265</td>\n",
       "      <td>65.776543</td>\n",
       "      <td>13.314952</td>\n",
       "      <td>14.173283</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>3.693878</td>\n",
       "      <td>5.722537</td>\n",
       "      <td>...</td>\n",
       "      <td>37.941626</td>\n",
       "      <td>20.549558</td>\n",
       "      <td>41.955015</td>\n",
       "      <td>25.677123</td>\n",
       "      <td>111.096134</td>\n",
       "      <td>243.805877</td>\n",
       "      <td>161.620207</td>\n",
       "      <td>18.368309</td>\n",
       "      <td>41.625136</td>\n",
       "      <td>28.204085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.294952e+06</td>\n",
       "      <td>7.690033e+06</td>\n",
       "      <td>149.244779</td>\n",
       "      <td>216.543048</td>\n",
       "      <td>68.002662</td>\n",
       "      <td>35.535740</td>\n",
       "      <td>45.509413</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>15.077336</td>\n",
       "      <td>1.406717</td>\n",
       "      <td>...</td>\n",
       "      <td>73.137039</td>\n",
       "      <td>42.156677</td>\n",
       "      <td>78.281729</td>\n",
       "      <td>56.321550</td>\n",
       "      <td>74.519367</td>\n",
       "      <td>352.289202</td>\n",
       "      <td>176.764416</td>\n",
       "      <td>18.476726</td>\n",
       "      <td>54.404279</td>\n",
       "      <td>31.688605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionID  TransactionDT  TransactionAmt       addr1      addr2  \\\n",
       "isFraud                                                                        \n",
       "0         3.281810e+06   7.360791e+06      134.511665  163.642265  65.776543   \n",
       "1         3.294952e+06   7.690033e+06      149.244779  216.543048  68.002662   \n",
       "\n",
       "                C1         C2        C3         C4        C5  ...       V312  \\\n",
       "isFraud                                                       ...              \n",
       "0        13.314952  14.173283  0.005840   3.693878  5.722537  ...  37.941626   \n",
       "1        35.535740  45.509413  0.000242  15.077336  1.406717  ...  73.137039   \n",
       "\n",
       "              V313       V314       V315        V316        V317        V318  \\\n",
       "isFraud                                                                        \n",
       "0        20.549558  41.955015  25.677123  111.096134  243.805877  161.620207   \n",
       "1        42.156677  78.281729  56.321550   74.519367  352.289202  176.764416   \n",
       "\n",
       "              V319       V320       V321  \n",
       "isFraud                                   \n",
       "0        18.368309  41.625136  28.204085  \n",
       "1        18.476726  54.404279  31.688605  \n",
       "\n",
       "[2 rows x 206 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_summary = df_train.groupby('isFraud')\n",
    "fraud_summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_summary.mean().to_csv('/Users/krahman/work/fraud_detection/saved_files/fraud_groupby.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "# D = time elapsed between each transaction, card = card information, C = counting, ie how many addresses \n",
    "# associated with card, M=True/False, V created features on ranking, counting, etc. \n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = 'isFraud'\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v \n",
    "col_num.append(col_target)\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# boolean columns. convert via dummy variable. We dont know if true/false is better than one or the other. \n",
    "# col_bool = col_m\n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i)))\n",
    "\n",
    "# columns removed dist1, dist2, R_emaildomain, DeviceInfo, DeviceType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 195 columns with null values.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "                \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 60:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()\n",
    "# df_null_info\n",
    "\n",
    "# determine what to do with columns that have too many unique values... obviously.. types of solutions\n",
    "# would be to put \"MISSING\" for those that dont have an email address... but you will need to evaluate \n",
    "# and make instead a counter of unique values, then append that and look at the CSV via google sheets. use \n",
    "# something like the code below \n",
    "\n",
    "# Planning - our preprocessing method must automatically drop missing values, but we can't do that because\n",
    "# we need to see about filling them in first, then decide if we need to drop them. Right now, we need to\n",
    "# create a dataframe that shows unique values for each column with missing values. \n",
    "\n",
    "# we need to look at each variable and see if it's unique or categorical. We need to use possibly PCA...? How do\n",
    "# we handle so many variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded: addr1 unique 333\n",
      "label encoded: addr2 unique 75\n",
      "dummies encoded: ProductCD unique 5\n",
      "dummies encoded: P_emaildomain unique 59\n",
      "label encoded: card1 unique 13553\n",
      "label encoded: card2 unique 501\n",
      "label encoded: card3 unique 115\n",
      "dummies encoded: card4 unique 4\n",
      "label encoded: card5 unique 120\n",
      "dummies encoded: card6 unique 4\n",
      "dummies encoded: M1 unique 2\n",
      "dummies encoded: M2 unique 2\n",
      "dummies encoded: M3 unique 2\n",
      "dummies encoded: M4 unique 3\n",
      "dummies encoded: M6 unique 2\n",
      "new dataframe shape:(590540, 285)\n"
     ]
    }
   ],
   "source": [
    "pp.df_train = pp.impute_features()\n",
    "pp.df_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')\n",
    "df_features = df_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_summary = df_features.groupby('isFraud')\n",
    "fraud_summary.mean().to_csv('/Users/krahman/work/fraud_detection/saved_files/fraud_summary.csv')\n",
    "\n",
    "corr = df_features.corr()\n",
    "corr.to_csv('/Users/krahman/work/fraud_detection/saved_files/corr_matrix.csv')\n",
    "corr2 = corr[corr['isFraud']>.17]\n",
    "corr2 = corr2.loc[:,corr2.index]\n",
    "\n",
    "sns.heatmap(corr2,\n",
    "           xticklabels=corr2.columns.values,\n",
    "           yticklabels=corr2.columns.values)\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.show()\n",
    "corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(x='addr1', y='addr2', data=df_features,\n",
    "#            fit_reg=False, hue='isFraud')\n",
    "# plt.show()\n",
    "\n",
    "# sns.distplot(df_features.addr1)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(X.card1, y)\n",
    "# sns.regplot(x='card5_237.0', y='isFraud', data=pp.df_train, logistic=True, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create features, target and split the dataframe\n",
    "# X = pp.df_train.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = pp.df_train[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dropping Features On Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# y_test = pp.df_train[col_target] #.rename(columns=['isFraud'])\n",
    "# y_test = pd.Series(y_test)\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing dropping columns\n",
    "# model_lr_results = pd.DataFrame()\n",
    "# model_recall = []\n",
    "# model_precision = []\n",
    "# model_auc_score = []\n",
    "\n",
    "# for col in ['addr1', 'addr2', 'P_emaildomain', 'card1', 'card2', 'card3', 'card5']:\n",
    "#     print('Dropping: ', col)\n",
    "#     X_drop = X.drop(col, axis=1)\n",
    "# #     X_drop = X_drop.loc[:10000,:]\n",
    "#     y_drop = y#[:10001]\n",
    "    \n",
    "#     scaled_X = StandardScaler().fit_transform(X_drop)\n",
    "#     # pca\n",
    "#     pca = PCA()\n",
    "#     pcomponents = pca.fit_transform(scaled_X)\n",
    "#     X_pca = pd.DataFrame(data=pcomponents)\n",
    "#     # split\n",
    "#     X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y_drop, test_size=0.1, random_state=42)\n",
    "#     # smote\n",
    "#     sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "#     X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "#     # model fit\n",
    "#     model_lr_pca = LogisticRegression(random_state=42)\n",
    "#     model_lr_pca.fit(X_train_res, y_train_res)\n",
    "#     # predict\n",
    "#     y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "#     y_pred_class = binarize(y_pred_prob, 0.5)[:,1]\n",
    "#     # scoring\n",
    "#     model_recall.append(recall_score(y_drop, y_pred_class))\n",
    "#     model_precision.append(precision_score(y_drop, y_pred_class))\n",
    "#     model_auc_score.append(roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "#     print(confusion_matrix(y_drop, y_pred_class))\n",
    "#     print(classification_report(y_drop, y_pred_class))\n",
    "#     print('AUC: ', roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "\n",
    "# model_recall = pd.Series(model_recall, name='recall')\n",
    "# model_precision = pd.Series(model_precision, name='precision')\n",
    "# model_auc_score = pd.Series(model_auc_score, name='auc_score')\n",
    "# model_results_final = pd.concat([model_auc_score, model_recall, model_precision],axis=1)\n",
    "# model_results_final\n",
    "\n",
    "# # it's apparent that label encoding on some of these don't really matter and if we drop them.. it doesn't really\n",
    "# # matter.. \n",
    "# # dropping these columns has little impact with logistic regression.. \n",
    "\n",
    "# # tune logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETE\n",
    "\n",
    "# model_lr_results = pd.DataFrame()\n",
    "# model_recall = []\n",
    "# model_precision = []\n",
    "# model_auc_score = []\n",
    "\n",
    "#     model_recall.append(recall_score(y_drop, y_pred_class))\n",
    "#     model_precision.append(precision_score(y_drop, y_pred_class))\n",
    "#     model_auc_score.append(roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "\n",
    "# model_recall = pd.Series(model_recall, name='recall')\n",
    "# model_precision = pd.Series(model_precision, name='precision')\n",
    "# model_auc_score = pd.Series(model_auc_score, name='auc_score')\n",
    "# model_results_final = pd.concat([model_auc_score, model_recall, model_precision],axis=1)\n",
    "# model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DELETE\n",
    "\n",
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "# df_temp[0:20]\n",
    "# pd.DataFrame(pca.components_, columns=list(X_temp.columns), index=range(0,225))\n",
    "# pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA (2 components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing our data, which is required for PCA.\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "pd.DataFrame(scaled_X, columns=X.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA instantiate and fit \n",
    "pca = PCA(n_components=2)\n",
    "pcomponents = pca.fit_transform(scaled_X)\n",
    "X_pca = pd.DataFrame(data = pcomponents, columns=['PC1','PC2'])\n",
    "print(X_pca.shape)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two principal components scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca['PC1'], X_pca['PC2'], c=y['isFraud'])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "\n",
    "# explaining vaariance\n",
    "print('Variance ratio:')\n",
    "print(pca.explained_variance_ratio_)\n",
    "# interpreting principal components\n",
    "print('\\nPrincipal components explained:')\n",
    "pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA to all features (all components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA().fit(scaled_X)\n",
    "# pca2.explained_variance_ratio_\n",
    "# np.cumsum(pca2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca2.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.title('Credit Card Fraud Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model w/ SMOTE only - base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=42)\n",
    "model_lr.fit(X_train_res, y_train_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using only SMOTE (and w/o PCA)\\n\")\n",
    "y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_lr.score(X_test1, y_test1))\n",
    "print(recall_score(y_test1, y_pred_test1))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test1, y_pred_test1))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test1, y_pred_test1))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred = model_lr.predict(X)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_lr.score(X, y))\n",
    "print(recall_score(y, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/PCA  w/SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='addr1', y='addr2', data=df_features,\n",
    "           fit_reg=False, hue='isFraud')\n",
    "plt.title('addr1 versus addr2')\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(df_features.addr1)\n",
    "plt.title('Addr1 Distribution')\n",
    "plt.show()\n",
    "\n",
    "# sns.distplot(df_features['addr3'])\n",
    "# plt.title('Addr3 Distribution')\n",
    "# plt.show()\n",
    "\n",
    "# sns.lmplot(x='addr3', y='addr4', data=df_features,\n",
    "#            fit_reg=False, hue='isFraud')\n",
    "# plt.title('addr3 versus addr4')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(df_features.addr2, kde=False)\n",
    "# plt.show()\n",
    "# df_features.addr2.value_counts(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in df_features.addr2.unique():\n",
    "#     if df_features.addr2.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_features.addr2.unique())\n",
    "# df_features.addr2.sort_values(ascending=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Email Class\n",
    "# P_emaildomain\n",
    "# list(df_train.columns)\n",
    "# df_train.P_emaildomain.unique()\n",
    "list_perc = []\n",
    "list_fraud_count = []\n",
    "list_non_fraud_count = []\n",
    "for val in df_train.P_emaildomain.unique():\n",
    "    non_fraud_count = df_train[(df_train.P_emaildomain==val) & (df_train.isFraud==0)].shape[0]\n",
    "    fraud_count = df_train[(df_train.P_emaildomain==val) & (df_train.isFraud==1)].shape[0]\n",
    "    \n",
    "    list_perc.append(fraud_count/non_fraud_count)\n",
    "    \n",
    "    list_fraud_count.append(fraud_count)\n",
    "    list_non_fraud_count.append(non_fraud_count)\n",
    "    \n",
    "col_email = pd.Series(df_train.P_emaildomain.unique(), name='email')\n",
    "col_perc = pd.Series(list_perc, name='fraud_perc')\n",
    "col_fraud_count = pd.Series(list_fraud_count, name='fraud_count')\n",
    "col_non_fraud_count = pd.Series(list_non_fraud_count, name='non_fraud_count')\n",
    "\n",
    "# col_perc\n",
    "df_email_fe = pd.concat([col_email, col_perc, col_fraud_count, col_non_fraud_count], axis=1)\n",
    "# df_email_fe\n",
    "# df_train[(df_train.P_emaildomain=='outlook.com') & (df_train.isFraud==1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df_email_fe.fraud_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: less occurring email domains are more likely to have higher fraud_perc because they are \n",
    "# perhaps an email provider with less resources. The card that the holder has attached to certain email\n",
    "# addresses puts them at higher risk for fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# # X = X.drop('addr1', axis=1)\n",
    "# # X = X.drop('addr2', axis=1)\n",
    "# y = df_features[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>...</th>\n",
       "      <th>M4_M1</th>\n",
       "      <th>M4_M2</th>\n",
       "      <th>M6_T</th>\n",
       "      <th>addr1_fe</th>\n",
       "      <th>addr2_fe</th>\n",
       "      <th>card1_fe</th>\n",
       "      <th>card2_fe</th>\n",
       "      <th>card3_fe</th>\n",
       "      <th>card5_fe</th>\n",
       "      <th>P_emaildomain_2_fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.285776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.393939</td>\n",
       "      <td>53.384489</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>9.237736</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.288929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.477727</td>\n",
       "      <td>74.807840</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>13.703223</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.161296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.577798</td>\n",
       "      <td>26.425206</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>2.274288</td>\n",
       "      <td>34.577807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.120587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.947229</td>\n",
       "      <td>17.790293</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>2.822371</td>\n",
       "      <td>7.708132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.257885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.882353</td>\n",
       "      <td>52.038936</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>13.703223</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590535</td>\n",
       "      <td>3577535</td>\n",
       "      <td>0</td>\n",
       "      <td>15811047</td>\n",
       "      <td>49.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.711321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.372666</td>\n",
       "      <td>53.384489</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>6.204501</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590536</td>\n",
       "      <td>3577536</td>\n",
       "      <td>0</td>\n",
       "      <td>15811049</td>\n",
       "      <td>39.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.452078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.257699</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>8.205715</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590537</td>\n",
       "      <td>3577537</td>\n",
       "      <td>0</td>\n",
       "      <td>15811079</td>\n",
       "      <td>30.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.720269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.734121</td>\n",
       "      <td>19.364771</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>8.205715</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590538</td>\n",
       "      <td>3577538</td>\n",
       "      <td>0</td>\n",
       "      <td>15811088</td>\n",
       "      <td>117.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.464734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.695459</td>\n",
       "      <td>32.481951</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>8.205715</td>\n",
       "      <td>7.380276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590539</td>\n",
       "      <td>3577539</td>\n",
       "      <td>0</td>\n",
       "      <td>15811131</td>\n",
       "      <td>279.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.738072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.925445</td>\n",
       "      <td>34.696214</td>\n",
       "      <td>2.946859</td>\n",
       "      <td>13.703223</td>\n",
       "      <td>13.592138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590540 rows × 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID  isFraud  TransactionDT  TransactionAmt   C1   C2   C3  \\\n",
       "0             2987000        0          86400           68.50  1.0  1.0  0.0   \n",
       "1             2987001        0          86401           29.00  1.0  1.0  0.0   \n",
       "2             2987002        0          86469           59.00  1.0  1.0  0.0   \n",
       "3             2987003        0          86499           50.00  2.0  5.0  0.0   \n",
       "4             2987004        0          86506           50.00  1.0  1.0  0.0   \n",
       "...               ...      ...            ...             ...  ...  ...  ...   \n",
       "590535        3577535        0       15811047           49.00  2.0  1.0  0.0   \n",
       "590536        3577536        0       15811049           39.50  1.0  1.0  0.0   \n",
       "590537        3577537        0       15811079           30.95  1.0  1.0  0.0   \n",
       "590538        3577538        0       15811088          117.00  1.0  1.0  0.0   \n",
       "590539        3577539        0       15811131          279.95  2.0  1.0  0.0   \n",
       "\n",
       "         C4   C5   C6  ...  M4_M1  M4_M2  M6_T  addr1_fe  addr2_fe  \\\n",
       "0       0.0  0.0  1.0  ...      0      1     1  2.285776       1.0   \n",
       "1       0.0  0.0  1.0  ...      0      0     1  3.288929       1.0   \n",
       "2       0.0  0.0  1.0  ...      0      0     0  4.161296       1.0   \n",
       "3       0.0  0.0  4.0  ...      0      0     0  4.120587       1.0   \n",
       "4       0.0  0.0  1.0  ...      0      0     0  4.257885       1.0   \n",
       "...     ...  ...  ...  ...    ...    ...   ...       ...       ...   \n",
       "590535  0.0  1.0  0.0  ...      0      0     0  3.711321       1.0   \n",
       "590536  0.0  0.0  1.0  ...      0      0     1  3.452078       1.0   \n",
       "590537  0.0  1.0  1.0  ...      0      0     1  3.720269       1.0   \n",
       "590538  0.0  0.0  3.0  ...      0      0     1  2.464734       1.0   \n",
       "590539  0.0  1.0  1.0  ...      0      0     1  2.738072       1.0   \n",
       "\n",
       "          card1_fe   card2_fe  card3_fe   card5_fe  P_emaildomain_2_fe  \n",
       "0       169.393939  53.384489  2.946859   9.237736           13.592138  \n",
       "1        27.477727  74.807840  2.946859  13.703223           13.592138  \n",
       "2         4.577798  26.425206  2.946859   2.274288           34.577807  \n",
       "3         7.947229  17.790293  2.946859   2.822371            7.708132  \n",
       "4        32.882353  52.038936  2.946859  13.703223           13.592138  \n",
       "...            ...        ...       ...        ...                 ...  \n",
       "590535    2.372666  53.384489  2.946859   6.204501           13.592138  \n",
       "590536    0.000000  30.257699  2.946859   8.205715           13.592138  \n",
       "590537   10.734121  19.364771  2.946859   8.205715           13.592138  \n",
       "590538   14.695459  32.481951  2.946859   8.205715            7.380276  \n",
       "590539   22.925445  34.696214  2.946859  13.703223           13.592138  \n",
       "\n",
       "[590540 rows x 286 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features['P_emaildomain_copy'] = df_train['P_emaildomain']\n",
    "list_col = ['addr1', 'addr2', 'card1', 'card2', 'card3', 'card5', 'P_emaildomain_copy']\n",
    "\n",
    "class FeatureEngineering():\n",
    "    '''create engineered features for columns without ordinal values'''\n",
    "    def __init__(self, list_col):\n",
    "        self.list_fraud_perc = []\n",
    "        self.df_feat = df_features.copy()\n",
    "        self.len_df_feat = self.df_feat.shape[0]\n",
    "        self.dict_all_feat = {}\n",
    "        self.new_col = []\n",
    "        self._create_feature()\n",
    "        \n",
    "    def _create_feature(self):\n",
    "        '''main method that executes functions'''\n",
    "        for col_val in list_col:\n",
    "            self._calculate_fraud_perc(col_val, self.df_feat)\n",
    "        self._map_col()\n",
    "        self._create_ratio()\n",
    "        self.df_feat = self.df_feat.drop(list_col, axis=1)\n",
    "            \n",
    "    def _calculate_fraud_perc(self, col_val, df_feat):\n",
    "        '''calculate fraud percentage for each column'''\n",
    "        list_perc = []\n",
    "        dict_feat = {}\n",
    "        unique_col_values = df_feat[col_val].unique()\n",
    "        for val in unique_col_values:\n",
    "            fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==1)].shape[0]\n",
    "            non_fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==0)].shape[0]\n",
    "            if (non_fraud_total==0):\n",
    "                list_perc.append(0)\n",
    "            else: \n",
    "                list_perc.append(fraud_total/non_fraud_total)\n",
    "        self._create_dict(col_val, list_perc, unique_col_values)\n",
    "\n",
    "    def _create_dict(self, col_val, list_perc, unique_col_values):\n",
    "        '''create dictionary for original values to new fraud percent values'''\n",
    "        col_name = col_val + '_fraud_perc'\n",
    "        series_perc = pd.Series(list_perc, name=col_name)\n",
    "        series_col = pd.Series(unique_col_values, name=col_val)\n",
    "        df_feat = pd.concat([series_col, series_perc], axis=1)\n",
    "        df_feat = df_feat.sort_values(col_name, ascending=False) \n",
    "        dict_feat = df_feat.set_index(col_val).to_dict()\n",
    "        self.dict_all_feat.update(dict_feat)\n",
    "\n",
    "    def _map_col(self):\n",
    "        '''map dictionary values to new features'''\n",
    "        dict_keys = self.dict_all_feat.keys()\n",
    "        for col, key in zip(list_col, dict_keys):\n",
    "            self.df_feat[col + '_fe'] = self.df_feat[col].map(self.dict_all_feat[key])\n",
    "            self.new_col.append(col + '_fe')\n",
    "            \n",
    "    def _create_ratio(self):\n",
    "        '''finalize new features with ranking values'''\n",
    "        for val in self.new_col:\n",
    "            col_min_val = self.df_feat[self.df_feat[val] > 0][val].min()\n",
    "            self.df_feat[val] = self.df_feat[val]/col_min_val\n",
    "\n",
    "fe = FeatureEngineering(list_col)\n",
    "fe.df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionDT',\n",
       " 'TransactionAmt',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D15',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V28',\n",
       " 'V29',\n",
       " 'V30',\n",
       " 'V31',\n",
       " 'V33',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V55',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V79',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V135',\n",
       " 'V137',\n",
       " 'V279',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V289',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V299',\n",
       " 'V300',\n",
       " 'V304',\n",
       " 'V307',\n",
       " 'V309',\n",
       " 'V314',\n",
       " 'V321',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_W',\n",
       " 'P_emaildomain_att.net',\n",
       " 'P_emaildomain_bellsouth.net',\n",
       " 'P_emaildomain_cableone.net',\n",
       " 'P_emaildomain_centurylink.net',\n",
       " 'P_emaildomain_cfl.rr.com',\n",
       " 'P_emaildomain_charter.net',\n",
       " 'P_emaildomain_embarqmail.com',\n",
       " 'P_emaildomain_frontier.com',\n",
       " 'P_emaildomain_gmail',\n",
       " 'P_emaildomain_gmail.com',\n",
       " 'P_emaildomain_hotmail.com',\n",
       " 'P_emaildomain_hotmail.de',\n",
       " 'P_emaildomain_hotmail.es',\n",
       " 'P_emaildomain_hotmail.fr',\n",
       " 'P_emaildomain_juno.com',\n",
       " 'P_emaildomain_live.com',\n",
       " 'P_emaildomain_mail.com',\n",
       " 'P_emaildomain_me.com',\n",
       " 'P_emaildomain_msn.com',\n",
       " 'P_emaildomain_netzero.com',\n",
       " 'P_emaildomain_outlook.es',\n",
       " 'P_emaildomain_prodigy.net.mx',\n",
       " 'P_emaildomain_protonmail.com',\n",
       " 'P_emaildomain_ptd.net',\n",
       " 'P_emaildomain_servicios-ta.com',\n",
       " 'P_emaildomain_twc.com',\n",
       " 'P_emaildomain_windstream.net',\n",
       " 'P_emaildomain_yahoo.co.jp',\n",
       " 'P_emaildomain_yahoo.co.uk',\n",
       " 'P_emaildomain_yahoo.com',\n",
       " 'P_emaildomain_yahoo.com.mx',\n",
       " 'P_emaildomain_yahoo.de',\n",
       " 'P_emaildomain_yahoo.fr',\n",
       " 'P_emaildomain_ymail.com',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'card6_credit',\n",
       " 'card6_debit',\n",
       " 'card6_debit or credit',\n",
       " 'M1_T',\n",
       " 'M3_T',\n",
       " 'M4_M2',\n",
       " 'addr1_fe',\n",
       " 'card3_fe']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "\n",
    "X = fe.df_feat.drop(col_target, axis=1)\n",
    "X = X.drop(col_id, axis=1)\n",
    "# TESTING DROP FEATURES\n",
    "X = X.drop(list_drop_col_1, axis=1)\n",
    "y = df_features[col_target]\n",
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed: 58.90604650179545\n",
      "[[413977  98955]\n",
      " [  4747  13807]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.81      0.89    512932\n",
      "           1       0.12      0.74      0.21     18554\n",
      "\n",
      "    accuracy                           0.80    531486\n",
      "   macro avg       0.56      0.78      0.55    531486\n",
      "weighted avg       0.96      0.80      0.87    531486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ### PCA + SMOTE testing algorithm ###\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# # applying PCA\n",
    "# scaled_X = StandardScaler().fit_transform(X)\n",
    "# # pca = PCA(n_components=250)\n",
    "# pca = PCA()\n",
    "# pcomponents = pca.fit_transform(scaled_X)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)\n",
    "\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# # applying SMOTE\n",
    "# sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "\n",
    "# # fit logistic regression\n",
    "# model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# # predict\n",
    "# y_pred = model_lr_pca_sm.predict(X_train2)\n",
    "# # time\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print('\\nTime elapsed:', elapsed_time / 60)\n",
    "# print(confusion_matrix(y_train2, y_pred))\n",
    "# print(classification_report(y_train2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed: 2.534430682659149\n",
      "[[237544 275388]\n",
      " [  3345  15209]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.46      0.63    512932\n",
      "           1       0.05      0.82      0.10     18554\n",
      "\n",
      "    accuracy                           0.48    531486\n",
      "   macro avg       0.52      0.64      0.36    531486\n",
      "weighted avg       0.95      0.48      0.61    531486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no PCA, SMOTE only\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "import time\n",
    "start_time = time.time()\n",
    "# applying SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "\n",
    "# fit logistic regression\n",
    "model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# predict\n",
    "y_pred = model_lr_pca_sm.predict(X_train2)\n",
    "# time\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)\n",
    "print(confusion_matrix(y_train2, y_pred))\n",
    "print(classification_report(y_train2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed: 2.5606173157691954\n"
     ]
    }
   ],
   "source": [
    "# # fit logistic regression\n",
    "# model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# # predict\n",
    "# y_pred = model_lr_pca_sm.predict(X_train2)\n",
    "# # time\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print('\\nTime elapsed:', elapsed_time / 60)\n",
    "# print(confusion_matrix(y_train2, y_pred))\n",
    "# print(classification_report(y_train2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS: NEXT, look at randys videos to figure out EDA and how we can uncover new potential features.. \n",
    "\n",
    "# RESULTS: dropped features and used PCA, worstened score\n",
    "# [[413977  98955]\n",
    "#  [  4747  13807]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.81      0.89    512932\n",
    "#            1       0.12      0.74      0.21     18554\n",
    "\n",
    "#     accuracy                           0.80    531486\n",
    "#    macro avg       0.56      0.78      0.55    531486\n",
    "# weighted avg       0.96      0.80      0.87    531486\n",
    "# RESULTS: drop low ranking features found in decision tree\n",
    "# [[329812 183120]\n",
    "#  [  4810  13744]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.64      0.78    512932\n",
    "#            1       0.07      0.74      0.13     18554\n",
    "\n",
    "#     accuracy                           0.65    531486\n",
    "#    macro avg       0.53      0.69      0.45    531486\n",
    "# weighted avg       0.95      0.65      0.76    531486\n",
    "\n",
    "# RESULTS: without PCA\n",
    "# [[298715 214217]\n",
    "#  [  3509  15045]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.58      0.73    512932\n",
    "#            1       0.07      0.81      0.12     18554\n",
    "\n",
    "#     accuracy                           0.59    531486\n",
    "#    macro avg       0.53      0.70      0.43    531486\n",
    "# weighted avg       0.96      0.59      0.71    531486\n",
    "\n",
    "# RESULTS: keep ohe for email and create p email feature calculate email domain fraud feature\n",
    "# [[434421  78511]\n",
    "#  [  4429  14125]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.84    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.84      0.89    531486\n",
    "\n",
    "# RESULTS: with new feature, try dropping ohe p_email and test\n",
    "# [[435499  77433]\n",
    "#  [  4539  14015]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.85    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.85      0.89    531486\n",
    "\n",
    "# RESULTS: cut FP rate quite a bit. fixing fraud perc calculation by doing fraud/non fraud for each value in each column\n",
    "# [[434423  78509]\n",
    "#  [  4430  14124]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.84    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.84      0.89    531486\n",
    "\n",
    "\n",
    "# RESULTS: adding 60 for one hot encoding\n",
    "# [[403339 109593]\n",
    "#  [  4393  14161]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.79      0.88    512932\n",
    "#            1       0.11      0.76      0.20     18554\n",
    "\n",
    "#     accuracy                           0.79    531486\n",
    "#    macro avg       0.55      0.77      0.54    531486\n",
    "# weighted avg       0.96      0.79      0.85    531486\n",
    "\n",
    "# RESULTS: fe for 'addr1', 'addr2', 'card1', 'card2', 'card3', 'card5'\n",
    "# [[397116 115816]\n",
    "#  [  4490  14064]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.77      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: only addr6. Creating ratio ranking of higher risk areas for fraud. \n",
    "# [[396866 116066]\n",
    "#  [  4534  14020]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# NEXT, test creating iqr range instead of percentage values, instead of what we did\n",
    "# with addr5, then create one for addr2, then create an addr7 based on interaction with addr2. \n",
    "\n",
    "\n",
    "\n",
    "# RESULTS: only addr5. testing mapping percentage values transformed..\n",
    "# [[396898 116034]\n",
    "#  [  4535  14019]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with only addr3, addr4\n",
    "# [[396868 116064]\n",
    "#  [  4544  14010]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: w/o addr1, addr2, addr3, addr4\n",
    "# [[396629 116303]\n",
    "#  [  4545  14009]]\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with only addr1, addr2\n",
    "# [[396803 116129]\n",
    "#  [  4555  13999]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.75      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with addr1, addf2, addr3, addr4\n",
    "# [[396877 116055]\n",
    "#  [  4549  14005]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.75      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_lr_pca_sm.predict(X_pca)\n",
    "\n",
    "# cross validation\n",
    "# result = cross_val_score(model_lr_pca_sm, X_train2, y_train2, cv=10, n_jobs=-1, scoring='recall_score')\n",
    "# result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes, do we need to visualize our results?? We need to look at our confusion matrix\n",
    "# and decide how to improve our results from here... Test and see what happens if we \n",
    "# increase our principal components... test removing columns, adding columns, imputing\n",
    "# certain columns all together. \n",
    "\n",
    "# test imputing 500 or less for ohe. Then test dropping card1 due to its number\n",
    "# of unique values that would make our data very high dimensional. We can try only ohe for\n",
    "# the entire dataset to see how our model performs over all... Though.. it will likely run \n",
    "# out of memory and crash. \n",
    "\n",
    "# ALSO test probabilities on logisticregression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "plt.hist(y_pred_prob[:,1], bins=8)\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Histogram of Probability of Fraud\")\n",
    "plt.xlabel(\"Predicted probability of Fraud\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "y_pred_class = binarize(y_pred_prob, 0.5)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y, y_pred_class))\n",
    "print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title(\"ROC curve for fraud detection classifier\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold):\n",
    "    print(\"Sensitivity:\", tpr[thresholds > threshold][-1])\n",
    "    print(\"Specificity:\", 1 - fpr[thresholds > threshold][-1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(.5)\n",
    "evaluate_threshold(.2)\n",
    "evaluate_threshold(.1)\n",
    "\n",
    "print(roc_auc_score(y, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_actual = model_lr_pca.predict(X_test2) # actual prediction test data set\n",
    "print('y_pred_actual on test set\\n')\n",
    "print(y_pred_actual[0:10])\n",
    "print(confusion_matrix(y_test2, y_pred_actual))\n",
    "print(classification_report(y_test2, y_pred_actual))\n",
    "\n",
    "print('y_pred_proba\\n')\n",
    "y_pred_proba = model_lr_pca.predict_proba(X_test2)\n",
    "y_pred_class = binarize(y_pred_proba, 0.5)[:,1]\n",
    "print(y_pred_class[0:10])\n",
    "print(confusion_matrix(y_test2, y_pred_class))\n",
    "print(classification_report(y_test2, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_actual = model_lr_pca.predict(X_pca) # actual full data set\n",
    "print('Logistic Regression')\n",
    "print('y_pred_actual full data set\\n')\n",
    "print(y_pred_actual[0:10])\n",
    "print(confusion_matrix(y, y_pred_actual))\n",
    "print(classification_report(y, y_pred_actual))\n",
    "\n",
    "print('y_pred_proba full data set\\n')\n",
    "y_pred_proba = model_lr_pca.predict_proba(X_pca)#[:,1]#[0:10]\n",
    "y_pred_class = binarize(y_pred_proba, 0.2)[:,1]\n",
    "print(y_pred_class[0:10])\n",
    "print(confusion_matrix(y, y_pred_class))\n",
    "print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using Logistic Regression, PCA, SMOTE\\n\")\n",
    "y_pred_pca_sm = model_lr_pca.predict(X_test2)\n",
    "# y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_lr_pca.score(X_test2, y_test2))\n",
    "print(recall_score(y_test2, y_pred_pca_sm))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test2, y_pred_pca_sm))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test2, y_pred_pca_sm))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred_pca_sm_whole = model_lr_pca.predict(X_pca)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_lr_pca.score(X_pca, y))\n",
    "print(recall_score(y, y_pred_pca_sm_whole))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred_pca_sm_whole))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred_pca_sm_whole))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - w/PCA w/SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "model_dt_pca_smote.fit(X_train_res, y_train_res)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>feat_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>V110</td>\n",
       "      <td>0.001368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>ProductCD_R</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>V136</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>card2_fe</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>V124</td>\n",
       "      <td>0.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>V303</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>M2_T</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>V54</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>P_emaildomain_web.de</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>P_emaildomain_sc.rr.com</td>\n",
       "      <td>0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>V301</td>\n",
       "      <td>0.001320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>V282</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>V308</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>ProductCD_S</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>V100</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>addr2_fe</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>V312</td>\n",
       "      <td>0.001291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>V310</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>V62</td>\n",
       "      <td>0.001275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>V115</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>P_emaildomain_suddenlink.net</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>P_emaildomain_sbcglobal.net</td>\n",
       "      <td>0.001266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>P_emaildomain_mac.com</td>\n",
       "      <td>0.001259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>V134</td>\n",
       "      <td>0.001250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>V305</td>\n",
       "      <td>0.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>V306</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>V317</td>\n",
       "      <td>0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>card4_discover</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>V32</td>\n",
       "      <td>0.001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>P_emaildomain_yahoo.es</td>\n",
       "      <td>0.001166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>V50</td>\n",
       "      <td>0.001148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>P_emaildomain_q.com</td>\n",
       "      <td>0.001094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>P_emaildomain_rocketmail.com</td>\n",
       "      <td>0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>P_emaildomain_roadrunner.com</td>\n",
       "      <td>0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>P_emaildomain_gmx.de</td>\n",
       "      <td>0.001049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>P_emaildomain_earthlink.net</td>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>P_emaildomain_aol.com</td>\n",
       "      <td>0.000964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>V295</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>M4_M1</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>P_emaildomain_frontiernet.net</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>V302</td>\n",
       "      <td>0.000853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>P_emaildomain_anonymous.com</td>\n",
       "      <td>0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>P_emaildomain_cox.net</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>V298</td>\n",
       "      <td>0.000799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>V123</td>\n",
       "      <td>0.000790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>P_emaildomain_hotmail.co.uk</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>V130</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>card5_fe</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>V319</td>\n",
       "      <td>0.000691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>P_emaildomain_live.com.mx</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>V316</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>V287</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>card1_fe</td>\n",
       "      <td>0.000624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>P_emaildomain_2_fe</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               col  feat_rank\n",
       "132                           V110   0.001368\n",
       "204                    ProductCD_R   0.001366\n",
       "158                           V136   0.001366\n",
       "280                       card2_fe   0.001364\n",
       "146                           V124   0.001353\n",
       "184                           V303   0.001340\n",
       "272                           M2_T   0.001339\n",
       "76                             V54   0.001331\n",
       "255           P_emaildomain_web.de   0.001325\n",
       "250        P_emaildomain_sc.rr.com   0.001322\n",
       "182                           V301   0.001320\n",
       "163                           V282   0.001316\n",
       "189                           V308   0.001312\n",
       "205                    ProductCD_S   0.001311\n",
       "122                           V100   0.001302\n",
       "278                       addr2_fe   0.001302\n",
       "193                           V312   0.001291\n",
       "191                           V310   0.001286\n",
       "84                             V62   0.001275\n",
       "137                           V115   0.001269\n",
       "252   P_emaildomain_suddenlink.net   0.001269\n",
       "249    P_emaildomain_sbcglobal.net   0.001266\n",
       "234          P_emaildomain_mac.com   0.001259\n",
       "156                           V134   0.001250\n",
       "186                           V305   0.001241\n",
       "187                           V306   0.001232\n",
       "198                           V317   0.001219\n",
       "265                 card4_discover   0.001180\n",
       "54                             V32   0.001171\n",
       "262         P_emaildomain_yahoo.es   0.001166\n",
       "72                             V50   0.001148\n",
       "246            P_emaildomain_q.com   0.001094\n",
       "248   P_emaildomain_rocketmail.com   0.001078\n",
       "247   P_emaildomain_roadrunner.com   0.001070\n",
       "223           P_emaildomain_gmx.de   0.001049\n",
       "217    P_emaildomain_earthlink.net   0.001043\n",
       "208          P_emaildomain_aol.com   0.000964\n",
       "176                           V295   0.000936\n",
       "274                          M4_M1   0.000932\n",
       "220  P_emaildomain_frontiernet.net   0.000900\n",
       "183                           V302   0.000853\n",
       "207    P_emaildomain_anonymous.com   0.000822\n",
       "216          P_emaildomain_cox.net   0.000817\n",
       "179                           V298   0.000799\n",
       "145                           V123   0.000790\n",
       "224    P_emaildomain_hotmail.co.uk   0.000789\n",
       "152                           V130   0.000763\n",
       "282                       card5_fe   0.000721\n",
       "200                           V319   0.000691\n",
       "232      P_emaildomain_live.com.mx   0.000681\n",
       "197                           V316   0.000656\n",
       "168                           V287   0.000640\n",
       "279                       card1_fe   0.000624\n",
       "283             P_emaildomain_2_fe   0.000000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature importance\n",
    "col_name = pd.Series(X.columns, name='col')\n",
    "col_feat_rank = pd.Series(model_dt_pca_smote.feature_importances_, name='feat_rank')\n",
    "df_feat_rank = pd.concat([col_name, col_feat_rank], axis=1).sort_values('feat_rank', ascending=False)\n",
    "# print(df_feat_rank.shape[0])\n",
    "df_feat_rank[230:284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature ranking\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.lineplot(x='col', y='feat_rank', data=df_feat_rank)\n",
    "plt.title('Feature Ranking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure(figsize=(5,10))\n",
    "sns.barplot(df_feat_rank.feat_rank[0:10], df_feat_rank.col[0:10], palette='Blues_d')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "sns.kdeplot(df_temp.loc[(df_temp.isFraud==0), 'TransactionAmt'], color='b', shade=True, label='Not Fraud')\n",
    "# sns.kdeplot(df_temp.loc[(df_temp.isFraud==1), 'TransactionAmt'], color='r', shade=True, label='Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['V294',\n",
       " 'P_emaildomain_optonline.net',\n",
       " 'P_emaildomain_netzero.net',\n",
       " 'V99',\n",
       " 'V288',\n",
       " 'M6_T',\n",
       " 'V98',\n",
       " 'V311',\n",
       " 'V286',\n",
       " 'V320',\n",
       " 'V318',\n",
       " 'V313',\n",
       " 'V120',\n",
       " 'P_emaildomain_outlook.com',\n",
       " 'P_emaildomain_comcast.net',\n",
       " 'V97',\n",
       " 'V116',\n",
       " 'P_emaildomain_icloud.com',\n",
       " 'P_emaildomain_verizon.net',\n",
       " 'V315',\n",
       " 'V56',\n",
       " 'P_emaildomain_live.fr',\n",
       " 'V78',\n",
       " 'V27',\n",
       " 'V11',\n",
       " 'V110',\n",
       " 'ProductCD_R',\n",
       " 'V136',\n",
       " 'card2_fe',\n",
       " 'V124',\n",
       " 'V303',\n",
       " 'M2_T',\n",
       " 'V54',\n",
       " 'P_emaildomain_web.de',\n",
       " 'P_emaildomain_sc.rr.com',\n",
       " 'V301',\n",
       " 'V282',\n",
       " 'V308',\n",
       " 'ProductCD_S',\n",
       " 'V100',\n",
       " 'addr2_fe',\n",
       " 'V312',\n",
       " 'V310',\n",
       " 'V62',\n",
       " 'V115',\n",
       " 'P_emaildomain_suddenlink.net',\n",
       " 'P_emaildomain_sbcglobal.net',\n",
       " 'P_emaildomain_mac.com',\n",
       " 'V134',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V317',\n",
       " 'card4_discover',\n",
       " 'V32',\n",
       " 'P_emaildomain_yahoo.es',\n",
       " 'V50',\n",
       " 'P_emaildomain_q.com',\n",
       " 'P_emaildomain_rocketmail.com',\n",
       " 'P_emaildomain_roadrunner.com',\n",
       " 'P_emaildomain_gmx.de',\n",
       " 'P_emaildomain_earthlink.net',\n",
       " 'P_emaildomain_aol.com',\n",
       " 'V295',\n",
       " 'M4_M1',\n",
       " 'P_emaildomain_frontiernet.net',\n",
       " 'V302',\n",
       " 'P_emaildomain_anonymous.com',\n",
       " 'P_emaildomain_cox.net',\n",
       " 'V298',\n",
       " 'V123',\n",
       " 'P_emaildomain_hotmail.co.uk',\n",
       " 'V130',\n",
       " 'card5_fe',\n",
       " 'V319',\n",
       " 'P_emaildomain_live.com.mx',\n",
       " 'V316',\n",
       " 'V287',\n",
       " 'card1_fe',\n",
       " 'P_emaildomain_2_fe']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_drop_col_1 = list(df_feat_rank[df_feat_rank.feat_rank < .0015]['col'])\n",
    "list_drop_col_2 = list(df_feat_rank[df_feat_rank.feat_rank < .0015]['col'])\n",
    "list_drop_col_3 = list(df_feat_rank[df_feat_rank.feat_rank < .01]['col'])\n",
    "\n",
    "print(len(list_drop_col_1))\n",
    "print(len(list_drop_col_2))\n",
    "print(len(list_drop_col_3))\n",
    "\n",
    "list_drop_col_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using PCA\\n\")\n",
    "y_pred_pca = model_dt_pca_smote.predict(X_test2)\n",
    "# y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_dt_pca_smote.score(X_test2, y_test2))\n",
    "print(recall_score(y_test2, y_pred_pca))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test2, y_pred_pca))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test2, y_pred_pca))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred_pca = model_dt_pca_smote.predict(X_pca)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_dt_pca_smote.score(X_pca, y))\n",
    "print(recall_score(y, y_pred_pca))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred_pca))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"\\nPrincipal components explained:\")\n",
    "pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explaining variance\n",
    "# print('Variance ratio:')\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # interpreting principal components\n",
    "# print('\\nPrincipal components explained:')\n",
    "# pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))\n",
    "\n",
    "# # # predicting on original dataset\n",
    "# # y_pred = clf_lr.predict(X)\n",
    "# # print(\"\\nTest Results\")\n",
    "# # print(clf_lr.score(X, y))\n",
    "# # print(recall_score(y, y_pred))\n",
    "# # print(\"\\nConfusion Matrix\")\n",
    "# # print(confusion_matrix(y, y_pred))\n",
    "# # print('\\nClassification Report:\\n')\n",
    "# # print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X2 = StandardScaler().fit_transform(X)\n",
    "# pca3 = PCA(n_components=275)\n",
    "# pcomponents = pca3.fit_transform(scaled_X2)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying SMOTE to train set to correct class imbalance\n",
    "sm = SMOTE(random_state=42, ratio = 1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to residuals created by SMOTE\n",
    "clf_lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "clf_lr.fit(X_train_res, y_train_res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set\n",
    "y_test_pred = clf_lr.predict(X_test)\n",
    "print(\"Validation results\")\n",
    "print(clf_lr.score(X_test, y_test))\n",
    "print(recall_score(y_test, y_test_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on original dataset\n",
    "y_pred = clf_lr.predict(X)\n",
    "print(\"\\nTest Results\")\n",
    "print(clf_lr.score(X, y))\n",
    "print(recall_score(y, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# clf = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\n",
    "# clf.predict(X_test)\n",
    "# clf.score(X_test, y_test)\n",
    "# 0.7870216903822372 \n",
    "# 0.8013998429794899 < 60\n",
    "# 0.5085411973583608 < 1000\n",
    "# 0.63556873752431 < 60 dropped col_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confusion matrix\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read up on class im balance and correct it. \n",
    "# perhaps we can one by one run our model through a decision tree and do one hot encoding for one big \n",
    "# categorical column at a time lets say 13,000 unique values, then we can see of the new 13,000 columns we\n",
    "# have if any actually have predictive value for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # define iv\n",
    "# iv = X.columns\n",
    "\n",
    "# # fit the logistic regression function\n",
    "# logReg = sm.Logit(y_train, X_train)\n",
    "# answer = logReg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in pp.df_train.columns:\n",
    "#     if (pp.df_train[pp.df_train[val]=='nan'].shape[0]) > 0:\n",
    "#         print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pp.df_train\n",
    "# # lets find out which columns are object... \n",
    "# list_col_object = []\n",
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[val].dtype=='O':\n",
    "#         list_col_object.append(val)\n",
    "        \n",
    "# pp.df_train[list_col_object]\n",
    "# # for card2, nan was the most commonly seen value... so it imputed that...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[pp.df_train[val]=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_col_object = []\n",
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[val]:\n",
    "#         list_col_object.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(pp.df_train[list_col_object].isnull())\n",
    "# pp.df_train['card2'].unique()\n",
    "# pp.df_train[df_train['card2']=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(np.sum(pp.df_train.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(df_train['V14'])\n",
    "# sns.barplot(df_train['V196'])\n",
    "# we need to imput the mode here.. \n",
    "# df_train['V14'].mode()\n",
    "# df_train['V22'].unique()\n",
    "# for val in col_v:\n",
    "#     print(val)\n",
    "#     print(df_train[val].unique())\n",
    "# we ned to descern what is a 0 1 outcome then impute.\n",
    "\n",
    "# col = 'V290'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mode()[0])\n",
    "# plt.hist(series_temp);\n",
    "# missing_val = np.sum(df_train[col].isnull())\n",
    "# print('Missing values: ' + str(missing_val))\n",
    "# print(\"REAL VALUE COUNTS: \")\n",
    "# df_train[col].value_counts().head()\n",
    "\n",
    "# col = 'card4'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mode()[0])\n",
    "# plt.hist(series_temp);\n",
    "# df_train[col].value_counts()\n",
    "\n",
    "# col = 'D1'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mean())\n",
    "# plt.hist(series_temp);\n",
    "# df_train['D1'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
