{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_raw = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "df_train = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = 'isFraud'\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v \n",
    "col_num.append(col_target)\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train.copy()\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "    \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 60:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.df_train = pp.impute_features()\n",
    "pp.df_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')\n",
    "df_features = df_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create features, target and split the dataframe\n",
    "# X = pp.df_train.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = pp.df_train[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after running final_features, run create_final_df.\n",
      "after running final_features, run create_final_df.\n",
      "keeping original feature card5\n",
      "keeping original feature V317\n",
      "keeping original feature V69\n",
      "keeping original feature D1\n",
      "keeping original feature D3\n",
      "keeping original feature D4\n",
      "keeping original feature D11\n",
      "dropping columns:  ['addr1', 'addr2', 'card2', 'card3', 'C1', 'V294', 'V279', 'C14', 'V306', 'D2', 'D10', 'C4']\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "bool_apply_pca set to false.\n",
      "bool_apply_pca set to false.\n",
      "creating tuning dataframe...\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "final dataframe created.\n",
      "To test a model use the mod.create_df_score_model(model_current)\n",
      "method where, for example, model_current=LogisticRegression().\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineering():\n",
    "    '''create new features for columns without ordinal values'''\n",
    "    def __init__(self):\n",
    "        self.list_fraud_perc = []\n",
    "        self.df_feat = df_features.copy()\n",
    "        self.df_raw = df_raw.copy()\n",
    "        \n",
    "        self.len_df_feat = self.df_feat.shape[0]\n",
    "        self.dict_all_feat = {}\n",
    "        self.new_col = []\n",
    "        \n",
    "        self.col = []\n",
    "        self.col_fe = []\n",
    "        self.df_new_feat = pd.DataFrame()\n",
    "        self.list_drop_col = []\n",
    "        self.str_list_col_fe = []\n",
    "        self.list_feat = []\n",
    "\n",
    "    def feature_testing(self, bool_drop_col, list_feat):\n",
    "        '''testing and scoring new potential features'''\n",
    "        print(\"While running feature_testing, do not run final_features.\")            \n",
    "        if list_feat:\n",
    "            for col in list_feat:\n",
    "                self.col_fe = col\n",
    "                bool_predict_proba = False\n",
    "                if col in df_features.columns:\n",
    "                    df_feat = self.create_test_feature(bool_drop_col, col)\n",
    "                    if df_feat_1000:\n",
    "                        df_feat = df_feat[0:1000]\n",
    "                    df_feat = df_feat.drop(self.list_drop_col[-1], axis=1)\n",
    "                    self._apply_df_transform(df_feat)\n",
    "                    model_lr = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "                    self._convert_list_to_string(list_feat)\n",
    "                    mod.create_df_score_model(model_lr)\n",
    "                else:\n",
    "                    print(\"\\nColumn\", col, \"does not exist in dataframe.\\n\")\n",
    "            self.col_fe = []\n",
    "        self.list_drop_col = []\n",
    "            \n",
    "    def final_features(self, bool_drop_col, list_feat):\n",
    "        '''creates final new features'''\n",
    "        print('after running final_features, run create_final_df.')\n",
    "        self.list_feat = list_feat\n",
    "        df_feat = self.create_feature(bool_drop_col, list_feat)  \n",
    "        if df_feat_1000:\n",
    "            df_feat = df_feat[0:1000]\n",
    "        for col in list_feat:\n",
    "            col_fe = self._append_col_lists(col)\n",
    "            df_feat[col] = self._fill_na(df_feat, col_fe)\n",
    "            self._concat_df_new_feat(df_feat, col_fe)\n",
    "        self._convert_list_to_string(list_feat)\n",
    "        return df_feat ### delete?\n",
    "    \n",
    "    def _append_col_lists(self, col):\n",
    "        '''appending columns and new feature column names'''\n",
    "        col_fe = col + '_fe'\n",
    "        self.col.append(col)\n",
    "        self.col_fe.append(col_fe)\n",
    "        return col_fe\n",
    "\n",
    "    def _fill_na(self, df_feat, col_fe):\n",
    "        '''fill na values for new features'''\n",
    "        col_mode = df_feat[col_fe].mode()[0]\n",
    "        return df_feat[col_fe].fillna(col_mode)\n",
    "        \n",
    "    def _concat_df_new_feat(self, df_feat, col_fe):\n",
    "        '''adding new feauture columns to one dataframe'''  \n",
    "        df_temp = df_feat[col_fe]\n",
    "        self.df_new_feat = pd.concat([self.df_new_feat, df_temp], axis=1)\n",
    "    \n",
    "    def _convert_list_to_string(self, list_feat):\n",
    "        '''convert list to string to print later'''\n",
    "        str_temp = ''\n",
    "        for val in list_feat:\n",
    "            str_temp = str_temp + val + ' '\n",
    "        self.str_list_col_fe = str_temp\n",
    "\n",
    "    def create_final_df(self):\n",
    "        '''creates final dataframe after creating final_features'''\n",
    "        df_feat = pd.concat([df_features, self.df_new_feat], axis=1)\n",
    "        if df_feat_1000:\n",
    "            df_feat = pd.concat([df_features[0:1000], self.df_new_feat], axis=1)\n",
    "        print('dropping columns: ', self.list_drop_col)\n",
    "        df_feat = df_feat.drop(self.list_drop_col, axis=1)\n",
    "        \n",
    "        self._apply_df_transform(df_feat)\n",
    "        \n",
    "        self._create_tuning_df(df_feat)\n",
    "        self.list_drop_col = []\n",
    "        self._final_df_summary()\n",
    "        \n",
    "    def _shuffle_df(self, X, y):\n",
    "        '''shuffle dataframe'''\n",
    "        y = pd.Series(y)\n",
    "        X = pd.DataFrame(X)\n",
    "        df_temp = pd.concat([X, y], keys=['features','target'], axis=1)\n",
    "        df_temp = shuffle(df_temp).reset_index(drop=True)\n",
    "        X = df_temp.features\n",
    "        y = df_temp.target\n",
    "        return X, y\n",
    "        \n",
    "    def _final_df_summary(self):\n",
    "        print(\"final dataframe created.\")\n",
    "        print(\"To test a model use the mod.create_df_score_model(model_current)\")\n",
    "        print(\"method where, for example, model_current=LogisticRegression().\")\n",
    "\n",
    "    def _apply_df_transform(self, df_feat):\n",
    "        '''create dataframe, apply pca, apply smote'''\n",
    "        self.df_feat = df_feat\n",
    "        X, y = self._drop_col_id_target(df_feat)\n",
    "        X_train, X_test, y_train, y_test = self._split_dataframe(X, y)\n",
    "        X_train, y_train = self._apply_downsampling(X_train, y_train) # apply only train set\n",
    "        X_train, y_train = self._apply_smote(X_train, y_train)        # apply only train set\n",
    "        X_train, y_train = self._shuffle_df(X_train, y_train)\n",
    "        \n",
    "        mod.X_train, mod.y_train = self._apply_pca(X_train, y_train)          # apply to train set\n",
    "        mod.X_test , mod.y_test = self._apply_pca(X_test, y_test) \n",
    "        self._convert_to_matrix()\n",
    "        \n",
    "    def _create_tuning_df(self, df_feat):\n",
    "        '''whole dataframe used for model tuning'''\n",
    "        if bool_create_tuning_df:\n",
    "            print(\"creating tuning dataframe...\")\n",
    "            X, y = self._drop_col_id_target(df_feat)\n",
    "            X, y = self._apply_downsampling(X, y)\n",
    "            X, y = self._apply_smote(X, y)\n",
    "            mod.X_features, mod.y_target = self._shuffle_df(X, y)\n",
    "        else:\n",
    "            print('bool_create_tuning_df set to false.')\n",
    "\n",
    "    def _split_dataframe(self, X, y):\n",
    "        '''splitting dataframe into training and test set'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size=0.1, \n",
    "                                                            random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def _drop_col_id_target(self, df_feat):\n",
    "        '''dropping col id and target from features and creating target dataframe'''\n",
    "        X = df_feat.drop(col_target, axis=1)\n",
    "        X = X.drop(col_id, axis=1)\n",
    "        y = df_feat[col_target]\n",
    "        return X, y\n",
    "    \n",
    "    def _apply_downsampling(self, X, y):\n",
    "        '''down sampling majority class'''\n",
    "        if bool_apply_downsampling:\n",
    "            len_y_one = len(y[y==1])\n",
    "            sampler = RandomUnderSampler(random_state=42, ratio={0:95000, 1:len_y_one})\n",
    "            X, y = sampler.fit_sample(X, y)\n",
    "            print(\"downsampling applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_downsampling set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_smote(self, X, y):\n",
    "        '''applying smote to split training set'''\n",
    "        if bool_apply_smote:\n",
    "            sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "            X, y = sm.fit_sample(X, y)\n",
    "            print(\"smote applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_smote set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_pca(self, X, y):\n",
    "        '''applying PCA and creating train and test set'''\n",
    "        if bool_apply_pca:\n",
    "            X = self._pca(X)\n",
    "            print('pca applied to training set, then test set.')\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_pca set to false.\")\n",
    "            return X, y\n",
    "\n",
    "    def _pca(self, X):\n",
    "        '''applying pca features dataframe'''\n",
    "        scaled_X = StandardScaler().fit_transform(X)\n",
    "        pca = PCA(n_components=250) #set value\n",
    "        pcomponents = pca.fit_transform(scaled_X)\n",
    "        X_pca = pd.DataFrame(data=pcomponents)\n",
    "        return X_pca\n",
    "        \n",
    "    def _convert_to_matrix(self):\n",
    "        '''converting X_test to matrix so columns match X_train'''\n",
    "        if bool_apply_downsampling or bool_apply_smote:\n",
    "            mod.X_test = pd.DataFrame(mod.X_test.values)\n",
    "            \n",
    "    def create_test_feature(self, bool_drop_col, col):\n",
    "        '''creates correllated ratio to target column'''\n",
    "        df_feat = df_features.copy()        \n",
    "        df_feat = self._calculate_target_perc(col, df_feat) \n",
    "        df_feat = self._map_col(col, df_feat)\n",
    "        df_feat = self._create_ratio(df_feat)\n",
    "        df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat\n",
    "    \n",
    "    def create_feature(self, bool_drop_col, list_col):\n",
    "        '''creating new feature'''\n",
    "        df_feat = self.df_feat       \n",
    "        for col in list_col:\n",
    "            df_feat = self._check_col_exist(col, df_feat)\n",
    "            df_feat = self._calculate_target_perc(col, df_feat) \n",
    "            df_feat = self._map_col(col, df_feat)\n",
    "            df_feat = self._create_ratio(df_feat)\n",
    "            df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat \n",
    "    \n",
    "    def _check_col_exist(self, col, df_feat):\n",
    "        '''recreates original column from original dataframe'''\n",
    "        if col not in df_feat.columns:\n",
    "            df_feat[col] = df_raw[col]\n",
    "            df_feat[col] = self._fill_na(df_feat, col)\n",
    "            df_feat[col] = self._label_encode(df_feat, col)\n",
    "        return df_feat\n",
    "    \n",
    "    def _label_encode(self, df_feat, col):\n",
    "        '''label encoding columns pulled from original df_raw'''\n",
    "        le = LabelEncoder()\n",
    "        df_feat[col] = le.fit_transform(df_feat[col])\n",
    "        return df_feat[col]\n",
    "    \n",
    "    def _drop_column(self, bool_drop_col, col, df_feat):\n",
    "        '''dropping or keeping columns'''\n",
    "        if bool_drop_col:\n",
    "            if (col in df_features.columns):    \n",
    "                self.list_drop_col.append(col) \n",
    "        else:\n",
    "            print(\"keeping original feature\", col)\n",
    "        return df_feat\n",
    "\n",
    "    def aggregate_features(self, list_col, val_aggreg):\n",
    "        for col in list_col:\n",
    "            df_groupby = self.df_raw.groupby(col).mean()\n",
    "            dict_aggreg_col = df_groupby[[val_aggreg]].to_dict()\n",
    "            self.df_feat[col + '_fe'] = self.df_raw[col].map(dict_aggreg_col['TransactionAmt'])\n",
    "            col_mode = self.df_feat[col + '_fe'].mode()[0]\n",
    "            self.df_feat[col + '_fe'] = self.df_feat[col + '_fe'].fillna(col_mode)\n",
    "\n",
    "    def _calculate_target_perc(self, col_val, df_feat):\n",
    "        '''calculate fraud percentage for each column'''\n",
    "        list_perc = []\n",
    "        dict_feat = {}\n",
    "        unique_col_values = df_feat[col_val].unique()\n",
    "        for val in unique_col_values:\n",
    "            list_perc = self._append_fraud_percentage(df_feat, col_val, val, list_perc)    \n",
    "        self._create_dict(col_val, list_perc, unique_col_values)\n",
    "        return df_feat\n",
    "    \n",
    "    def _append_fraud_percentage(self, df_feat, col_val, val, list_perc):\n",
    "        '''calculating fraud percentage and adding to list'''\n",
    "        fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                            & (df_feat[col_target]==1)].shape[0]\n",
    "        non_fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==0)].shape[0]\n",
    "        if (non_fraud_total==0):\n",
    "            list_perc.append(0)\n",
    "        else: \n",
    "            list_perc.append(fraud_total/non_fraud_total)\n",
    "        return list_perc\n",
    "\n",
    "    def _create_dict(self, col_val, list_perc, unique_col_values):\n",
    "        '''create dictionary for original values to new fraud percent values'''\n",
    "        col_name = col_val + '_fraud_perc'\n",
    "        series_perc = pd.Series(list_perc, name=col_name)\n",
    "        series_col = pd.Series(unique_col_values, name=col_val)\n",
    "        df_feat = pd.concat([series_col, series_perc], axis=1)\n",
    "        df_feat = df_feat.sort_values(col_name, ascending=False) \n",
    "        dict_feat = df_feat.set_index(col_val).to_dict()\n",
    "        self.dict_all_feat.update(dict_feat)\n",
    "\n",
    "    def _map_col(self, col, df_feat):\n",
    "        '''map dictionary values to new features'''\n",
    "        dict_keys = self.dict_all_feat.keys()\n",
    "        for val in dict_keys:\n",
    "            df_feat[col + '_fe'] = df_feat[col].map(self.dict_all_feat[val])\n",
    "            self.new_col.append(col + '_fe')\n",
    "        return df_feat\n",
    "            \n",
    "    def _create_ratio(self, df_feat):\n",
    "        '''finalize new features with ranking values'''\n",
    "        for val in self.new_col:\n",
    "            col_min_val = df_feat[df_feat[val] > 0][val].min()\n",
    "            df_feat[val] = df_feat[val]/col_min_val\n",
    "        self.new_col = []\n",
    "        return df_feat\n",
    "    \n",
    "mod = Model()\n",
    "fe = FeatureEngineering()\n",
    "\n",
    "bool_predict_proba = False\n",
    "bool_thres_cost = False\n",
    "bool_apply_pca = False\n",
    "bool_apply_smote = True\n",
    "bool_apply_downsampling = True\n",
    "\n",
    "bool_create_tuning_df = True\n",
    "bool_drop_col = True\n",
    "df_feat_1000 = False\n",
    "fe.final_features(bool_drop_col, list_feat=['addr1','addr2','card2','card3','C1','P_emaildomain', \n",
    "                                            'card6', 'V294','V279','C14','V306','D2','D10'])\n",
    "bool_drop_col = False\n",
    "fe.final_features(bool_drop_col, list_feat=['card5', 'V317', 'V69', 'D1','D3','D4','D11'])\n",
    "fe.list_drop_col.append('C4')\n",
    "fe.create_final_df()\n",
    "\n",
    "# fe.feature_testing(bool_drop_col, list_feat=['addr1'])\n",
    "\n",
    "# NEXT, we need to fix our tuning dataframe. method, decide how we are going to tune our model, then \n",
    "# we also need to decide how this effects our results. Should we tune on the hold outset? If we applied\n",
    "# smote and undersampling, etc. how does this effect our outcome?\n",
    "# pca did not work properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('X_train', mod.X_train.shape)\n",
    "# print('y_train', mod.y_train.shape)\n",
    "# print('X_test', mod.X_test.shape)\n",
    "# print('y_test', mod.y_test.shape)\n",
    "# print('y_train==1', np.sum(mod.y_train[mod.y_train==0].isnull()))\n",
    "# print('y_train==0', np.sum(mod.y_train[mod.y_train==1].isnull()))\n",
    "# we need to test tuning on the entire dataframe and also the split version we have created. \n",
    "# lets test using logisticregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "mod.X_train = pd.DataFrame(mod.X_train)\n",
    "mod.y_train = pd.DataFrame(mod.y_train)\n",
    "mod.X_test = pd.DataFrame(mod.X_test)\n",
    "mod.y_test = pd.DataFrame(mod.y_test)\n",
    "mod.X_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv')\n",
    "mod.y_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv')\n",
    "mod.X_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv')\n",
    "mod.y_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv')\n",
    "mod.X_features = pd.DataFrame(mod.X_features)\n",
    "mod.y_target = pd.DataFrame(mod.y_target)\n",
    "mod.X_features.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv')\n",
    "mod.y_target.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "mod.X_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.X_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv').drop('Unnamed: 0',axis=1)\n",
    "\n",
    "# mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv').drop('Unnamed: 0', axis=1)\n",
    "# mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv').drop('Unnamed: 0', axis=1)\n",
    "# mod.X_features.info(memory_usage='deep')\n",
    "# mod.y_target.info(memory_usage='deep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = True\n",
    "# model_current = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing LogisticRegression\n",
    "# bool_predict_proba = False\n",
    "# model_current =  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
    "#                                    intercept_scaling=0.1, l1_ratio=1e-06, max_iter=150,\n",
    "#                                    multi_class='multinomial', n_jobs=-1, penalty='none',\n",
    "#                                    random_state=42, solver='lbfgs', tol=1e-05, verbose=0,\n",
    "#                                    warm_start=False)\n",
    "# mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=0.1, l1_ratio=0.01, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='none',\n",
      "                   random_state=42, solver='newton-cg', tol=1e-06, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-73b935177bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                    warm_start=False)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_df_score_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-158-24fba75d6061>\u001b[0m in \u001b[0;36mcreate_df_score_model\u001b[0;34m(self, model_current)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m'''scores model'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting model:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-24fba75d6061>\u001b[0m in \u001b[0;36madd_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m'''fitting model and calculating time elapsed'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1606\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing LogisticRegression\n",
    "bool_predict_proba = True\n",
    "model_current =  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                                   intercept_scaling=0.1, l1_ratio=0.01, max_iter=100,\n",
    "                                   multi_class='auto', n_jobs=-1, penalty='none',\n",
    "                                   random_state=42, solver='newton-cg', tol=1e-06, verbose=0,\n",
    "                                   warm_start=False)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING XGBClassifier n_estimators=150\n",
    "bool_predict_proba = False\n",
    "model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "                              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "                              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "                              min_child_weight=1, missing=None, n_estimators=150, n_jobs=-1,\n",
    "                              nthread=None, objective='binary:logistic', random_state=42,\n",
    "                              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "                              silent=None, subsample=0.5, verbosity=1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing XGBClassifier 200 n_estimators\n",
    "model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "                              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "                              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "                              min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,\n",
    "                              nthread=None, objective='binary:logistic', random_state=42,\n",
    "                              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "                              silent=None, subsample=0.5, verbosity=1)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing RandomForestClassifier\n",
    "model_current = RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                       criterion='gini', max_depth=5, max_features='auto',\n",
    "                                       max_leaf_nodes=9,\n",
    "                                       min_impurity_decrease=0.1, min_impurity_split=None,\n",
    "                                       min_samples_leaf=4, min_samples_split=3,\n",
    "                                       min_weight_fraction_leaf=0.3, n_estimators=100,\n",
    "                                       n_jobs=-1, oob_score=False, random_state=42, verbose=1,\n",
    "                                       warm_start=False)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing RandomForestClassifier n_estimators=150\n",
    "model_current = RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                       criterion='gini', max_depth=5, max_features='auto',\n",
    "                                       max_leaf_nodes=9,\n",
    "                                       min_impurity_decrease=0.1, min_impurity_split=None,\n",
    "                                       min_samples_leaf=4, min_samples_split=3,\n",
    "                                       min_weight_fraction_leaf=0.3, n_estimators=150,\n",
    "                                       n_jobs=-1, oob_score=False, random_state=42, verbose=1,\n",
    "                                       warm_start=False)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing DecisionTreeClassifier max_depth=11\n",
    "model_current = DecisionTreeClassifier(class_weight='balanced',\n",
    "                                       criterion='entropy', max_depth=11, max_features=None,\n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                       min_impurity_split=None, min_samples_leaf=9,\n",
    "                                       min_samples_split=5, min_weight_fraction_leaf=0,\n",
    "                                       presort='auto', random_state=42, splitter='best')\n",
    "\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing DecisionTreeClassifier max_depth=13\n",
    "model_current = DecisionTreeClassifier(class_weight='balanced',\n",
    "                                       criterion='entropy', max_depth=13, max_features=None,\n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                       min_impurity_split=None, min_samples_leaf=9,\n",
    "                                       min_samples_split=5, min_weight_fraction_leaf=0,\n",
    "                                       presort='auto', random_state=42, splitter='best')\n",
    "\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# base score LogisticRegression threshold\n",
    "bool_predict_proba = True\n",
    "model_current = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)\n",
    "bool_predict_proba = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model LogisticRegression\n",
    "model_current = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base DecisionTreeClassifier\n",
    "bool_predict_proba = False\n",
    "model_current = DecisionTreeClassifier(random_state=42)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base RandomForestClassifier\n",
    "bool_predict_proba = False\n",
    "model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base XGBClassifier threshold\n",
    "bool_predict_proba = True\n",
    "model_current = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base XGBClassifier\n",
    "bool_predict_proba = False\n",
    "model_current = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba CatBoostClassifier\n",
    "bool_predict_proba = True\n",
    "model_current = CatBoostClassifier(random_state=42, verbose=0)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base CatBoostClassifier\n",
    "model_current = CatBoostClassifier(random_state=42, verbose=0)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = True\n",
    "# model_current = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
    "#                    intercept_scaling=0.1, l1_ratio=1e-06, max_iter=150,\n",
    "#                    multi_class='multinomial', n_jobs=-1, penalty='none',\n",
    "#                    random_state=42, solver='lbfgs', tol=1e-05, verbose=0,\n",
    "#                    warm_start=False)\n",
    "# mod.create_df_score_model(model_current)\n",
    "# bool_predict_proba = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # newly tuned LR\n",
    "# bool_predict_proba = True\n",
    "# model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "#               colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
    "#               nthread=None, objective='binary:logistic', random_state=42,\n",
    "#               reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "#               silent=None, subsample=0.5, verbosity=1)\n",
    "# mod.create_df_score_model(model_current)\n",
    "# bool_predict_proba = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = True\n",
    "# model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "#               colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
    "#               nthread=None, objective='binary:logistic', random_state=42,\n",
    "#               reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "#               silent=None, subsample=0.5, verbosity=1)\n",
    "# mod.create_df_score_model(model_current)\n",
    "# bool_predict_proba = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.X_train = pd.DataFrame()\n",
    "        self.y_train = pd.DataFrame()\n",
    "        self.X_test = pd.DataFrame()\n",
    "        self.y_test = pd.DataFrame()\n",
    "        self.X_features = pd.DataFrame()\n",
    "        self.y_target = pd.DataFrame()\n",
    "        self.dict_results = {'index':[0], 'threshold':[], 'roc_auc_score':[], \n",
    "                             'total_cost':[], 'fn':[], 'fp':[]}\n",
    "        \n",
    "        self.X_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv').drop('Unnamed: 0',axis=1)\n",
    "        self.y_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv').drop('Unnamed: 0',axis=1)\n",
    "        self.X_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv').drop('Unnamed: 0',axis=1)\n",
    "        self.y_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv').drop('Unnamed: 0',axis=1)\n",
    "        self.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv').drop('Unnamed: 0',axis=1)\n",
    "        self.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv').drop('Unnamed: 0',axis=1)\n",
    "        \n",
    "    def create_df_score_model(self, model_current):\n",
    "        '''scores model'''\n",
    "        print(\"Fitting model:\\n\", model_current)\n",
    "        y_pred, elapsed_time = self.add_model(model_current) \n",
    "        df_scores, df_temp, y_pred = self._score_model(y_pred, elapsed_time)\n",
    "        self._save_results(df_scores, df_temp, y_pred)\n",
    "        self._feature_importance(model_current)\n",
    "        fe.col_fe = []\n",
    "        \n",
    "    def add_model(self, model):        \n",
    "        '''fitting model and calculating time elapsed'''\n",
    "        start_time = time.time()\n",
    "        model.fit(mod.X_train, mod.y_train)\n",
    "        y_pred = self._predict(model)\n",
    "        bool_predict_proba = False\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        return y_pred, elapsed_time\n",
    "    \n",
    "    def _predict(self, model):\n",
    "        '''make prediction'''\n",
    "        if bool_predict_proba:\n",
    "            y_pred = self._predict_proba(model)\n",
    "            y_pred = self._predict_proba_threshold(y_pred)\n",
    "            return y_pred \n",
    "        else:\n",
    "            y_pred = model.predict(mod.X_test)\n",
    "            return y_pred\n",
    "        \n",
    "    def _predict_proba(self, model):\n",
    "        try:\n",
    "            y_pred_prob = model.predict_proba(mod.X_test)\n",
    "            return y_pred_prob\n",
    "        except:\n",
    "            print(\"Model does not have predict_proba attribute.\")\n",
    "\n",
    "    def _predict_proba_threshold(self, y_pred):\n",
    "        '''create prediction for each threshold'''\n",
    "        df_results = pd.DataFrame()\n",
    "        list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "                          .35, .4, .45, .5, .55, .6]\n",
    "        for threshold in list_threshold:\n",
    "            df_temp = self._compute_thres_df(y_pred, threshold)\n",
    "            df_results = pd.concat([df_results, df_temp], axis=0)\n",
    "        df_results = df_results.drop('index', axis=1).reset_index(drop=True)\n",
    "        val_thres_cost, val_thres_auc = self._calc_and_plot_results(df_results)\n",
    "        y_pred_class = self._y_pred_class(y_pred, val_thres_cost, val_thres_auc)\n",
    "        print('threshold results dataframe:\\n', df_results)\n",
    "        return y_pred_class\n",
    "        \n",
    "    def _compute_thres_df(self, y_pred_prob, threshold):\n",
    "        '''compute the values for each threshold'''\n",
    "        y_pred_class = binarize(y_pred_prob, threshold)[:,1]\n",
    "        df_conf_matrix = self._compute_conf_matrix(y_pred_class)\n",
    "        df_dict_results = self._compute_results(df_conf_matrix, y_pred_class, threshold)\n",
    "        return df_dict_results\n",
    "    \n",
    "    def _compute_conf_matrix(self, y_pred_class):\n",
    "        '''compute the confusion matrix for the current threshold'''\n",
    "        conf_matr = confusion_matrix(self.y_test, y_pred_class)\n",
    "        df_conf_matrix = pd.DataFrame(conf_matr)\n",
    "        return df_conf_matrix\n",
    "    \n",
    "    def _compute_results(self, df_conf_matrix, y_pred_class, threshold):\n",
    "        '''compute results for each threshold and save to dictionary'''\n",
    "        val_fn = df_conf_matrix[0][1]\n",
    "        val_fp = df_conf_matrix[1][0]\n",
    "        val_cost = 3000*val_fn + 20*val_fp\n",
    "        val_score = roc_auc_score(self.y_test, y_pred_class)\n",
    "        self.dict_results.update([('threshold', threshold)])\n",
    "        self.dict_results.update([('roc_auc_score', val_score)])\n",
    "        self.dict_results.update([('total_cost', val_cost)])\n",
    "        self.dict_results.update([('fn', val_fn)])\n",
    "        self.dict_results.update([('fp', val_fp)])\n",
    "        df_dict_results = pd.DataFrame.from_dict(self.dict_results)\n",
    "        return df_dict_results\n",
    "    \n",
    "    def _calc_and_plot_results(self, df_results):\n",
    "        '''plot results and calculate best threshold for auc and cost'''\n",
    "        self._plot_thres_auc(df_results)\n",
    "        self._plot_thres_score(df_results)\n",
    "        val_thres_cost = self._calc_best_thres_cost(df_results)\n",
    "        val_thres_auc = self._calc_best_thres_auc(df_results)\n",
    "        return val_thres_cost, val_thres_auc\n",
    "        \n",
    "    def _plot_thres_auc(self, df_results):\n",
    "        '''plot auc roc score by threshold'''\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.title(\"Threshold by Auc Roc Score\", fontsize=14)\n",
    "        plt.xlabel('Threshold', fontsize=13)\n",
    "        plt.ylabel(\"Auc Roc Score\", fontsize=13)\n",
    "        plt.plot('threshold', 'roc_auc_score', data=df_results)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_thres_score(self, df_results):\n",
    "        '''plot cost by threshold'''\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.title(\"Threshold by Cost\", fontsize=14)\n",
    "        plt.xlabel('Threshold', fontsize=13)\n",
    "        plt.ylabel(\"Cost in Dollars\", fontsize=13)\n",
    "        plt.plot('threshold', 'total_cost', data=df_results)\n",
    "        plt.show()\n",
    "    \n",
    "    def _calc_best_thres_cost(self, df_results):\n",
    "        '''calculate the best threshold by cost'''\n",
    "        val_min_cost = df_results.total_cost.min()\n",
    "        index_min_cost = df_results[df_results.total_cost==val_min_cost].index[0]\n",
    "        val_threshold_min_cost = df_results.loc[index_min_cost,:].threshold.item()\n",
    "        print(\"Best threshold by cost:\", val_threshold_min_cost)\n",
    "        return val_threshold_min_cost\n",
    "\n",
    "    def _calc_best_thres_auc(self, df_results):\n",
    "        '''calculate the best threshold by highest auc roc score'''\n",
    "        val_max_roc_auc = df_results.roc_auc_score.max()\n",
    "        index_max_roc_auc = df_results[df_results.roc_auc_score==val_max_roc_auc].index[0]\n",
    "        val_threshold_roc_auc = df_results.loc[index_max_roc_auc,:].threshold.item()\n",
    "        print(\"Best threshold by roc auc score:\", val_threshold_roc_auc, '\\n')\n",
    "        return val_threshold_roc_auc\n",
    "    \n",
    "    def _y_pred_class(self, y_pred_prob, val_thres_cost, val_thres_auc):\n",
    "        '''calculate y_pred_class depending on if we want cost or auc threshold'''\n",
    "        if bool_thres_cost:\n",
    "            y_pred_class = binarize(y_pred_prob, val_thres_cost)[:,1]\n",
    "            return y_pred_class\n",
    "        else:\n",
    "            y_pred_class = binarize(y_pred_prob, val_thres_auc)[:,1]\n",
    "            return y_pred_class\n",
    "\n",
    "    def _score_model(self, y_pred, elapsed_time):      \n",
    "        '''creating dataframe with score results'''\n",
    "        col_recall, col_precision, col_time = self._calc_scores(y_pred, elapsed_time)        \n",
    "        df_conf_matrix = self._confusion_matrix(y_pred)\n",
    "        df_temp = pd.concat([col_recall, col_precision, df_conf_matrix, col_time], axis=1)\n",
    "        if fe.col_fe:\n",
    "            df_temp = self._concat_new_feat(df_temp)\n",
    "        df_scores = self._read_create_score_file(df_temp)\n",
    "        return df_scores, df_temp, y_pred\n",
    "\n",
    "    def _calc_scores(self, y_pred, elapsed_time):\n",
    "        '''calculating recall, precision and elapsed time'''\n",
    "        col_recall = pd.Series(recall_score(mod.y_test, y_pred), name='recall')\n",
    "        col_precision = pd.Series(precision_score(mod.y_test, y_pred), name='precision')\n",
    "        col_time = pd.Series(elapsed_time, name='time_elapsed (min)')\n",
    "        print('\\nroc auc score:', roc_auc_score(mod.y_test, y_pred), '\\n')\n",
    "        return col_recall, col_precision, col_time\n",
    "    \n",
    "    def _confusion_matrix(self, y_pred):\n",
    "        '''creating confusion matrix dataframe'''\n",
    "        df_conf_matrix = pd.DataFrame(confusion_matrix(mod.y_test, y_pred))\n",
    "        val_tp = pd.Series(df_conf_matrix[0][0], name='tp')\n",
    "        val_fn = pd.Series(df_conf_matrix[0][1], name='fn')\n",
    "        val_fp = pd.Series(df_conf_matrix[1][0], name='fp')\n",
    "        val_tn = pd.Series(df_conf_matrix[1][1], name='tn')\n",
    "        return pd.concat([val_fn, val_fp, val_tp, val_tn], axis=1)\n",
    "\n",
    "    def _concat_new_feat(self, df_temp):\n",
    "        '''concatenate scoring results'''        \n",
    "        print(\"\\nThe following new features have been created:\", fe.col_fe, '\\n')\n",
    "        if len(fe.col_fe) > 1: \n",
    "            fe.col_fe = \"model score\"\n",
    "        col_fe = pd.Series(fe.col_fe, name='feat_tested')\n",
    "        return pd.concat([col_fe, df_temp], axis=1)\n",
    "    \n",
    "    def _read_create_score_file(self, df_temp):\n",
    "        '''reading or creating df_scores file'''\n",
    "        try: \n",
    "            df_scores = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "            df_scores = df_scores.drop('Unnamed: 0', axis=1)\n",
    "        except:\n",
    "            print(\"\\nCreating df_scores.csv file.\") \n",
    "            df_scores = df_temp\n",
    "        return df_scores\n",
    "            \n",
    "    def _save_results(self, df_scores, df_temp, y_pred):\n",
    "        '''printing scores for new features'''            \n",
    "        df_scores = pd.concat([df_scores, df_temp], axis=0)\n",
    "        df_scores.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "        classif_report = classification_report(mod.y_test, y_pred)\n",
    "        self._print_summary(classif_report, df_scores)\n",
    "        self._save_summary(classif_report)\n",
    "\n",
    "    def _print_summary(self, classif_report, df_scores):\n",
    "        '''print last 5 rows of previous score results'''\n",
    "        print(classif_report)\n",
    "        print('\\ndf_scores:\\n\\n', df_scores.tail(5))\n",
    "    \n",
    "    def _save_summary(self, classif_report):\n",
    "        '''save score result summary to text file'''\n",
    "        file_summary = open('/Users/krahman/work/fraud_detection/saved_files/df_scores_summary.txt', \"a\")\n",
    "        file_summary.write('New features created from: ' \n",
    "                           + fe.str_list_col_fe \n",
    "                           + '\\n')\n",
    "        file_summary.write(classif_report)\n",
    "        file_summary.close()\n",
    "        \n",
    "    def _feature_importance(self, model):\n",
    "        '''create feature importance dataframe and bar plot'''\n",
    "        try:\n",
    "            df_feat_rank = self._feat_import_create_df(model)\n",
    "            self._feat_import_create_plot(df_feat_rank)\n",
    "            print(df_feat_rank[0:10].reset_index(drop=True))\n",
    "        except:\n",
    "            print(\"\\nmodel does not have _feature_importance attribute.\")\n",
    "        \n",
    "    def _feat_import_create_df(self, model):\n",
    "        '''creating dataframe of important features'''\n",
    "        col_name = pd.Series(fe.df_feat.columns, name='col')\n",
    "        col_feat_rank = pd.Series(model.feature_importances_, \n",
    "                                  name='feat_rank')\n",
    "        df_feat_rank = pd.concat([col_name, col_feat_rank], axis=1)\n",
    "        df_feat_rank = df_feat_rank.sort_values('feat_rank', ascending=False)\n",
    "        return df_feat_rank\n",
    "    \n",
    "    def _feat_import_create_plot(self, df_feat_rank):\n",
    "        '''create feature importance bar plot'''\n",
    "        plt.figure(figsize=(5,6))\n",
    "        sns.barplot(df_feat_rank.feat_rank[0:10],\n",
    "                    df_feat_rank.col[0:10],\n",
    "                    palette='Blues_d')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "        \n",
    "mod = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# base XGBClassifier threshold\n",
    "bool_predict_proba = True\n",
    "model_current = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base RandomForestClassifier\n",
    "model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_predict_proba = True\n",
    "model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning xgbc\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 7.5min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.986, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 14.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.985, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 28.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 34.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 40.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 46.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 53.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 59.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 3.1min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 2.1min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.2min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.966, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 2.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.800, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.796, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.799, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.805, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.798, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.807, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.990, total= 3.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.1min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 4.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.984, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total=14.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.987, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.3min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.3min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.2min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.7min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.961, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.959, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.961, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.964, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.960, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.963, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.960, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.7min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 5.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.985, total= 5.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 5.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.985, total= 7.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.5min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.5min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.982, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.982, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.985, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 3.0min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 3.1min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.983, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.985, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.8min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.983, total= 2.8min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 315.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 13\n",
      "Best learning_rate: 0.1\n",
      "Best booster: gbtree\n",
      "Best subsample: 0.5\n",
      "Best colsample_bytree: 0.7\n",
      "Best colsample_bylevel: 0.3\n",
      "Best colsample_bynode: 0.5\n",
      "Best reg_alpha: 3\n",
      "Best reg_lambda: 5\n",
      "Best scale_pos_weight: 5\n",
      "Best base_score: 0.4\n"
     ]
    }
   ],
   "source": [
    "### Tuning XGBClassifier READY ###\n",
    "print('tuning xgbc')\n",
    "xgbc = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "# learning_rate = [0,.1,.3,.5,.7,.9]\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "subsample = [.1,.3,.5,.7]\n",
    "colsample_bytree = [.1,.3,.5,.7]\n",
    "colsample_bylevel = [0,.1,.3,.5,.7,.9,1]\n",
    "colsample_bynode = [.1,.3,.5,.7]\n",
    "reg_alpha = [0,1,3,5,7]\n",
    "reg_lambda = [1,3,5,7]\n",
    "scale_pos_weight = [1,3,5,7]\n",
    "base_score = [.1,.2,.3,.4,.5]\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "#                        learning_rate=learning_rate, \n",
    "                       booster=booster, \n",
    "                       subsample=subsample, \n",
    "                       colsample_bytree=colsample_bytree, colsample_bylevel=colsample_bylevel, \n",
    "                       colsample_bynode=colsample_bynode, reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                       scale_pos_weight=scale_pos_weight,\n",
    "                       base_score=base_score\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(xgbc, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best booster:', best_model.best_estimator_.get_params()['booster'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n",
    "print('Best colsample_bytree:', best_model.best_estimator_.get_params()['colsample_bytree'])\n",
    "print('Best colsample_bylevel:', best_model.best_estimator_.get_params()['colsample_bylevel'])\n",
    "print('Best colsample_bynode:', best_model.best_estimator_.get_params()['colsample_bynode'])\n",
    "print('Best reg_alpha:', best_model.best_estimator_.get_params()['reg_alpha'])\n",
    "print('Best reg_lambda:', best_model.best_estimator_.get_params()['reg_lambda'])\n",
    "print('Best scale_pos_weight:', best_model.best_estimator_.get_params()['scale_pos_weight'])\n",
    "print('Best base_score:', best_model.best_estimator_.get_params()['base_score'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>395.304133</td>\n",
       "      <td>25.412146</td>\n",
       "      <td>0.587299</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985380</td>\n",
       "      <td>0.986789</td>\n",
       "      <td>0.987938</td>\n",
       "      <td>0.986694</td>\n",
       "      <td>0.987347</td>\n",
       "      <td>0.986711</td>\n",
       "      <td>0.987547</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>167.217239</td>\n",
       "      <td>11.165350</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.964751</td>\n",
       "      <td>0.967276</td>\n",
       "      <td>0.963401</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.966658</td>\n",
       "      <td>0.965024</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>120.322799</td>\n",
       "      <td>8.478442</td>\n",
       "      <td>0.271042</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961926</td>\n",
       "      <td>0.962940</td>\n",
       "      <td>0.965645</td>\n",
       "      <td>0.961338</td>\n",
       "      <td>0.963113</td>\n",
       "      <td>0.962412</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.963005</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>99.684576</td>\n",
       "      <td>5.295310</td>\n",
       "      <td>0.226211</td>\n",
       "      <td>0.090092</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>0.798554</td>\n",
       "      <td>0.805074</td>\n",
       "      <td>0.801674</td>\n",
       "      <td>0.804025</td>\n",
       "      <td>0.797907</td>\n",
       "      <td>0.806804</td>\n",
       "      <td>0.801659</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>203.073417</td>\n",
       "      <td>25.312860</td>\n",
       "      <td>0.677408</td>\n",
       "      <td>0.173201</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987628</td>\n",
       "      <td>0.988995</td>\n",
       "      <td>0.989606</td>\n",
       "      <td>0.988777</td>\n",
       "      <td>0.989180</td>\n",
       "      <td>0.988744</td>\n",
       "      <td>0.989257</td>\n",
       "      <td>0.988861</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>217.663624</td>\n",
       "      <td>223.572931</td>\n",
       "      <td>0.553119</td>\n",
       "      <td>0.219257</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984488</td>\n",
       "      <td>0.985504</td>\n",
       "      <td>0.986885</td>\n",
       "      <td>0.985129</td>\n",
       "      <td>0.985695</td>\n",
       "      <td>0.984936</td>\n",
       "      <td>0.986337</td>\n",
       "      <td>0.985598</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>103.800450</td>\n",
       "      <td>5.746303</td>\n",
       "      <td>0.394553</td>\n",
       "      <td>0.301245</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959114</td>\n",
       "      <td>0.960575</td>\n",
       "      <td>0.963978</td>\n",
       "      <td>0.959773</td>\n",
       "      <td>0.963111</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.962418</td>\n",
       "      <td>0.961344</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>305.320971</td>\n",
       "      <td>59.342621</td>\n",
       "      <td>0.606575</td>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985453</td>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.987005</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.986576</td>\n",
       "      <td>0.985673</td>\n",
       "      <td>0.987112</td>\n",
       "      <td>0.986233</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>102.538558</td>\n",
       "      <td>3.363380</td>\n",
       "      <td>0.421863</td>\n",
       "      <td>0.036955</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982312</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.984753</td>\n",
       "      <td>0.982607</td>\n",
       "      <td>0.983767</td>\n",
       "      <td>0.982724</td>\n",
       "      <td>0.984011</td>\n",
       "      <td>0.983344</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>173.911546</td>\n",
       "      <td>5.071884</td>\n",
       "      <td>0.338486</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982008</td>\n",
       "      <td>0.983266</td>\n",
       "      <td>0.984789</td>\n",
       "      <td>0.982414</td>\n",
       "      <td>0.983546</td>\n",
       "      <td>0.982560</td>\n",
       "      <td>0.983857</td>\n",
       "      <td>0.983167</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     395.304133     25.412146         0.587299        0.052609   \n",
       "1     167.217239     11.165350         0.489047        0.155241   \n",
       "2     120.322799      8.478442         0.271042        0.108111   \n",
       "3      99.684576      5.295310         0.226211        0.090092   \n",
       "4     203.073417     25.312860         0.677408        0.173201   \n",
       "5     217.663624    223.572931         0.553119        0.219257   \n",
       "6     103.800450      5.746303         0.394553        0.301245   \n",
       "7     305.320971     59.342621         0.606575        0.120821   \n",
       "8     102.538558      3.363380         0.421863        0.036955   \n",
       "9     173.911546      5.071884         0.338486        0.032476   \n",
       "\n",
       "  param_subsample param_scale_pos_weight param_reg_lambda param_reg_alpha  \\\n",
       "0             0.5                      3                5               5   \n",
       "1             0.7                      1                7               7   \n",
       "2             0.1                      7                3               7   \n",
       "3             0.5                      7                1               5   \n",
       "4             0.5                      5                5               3   \n",
       "5             0.7                      3                1               7   \n",
       "6             0.1                      7                7               3   \n",
       "7             0.1                      3                3               0   \n",
       "8             0.5                      3                5               7   \n",
       "9             0.3                      5                7               5   \n",
       "\n",
       "  param_max_depth param_colsample_bytree  ... split3_test_score  \\\n",
       "0               9                    0.5  ...          0.985380   \n",
       "1              11                    0.7  ...          0.963229   \n",
       "2               2                    0.7  ...          0.961926   \n",
       "3               9                    0.7  ...          0.802181   \n",
       "4              13                    0.7  ...          0.987628   \n",
       "5               9                    0.7  ...          0.984488   \n",
       "6               3                    0.3  ...          0.959114   \n",
       "7              13                    0.5  ...          0.985453   \n",
       "8              11                    0.1  ...          0.982312   \n",
       "9               7                    0.3  ...          0.982008   \n",
       "\n",
       "  split4_test_score split5_test_score split6_test_score split7_test_score  \\\n",
       "0          0.986789          0.987938          0.986694          0.987347   \n",
       "1          0.964751          0.967276          0.963401          0.966066   \n",
       "2          0.962940          0.965645          0.961338          0.963113   \n",
       "3          0.798554          0.805074          0.801674          0.804025   \n",
       "4          0.988995          0.989606          0.988777          0.989180   \n",
       "5          0.985504          0.986885          0.985129          0.985695   \n",
       "6          0.960575          0.963978          0.959773          0.963111   \n",
       "7          0.986445          0.987005          0.985547          0.986576   \n",
       "8          0.983491          0.984753          0.982607          0.983767   \n",
       "9          0.983266          0.984789          0.982414          0.983546   \n",
       "\n",
       "   split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.986711           0.987547         0.986829        0.000743   \n",
       "1           0.964305           0.966658         0.965024        0.001343   \n",
       "2           0.962412           0.964037         0.963005        0.001308   \n",
       "3           0.797907           0.806804         0.801659        0.003228   \n",
       "4           0.988744           0.989257         0.988861        0.000556   \n",
       "5           0.984936           0.986337         0.985598        0.000679   \n",
       "6           0.959996           0.962418         0.961344        0.001475   \n",
       "7           0.985673           0.987112         0.986233        0.000623   \n",
       "8           0.982724           0.984011         0.983344        0.000777   \n",
       "9           0.982560           0.983857         0.983167        0.000871   \n",
       "\n",
       "   rank_test_score  \n",
       "0                2  \n",
       "1                7  \n",
       "2                8  \n",
       "3               10  \n",
       "4                1  \n",
       "5                4  \n",
       "6                9  \n",
       "7                3  \n",
       "8                5  \n",
       "9                6  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### testing manual tuning ######\n",
    "### manual xbgc tuning\n",
    "max_depth = [3,5,7,9,11,13,15]\n",
    "# max_depth = [1,3,5]\n",
    "list_time_elapsed = []\n",
    "list_roc_auc_score = []\n",
    "for val in max_depth:\n",
    "    model_xgbc = XGBClassifier(max_depth=val, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_xgbc.fit(mod.X_train_test, mod.y_train)\n",
    "    y_pred_xgbc = model_xgbc.predict(mod.X_test)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    score_roc_auc = roc_auc_score(mod.y_test, y_pred_xgbc)\n",
    "\n",
    "    list_time_elapsed.append(elapsed_time)\n",
    "    list_roc_auc_score.append(score_roc_auc)\n",
    "    print('max depth: ', val)\n",
    "    print(confusion_matrix(mod.y_test, y_pred_xgbc))\n",
    "\n",
    "col_time_elapsed = pd.Series(list_time_elapsed)\n",
    "col_roc_score = pd.Series(list_roc_auc_score)\n",
    "col_max_depth = pd.Series(max_depth)\n",
    "df_results_xgbc = pd.concat([col_max_depth, col_roc_score,col_time_elapsed], \n",
    "                            keys=['max_depth', 'roc_auc_score', 'time_elap'], \n",
    "                            axis=1)\n",
    "print(df_results_xgbc)\n",
    "\n",
    "sns.lineplot(x='max_depth', y='roc_auc_score', data=df_results_xgbc)\n",
    "plt.title(\"XGBC manual tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Tuning RandomForestClassifier READY ####\n",
    "rfc = RandomForestClassifier(oob_score=False, n_jobs=1, random_state=42, verbose=1)\n",
    "\n",
    "n_estimators = [50,75,100,125,150,200]\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [3,5,7,9,11,13,15]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,2,4,6,8,10]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "max_leaf_nodes = [2,3,5,7,9,None]\n",
    "min_impurity_decrease = [0,.1,.3,.5,.7,.9]\n",
    "\n",
    "hyperparameters = dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split,\n",
    "                       min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                       min_impurity_decrease=min_impurity_decrease\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(rfc, hyperparameters, random_state=42, cv=5, verbose=5, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best max_leaf_nodes:', best_model.best_estimator_.get_params()['max_leaf_nodes'])\n",
    "print('Best min_impurity_decrease:', best_model.best_estimator_.get_params()['min_impurity_decrease'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### LR Tuning ####\n",
    "lr = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "\n",
    "penalty = ['l1', 'l2', 'elasticnet','none']\n",
    "tol = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "C = [1e-1,.2,.3,.5,.7,1]\n",
    "fit_intercept = [True,False]\n",
    "intercept_scaling = [1,.1,.01,.001]\n",
    "class_weight = ['balanced', None]\n",
    "solver = ['newton-cg', 'lbfgs', 'sag']#, 'liblinear','saga']\n",
    "max_iter = [50,75,100,150,200]\n",
    "multi_class = ['auto', 'ovr', 'multinomial']\n",
    "l1_ratio = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "\n",
    "\n",
    "hyperparameters = dict(penalty=penalty, \n",
    "#                        tol=tol, \n",
    "#                        C=C, \n",
    "#                        fit_intercept=fit_intercept,\n",
    "                       intercept_scaling=intercept_scaling, \n",
    "                       class_weight=class_weight,\n",
    "                       solver=solver, \n",
    "#                        max_iter=max_iter\n",
    "                       multi_class=multi_class, \n",
    "                       l1_ratio=l1_ratio\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best tol:', best_model.best_estimator_.get_params()['tol'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best fit_intercept:', best_model.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best intercept_scaling:', best_model.best_estimator_.get_params()['intercept_scaling'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best max_iter:', best_model.best_estimator_.get_params()['max_iter'])\n",
    "print('Best multi_class:', best_model.best_estimator_.get_params()['multi_class'])\n",
    "print('Best l1_ratio:', best_model.best_estimator_.get_params()['l1_ratio'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Tuning DTC READY ####\n",
    "dt = DecisionTreeClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_depth = [3,5,7,9,11]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,3,5,7,9]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "hyperparameters = dict(criterion=criterion, splitter=splitter, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                       min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, class_weight=class_weight\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(dt, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    " \n",
    "# best hyper parameters\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best splitter:', best_model.best_estimator_.get_params()['splitter'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tuning CatBoost READY ###\n",
    "# Tune learning rate manually.\n",
    "cbc = CatBoostClassifier(random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "learning_rate = [.1,.3,.5,.7,.9]\n",
    "# bagging_temperature = []\n",
    "subsample = [1,3,5,7]\n",
    "n_estimators = [50,75,100,150]\n",
    "depth = [2,4,6,8,10]\n",
    "grow_policy = ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "#                        learning_rate=learning_rate, \n",
    "#                        n_estimators=n_estimators,\n",
    "                       subsample=subsample,\n",
    "#                        depth=depth,\n",
    "                       grow_policy=grow_policy\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(cbc, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>feat_tested</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>time_elapsed (min)</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>623.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>0.607771</td>\n",
       "      <td>0.704599</td>\n",
       "      <td>3.812618</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>55986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>model score</td>\n",
       "      <td>842.0</td>\n",
       "      <td>9269.0</td>\n",
       "      <td>0.120254</td>\n",
       "      <td>0.600759</td>\n",
       "      <td>0.768942</td>\n",
       "      <td>1267.0</td>\n",
       "      <td>47676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>8597.0</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.594120</td>\n",
       "      <td>0.755315</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>48348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47787.0</td>\n",
       "      <td>0.041384</td>\n",
       "      <td>0.978189</td>\n",
       "      <td>5.028391</td>\n",
       "      <td>2063.0</td>\n",
       "      <td>9158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>574.0</td>\n",
       "      <td>15964.0</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.727833</td>\n",
       "      <td>1.004976</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>40981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>338.0</td>\n",
       "      <td>26593.0</td>\n",
       "      <td>0.062438</td>\n",
       "      <td>0.839734</td>\n",
       "      <td>3.194767</td>\n",
       "      <td>1771.0</td>\n",
       "      <td>30352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>8597.0</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.594120</td>\n",
       "      <td>0.773552</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>48348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>8597.0</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.594120</td>\n",
       "      <td>0.960058</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>48348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>8597.0</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.594120</td>\n",
       "      <td>0.819725</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>48348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>8597.0</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.594120</td>\n",
       "      <td>0.911682</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>48348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>419</td>\n",
       "      <td>model score</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.874910</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>51418.0</td>\n",
       "      <td>0.039042</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.797138</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>5527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>51418.0</td>\n",
       "      <td>0.039042</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.809128</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>5527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>51418.0</td>\n",
       "      <td>0.039042</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.897312</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>5527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.797645</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.882907</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.742118</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>1.009948</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.996657</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>9082.0</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.885854</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>47863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.0</td>\n",
       "      <td>31562.0</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.969671</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>25383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493.0</td>\n",
       "      <td>9481.0</td>\n",
       "      <td>0.145625</td>\n",
       "      <td>0.766240</td>\n",
       "      <td>33.358289</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>47464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.0</td>\n",
       "      <td>31562.0</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>1.004172</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>25383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.0</td>\n",
       "      <td>5546.0</td>\n",
       "      <td>0.244002</td>\n",
       "      <td>0.848743</td>\n",
       "      <td>3.416962</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>51399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>787.0</td>\n",
       "      <td>2741.0</td>\n",
       "      <td>0.325375</td>\n",
       "      <td>0.626837</td>\n",
       "      <td>4.003713</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>54204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450.0</td>\n",
       "      <td>8576.0</td>\n",
       "      <td>0.162091</td>\n",
       "      <td>0.786629</td>\n",
       "      <td>3.838663</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>48369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>354.0</td>\n",
       "      <td>3854.0</td>\n",
       "      <td>0.312890</td>\n",
       "      <td>0.832148</td>\n",
       "      <td>4.621600</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>53091.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>522.0</td>\n",
       "      <td>7036.0</td>\n",
       "      <td>0.184043</td>\n",
       "      <td>0.752489</td>\n",
       "      <td>0.725779</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>49909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.0</td>\n",
       "      <td>31562.0</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>1.062953</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>25383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>0.080581</td>\n",
       "      <td>0.565671</td>\n",
       "      <td>0.099089</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>43333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>535.0</td>\n",
       "      <td>8425.0</td>\n",
       "      <td>0.157416</td>\n",
       "      <td>0.746325</td>\n",
       "      <td>0.106405</td>\n",
       "      <td>1574.0</td>\n",
       "      <td>48520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577.0</td>\n",
       "      <td>6287.0</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.726411</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>50658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577.0</td>\n",
       "      <td>6287.0</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.726411</td>\n",
       "      <td>0.832118</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>50658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577.0</td>\n",
       "      <td>6287.0</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.726411</td>\n",
       "      <td>0.725939</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>50658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>756.0</td>\n",
       "      <td>17078.0</td>\n",
       "      <td>0.073409</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>0.140410</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>39867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>755.0</td>\n",
       "      <td>17122.0</td>\n",
       "      <td>0.073284</td>\n",
       "      <td>0.642010</td>\n",
       "      <td>0.378174</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>39823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577.0</td>\n",
       "      <td>6287.0</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.726411</td>\n",
       "      <td>0.812181</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>50658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450.0</td>\n",
       "      <td>8576.0</td>\n",
       "      <td>0.162091</td>\n",
       "      <td>0.786629</td>\n",
       "      <td>3.950133</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>48369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>0.080581</td>\n",
       "      <td>0.565671</td>\n",
       "      <td>0.074415</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>43333.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  feat_tested     fn       fp  precision    recall  \\\n",
       "409         409          NaN  623.0    959.0   0.607771  0.704599   \n",
       "410         410  model score  842.0   9269.0   0.120254  0.600759   \n",
       "411         411          NaN  856.0   8597.0   0.127208  0.594120   \n",
       "412         412          NaN   46.0  47787.0   0.041384  0.978189   \n",
       "413         413          NaN  574.0  15964.0   0.087719  0.727833   \n",
       "414         414          NaN  338.0  26593.0   0.062438  0.839734   \n",
       "415         415          NaN  856.0   8597.0   0.127208  0.594120   \n",
       "416         416          NaN  856.0   8597.0   0.127208  0.594120   \n",
       "417         417          NaN  856.0   8597.0   0.127208  0.594120   \n",
       "418         418          NaN  856.0   8597.0   0.127208  0.594120   \n",
       "419         419  model score  832.0   9082.0   0.123274  0.605500   \n",
       "420         420          NaN   20.0  51418.0   0.039042  0.990517   \n",
       "421         421          NaN   20.0  51418.0   0.039042  0.990517   \n",
       "422         422          NaN   20.0  51418.0   0.039042  0.990517   \n",
       "423         423          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "424         424          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "425         425          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "426         426          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "427         427          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "428         428          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "429         429          NaN  832.0   9082.0   0.123274  0.605500   \n",
       "430         430          NaN  437.0  31562.0   0.050310  0.792793   \n",
       "431         431          NaN  493.0   9481.0   0.145625  0.766240   \n",
       "432         432          NaN  437.0  31562.0   0.050310  0.792793   \n",
       "433         433          NaN  319.0   5546.0   0.244002  0.848743   \n",
       "434         434          NaN  787.0   2741.0   0.325375  0.626837   \n",
       "435         435          NaN  450.0   8576.0   0.162091  0.786629   \n",
       "436         436          NaN  354.0   3854.0   0.312890  0.832148   \n",
       "437         437          NaN  522.0   7036.0   0.184043  0.752489   \n",
       "438         438          NaN  437.0  31562.0   0.050310  0.792793   \n",
       "439         439          NaN  916.0  13612.0   0.080581  0.565671   \n",
       "440         440          NaN  535.0   8425.0   0.157416  0.746325   \n",
       "441         441          NaN  577.0   6287.0   0.195933  0.726411   \n",
       "442         442          NaN  577.0   6287.0   0.195933  0.726411   \n",
       "443         443          NaN  577.0   6287.0   0.195933  0.726411   \n",
       "444         444          NaN  756.0  17078.0   0.073409  0.641536   \n",
       "445         445          NaN  755.0  17122.0   0.073284  0.642010   \n",
       "446         446          NaN  577.0   6287.0   0.195933  0.726411   \n",
       "447         447          NaN  450.0   8576.0   0.162091  0.786629   \n",
       "448           0          NaN  916.0  13612.0   0.080581  0.565671   \n",
       "\n",
       "     time_elapsed (min)      tn       tp  \n",
       "409            3.812618  1486.0  55986.0  \n",
       "410            0.768942  1267.0  47676.0  \n",
       "411            0.755315  1253.0  48348.0  \n",
       "412            5.028391  2063.0   9158.0  \n",
       "413            1.004976  1535.0  40981.0  \n",
       "414            3.194767  1771.0  30352.0  \n",
       "415            0.773552  1253.0  48348.0  \n",
       "416            0.960058  1253.0  48348.0  \n",
       "417            0.819725  1253.0  48348.0  \n",
       "418            0.911682  1253.0  48348.0  \n",
       "419            0.874910  1277.0  47863.0  \n",
       "420            0.797138  2089.0   5527.0  \n",
       "421            0.809128  2089.0   5527.0  \n",
       "422            0.897312  2089.0   5527.0  \n",
       "423            0.797645  1277.0  47863.0  \n",
       "424            0.882907  1277.0  47863.0  \n",
       "425            0.827533  1277.0  47863.0  \n",
       "426            0.742118  1277.0  47863.0  \n",
       "427            1.009948  1277.0  47863.0  \n",
       "428            0.996657  1277.0  47863.0  \n",
       "429            0.885854  1277.0  47863.0  \n",
       "430            0.969671  1672.0  25383.0  \n",
       "431           33.358289  1616.0  47464.0  \n",
       "432            1.004172  1672.0  25383.0  \n",
       "433            3.416962  1790.0  51399.0  \n",
       "434            4.003713  1322.0  54204.0  \n",
       "435            3.838663  1659.0  48369.0  \n",
       "436            4.621600  1755.0  53091.0  \n",
       "437            0.725779  1587.0  49909.0  \n",
       "438            1.062953  1672.0  25383.0  \n",
       "439            0.099089  1193.0  43333.0  \n",
       "440            0.106405  1574.0  48520.0  \n",
       "441            0.804878  1532.0  50658.0  \n",
       "442            0.832118  1532.0  50658.0  \n",
       "443            0.725939  1532.0  50658.0  \n",
       "444            0.140410  1353.0  39867.0  \n",
       "445            0.378174  1354.0  39823.0  \n",
       "446            0.812181  1532.0  50658.0  \n",
       "447            3.950133  1659.0  48369.0  \n",
       "448            0.074415  1193.0  43333.0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_read = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "df_temp_read[len(df_temp_read)-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
