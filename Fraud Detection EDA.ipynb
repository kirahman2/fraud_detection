{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_raw = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "df_train = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = 'isFraud'\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v \n",
    "col_num.append(col_target)\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train.copy()\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "    \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 60:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.df_train = pp.impute_features()\n",
    "pp.df_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')\n",
    "df_features = df_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create features, target and split the dataframe\n",
    "# X = pp.df_train.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = pp.df_train[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after running final_features, run create_final_df.\n",
      "after running final_features, run create_final_df.\n",
      "keeping original feature card5\n",
      "keeping original feature V317\n",
      "keeping original feature V69\n",
      "keeping original feature D1\n",
      "keeping original feature D3\n",
      "keeping original feature D4\n",
      "keeping original feature D11\n",
      "dropping columns:  ['addr1', 'addr2', 'card2', 'card3', 'C1', 'V294', 'V279', 'C14', 'V306', 'D2', 'D10', 'C4']\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "bool_apply_pca set to false.\n",
      "bool_apply_pca set to false.\n",
      "creating tuning dataframe...\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "final dataframe created.\n",
      "To test a model use the mod.create_df_score_model(model_current)\n",
      "method where, for example, model_current=LogisticRegression().\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineering():\n",
    "    '''create new features for columns without ordinal values'''\n",
    "    def __init__(self):\n",
    "        self.list_fraud_perc = []\n",
    "        self.df_feat = df_features.copy()\n",
    "        self.df_raw = df_raw.copy()\n",
    "        \n",
    "        self.len_df_feat = self.df_feat.shape[0]\n",
    "        self.dict_all_feat = {}\n",
    "        self.new_col = []\n",
    "        \n",
    "        self.col = []\n",
    "        self.col_fe = []\n",
    "        self.df_new_feat = pd.DataFrame()\n",
    "        self.list_drop_col = []\n",
    "        self.str_list_col_fe = []\n",
    "        self.list_feat = []\n",
    "\n",
    "    def feature_testing(self, bool_drop_col, list_feat):\n",
    "        '''testing and scoring new potential features'''\n",
    "        print(\"While running feature_testing, do not run final_features.\")            \n",
    "        if list_feat:\n",
    "            for col in list_feat:\n",
    "                self.col_fe = col\n",
    "                bool_predict_proba = False\n",
    "                if col in df_features.columns:\n",
    "                    df_feat = self.create_test_feature(bool_drop_col, col)\n",
    "                    if df_feat_1000:\n",
    "                        df_feat = df_feat[0:1000]\n",
    "                    df_feat = df_feat.drop(self.list_drop_col[-1], axis=1)\n",
    "                    self._apply_df_transform(df_feat)\n",
    "                    model_lr = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "                    self._convert_list_to_string(list_feat)\n",
    "                    mod.create_df_score_model(model_lr)\n",
    "                else:\n",
    "                    print(\"\\nColumn\", col, \"does not exist in dataframe.\\n\")\n",
    "            self.col_fe = []\n",
    "        self.list_drop_col = []\n",
    "            \n",
    "    def final_features(self, bool_drop_col, list_feat):\n",
    "        '''creates final new features'''\n",
    "        print('after running final_features, run create_final_df.')\n",
    "        self.list_feat = list_feat\n",
    "        df_feat = self.create_feature(bool_drop_col, list_feat)  \n",
    "        if df_feat_1000:\n",
    "            df_feat = df_feat[0:1000]\n",
    "        for col in list_feat:\n",
    "            col_fe = self._append_col_lists(col)\n",
    "            df_feat[col] = self._fill_na(df_feat, col_fe)\n",
    "            self._concat_df_new_feat(df_feat, col_fe)\n",
    "        self._convert_list_to_string(list_feat)\n",
    "        return df_feat ### delete?\n",
    "    \n",
    "    def _append_col_lists(self, col):\n",
    "        '''appending columns and new feature column names'''\n",
    "        col_fe = col + '_fe'\n",
    "        self.col.append(col)\n",
    "        self.col_fe.append(col_fe)\n",
    "        return col_fe\n",
    "\n",
    "    def _fill_na(self, df_feat, col_fe):\n",
    "        '''fill na values for new features'''\n",
    "        col_mode = df_feat[col_fe].mode()[0]\n",
    "        return df_feat[col_fe].fillna(col_mode)\n",
    "        \n",
    "    def _concat_df_new_feat(self, df_feat, col_fe):\n",
    "        '''adding new feauture columns to one dataframe'''  \n",
    "        df_temp = df_feat[col_fe]\n",
    "        self.df_new_feat = pd.concat([self.df_new_feat, df_temp], axis=1)\n",
    "    \n",
    "    def _convert_list_to_string(self, list_feat):\n",
    "        '''convert list to string to print later'''\n",
    "        str_temp = ''\n",
    "        for val in list_feat:\n",
    "            str_temp = str_temp + val + ' '\n",
    "        self.str_list_col_fe = str_temp\n",
    "\n",
    "    def create_final_df(self):\n",
    "        '''creates final dataframe after creating final_features'''\n",
    "        df_feat = pd.concat([df_features, self.df_new_feat], axis=1)\n",
    "        if df_feat_1000:\n",
    "            df_feat = pd.concat([df_features[0:1000], self.df_new_feat], axis=1)\n",
    "        print('dropping columns: ', self.list_drop_col)\n",
    "        df_feat = df_feat.drop(self.list_drop_col, axis=1)\n",
    "        \n",
    "        self._apply_df_transform(df_feat)\n",
    "        \n",
    "        self._create_tuning_df(df_feat)\n",
    "        self.list_drop_col = []\n",
    "        self._final_df_summary()\n",
    "        \n",
    "    def _shuffle_df(self, X, y):\n",
    "        '''shuffle dataframe'''\n",
    "        y = pd.Series(y)\n",
    "        X = pd.DataFrame(X)\n",
    "        df_temp = pd.concat([X, y], keys=['features','target'], axis=1)\n",
    "        df_temp = shuffle(df_temp).reset_index(drop=True)\n",
    "        X = df_temp.features\n",
    "        y = df_temp.target\n",
    "        return X, y\n",
    "        \n",
    "    def _final_df_summary(self):\n",
    "        print(\"final dataframe created.\")\n",
    "        print(\"To test a model use the mod.create_df_score_model(model_current)\")\n",
    "        print(\"method where, for example, model_current=LogisticRegression().\")\n",
    "\n",
    "    def _apply_df_transform(self, df_feat):\n",
    "        '''create dataframe, apply pca, apply smote'''\n",
    "        self.df_feat = df_feat\n",
    "        X, y = self._drop_col_id_target(df_feat)\n",
    "        X_train, X_test, y_train, y_test = self._split_dataframe(X, y)\n",
    "        X_train, y_train = self._apply_downsampling(X_train, y_train) # apply only train set\n",
    "        X_train, y_train = self._apply_smote(X_train, y_train)        # apply only train set\n",
    "        X_train, y_train = self._shuffle_df(X_train, y_train)\n",
    "        \n",
    "        mod.X_train, mod.y_train = self._apply_pca(X_train, y_train)          # apply to train set\n",
    "        mod.X_test , mod.y_test = self._apply_pca(X_test, y_test) \n",
    "        self._convert_to_matrix()\n",
    "        \n",
    "    def _create_tuning_df(self, df_feat):\n",
    "        '''whole dataframe used for model tuning'''\n",
    "        if bool_create_tuning_df:\n",
    "            print(\"creating tuning dataframe...\")\n",
    "            X, y = self._drop_col_id_target(df_feat)\n",
    "            X, y = self._apply_downsampling(X, y)\n",
    "            X, y = self._apply_smote(X, y)\n",
    "            mod.X_features, mod.y_target = self._shuffle_df(X, y)\n",
    "        else:\n",
    "            print('bool_create_tuning_df set to false.')\n",
    "\n",
    "    def _split_dataframe(self, X, y):\n",
    "        '''splitting dataframe into training and test set'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size=0.1, \n",
    "                                                            random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def _drop_col_id_target(self, df_feat):\n",
    "        '''dropping col id and target from features and creating target dataframe'''\n",
    "        X = df_feat.drop(col_target, axis=1)\n",
    "        X = X.drop(col_id, axis=1)\n",
    "        y = df_feat[col_target]\n",
    "        return X, y\n",
    "    \n",
    "    def _apply_downsampling(self, X, y):\n",
    "        '''down sampling majority class'''\n",
    "        if bool_apply_downsampling:\n",
    "            len_y_one = len(y[y==1])\n",
    "            sampler = RandomUnderSampler(random_state=42, ratio={0:95000, 1:len_y_one})\n",
    "            X, y = sampler.fit_sample(X, y)\n",
    "            print(\"downsampling applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_downsampling set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_smote(self, X, y):\n",
    "        '''applying smote to split training set'''\n",
    "        if bool_apply_smote:\n",
    "            sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "            X, y = sm.fit_sample(X, y)\n",
    "            print(\"smote applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_smote set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_pca(self, X, y):\n",
    "        '''applying PCA and creating train and test set'''\n",
    "        if bool_apply_pca:\n",
    "            X = self._pca(X)\n",
    "            print('pca applied to training set, then test set.')\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_pca set to false.\")\n",
    "            return X, y\n",
    "\n",
    "    def _pca(self, X):\n",
    "        '''applying pca features dataframe'''\n",
    "        scaled_X = StandardScaler().fit_transform(X)\n",
    "        pca = PCA(n_components=250) #set value\n",
    "        pcomponents = pca.fit_transform(scaled_X)\n",
    "        X_pca = pd.DataFrame(data=pcomponents)\n",
    "        return X_pca\n",
    "        \n",
    "    def _convert_to_matrix(self):\n",
    "        '''converting X_test to matrix so columns match X_train'''\n",
    "        if bool_apply_downsampling or bool_apply_smote:\n",
    "            mod.X_test = pd.DataFrame(mod.X_test.values)\n",
    "            \n",
    "    def create_test_feature(self, bool_drop_col, col):\n",
    "        '''creates correllated ratio to target column'''\n",
    "        df_feat = df_features.copy()        \n",
    "        df_feat = self._calculate_target_perc(col, df_feat) \n",
    "        df_feat = self._map_col(col, df_feat)\n",
    "        df_feat = self._create_ratio(df_feat)\n",
    "        df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat\n",
    "    \n",
    "    def create_feature(self, bool_drop_col, list_col):\n",
    "        '''creating new feature'''\n",
    "        df_feat = self.df_feat       \n",
    "        for col in list_col:\n",
    "            df_feat = self._check_col_exist(col, df_feat)\n",
    "            df_feat = self._calculate_target_perc(col, df_feat) \n",
    "            df_feat = self._map_col(col, df_feat)\n",
    "            df_feat = self._create_ratio(df_feat)\n",
    "            df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat \n",
    "    \n",
    "    def _check_col_exist(self, col, df_feat):\n",
    "        '''recreates original column from original dataframe'''\n",
    "        if col not in df_feat.columns:\n",
    "            df_feat[col] = df_raw[col]\n",
    "            df_feat[col] = self._fill_na(df_feat, col)\n",
    "            df_feat[col] = self._label_encode(df_feat, col)\n",
    "        return df_feat\n",
    "    \n",
    "    def _label_encode(self, df_feat, col):\n",
    "        '''label encoding columns pulled from original df_raw'''\n",
    "        le = LabelEncoder()\n",
    "        df_feat[col] = le.fit_transform(df_feat[col])\n",
    "        return df_feat[col]\n",
    "    \n",
    "    def _drop_column(self, bool_drop_col, col, df_feat):\n",
    "        '''dropping or keeping columns'''\n",
    "        if bool_drop_col:\n",
    "            if (col in df_features.columns):    \n",
    "                self.list_drop_col.append(col) \n",
    "        else:\n",
    "            print(\"keeping original feature\", col)\n",
    "        return df_feat\n",
    "\n",
    "    def aggregate_features(self, list_col, val_aggreg):\n",
    "        for col in list_col:\n",
    "            df_groupby = self.df_raw.groupby(col).mean()\n",
    "            dict_aggreg_col = df_groupby[[val_aggreg]].to_dict()\n",
    "            self.df_feat[col + '_fe'] = self.df_raw[col].map(dict_aggreg_col['TransactionAmt'])\n",
    "            col_mode = self.df_feat[col + '_fe'].mode()[0]\n",
    "            self.df_feat[col + '_fe'] = self.df_feat[col + '_fe'].fillna(col_mode)\n",
    "\n",
    "    def _calculate_target_perc(self, col_val, df_feat):\n",
    "        '''calculate fraud percentage for each column'''\n",
    "        list_perc = []\n",
    "        dict_feat = {}\n",
    "        unique_col_values = df_feat[col_val].unique()\n",
    "        for val in unique_col_values:\n",
    "            list_perc = self._append_fraud_percentage(df_feat, col_val, val, list_perc)    \n",
    "        self._create_dict(col_val, list_perc, unique_col_values)\n",
    "        return df_feat\n",
    "    \n",
    "    def _append_fraud_percentage(self, df_feat, col_val, val, list_perc):\n",
    "        '''calculating fraud percentage and adding to list'''\n",
    "        fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                            & (df_feat[col_target]==1)].shape[0]\n",
    "        non_fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==0)].shape[0]\n",
    "        if (non_fraud_total==0):\n",
    "            list_perc.append(0)\n",
    "        else: \n",
    "            list_perc.append(fraud_total/non_fraud_total)\n",
    "        return list_perc\n",
    "\n",
    "    def _create_dict(self, col_val, list_perc, unique_col_values):\n",
    "        '''create dictionary for original values to new fraud percent values'''\n",
    "        col_name = col_val + '_fraud_perc'\n",
    "        series_perc = pd.Series(list_perc, name=col_name)\n",
    "        series_col = pd.Series(unique_col_values, name=col_val)\n",
    "        df_feat = pd.concat([series_col, series_perc], axis=1)\n",
    "        df_feat = df_feat.sort_values(col_name, ascending=False) \n",
    "        dict_feat = df_feat.set_index(col_val).to_dict()\n",
    "        self.dict_all_feat.update(dict_feat)\n",
    "\n",
    "    def _map_col(self, col, df_feat):\n",
    "        '''map dictionary values to new features'''\n",
    "        dict_keys = self.dict_all_feat.keys()\n",
    "        for val in dict_keys:\n",
    "            df_feat[col + '_fe'] = df_feat[col].map(self.dict_all_feat[val])\n",
    "            self.new_col.append(col + '_fe')\n",
    "        return df_feat\n",
    "            \n",
    "    def _create_ratio(self, df_feat):\n",
    "        '''finalize new features with ranking values'''\n",
    "        for val in self.new_col:\n",
    "            col_min_val = df_feat[df_feat[val] > 0][val].min()\n",
    "            df_feat[val] = df_feat[val]/col_min_val\n",
    "        self.new_col = []\n",
    "        return df_feat\n",
    "    \n",
    "mod = Model()\n",
    "fe = FeatureEngineering()\n",
    "\n",
    "bool_predict_proba = False\n",
    "\n",
    "bool_apply_pca = False\n",
    "bool_apply_smote = True\n",
    "bool_apply_downsampling = True\n",
    "\n",
    "bool_create_tuning_df = True\n",
    "bool_drop_col = True\n",
    "df_feat_1000 = False\n",
    "fe.final_features(bool_drop_col, list_feat=['addr1','addr2','card2','card3','C1','P_emaildomain', \n",
    "                                            'card6', 'V294','V279','C14','V306','D2','D10'])\n",
    "bool_drop_col = False\n",
    "fe.final_features(bool_drop_col, list_feat=['card5', 'V317', 'V69', 'D1','D3','D4','D11'])\n",
    "fe.list_drop_col.append('C4')\n",
    "fe.create_final_df()\n",
    "\n",
    "# fe.feature_testing(bool_drop_col, list_feat=['addr1'])\n",
    "\n",
    "# FN = 540, FP = 12598.0, pca, smote, downsampling\n",
    "# FN = 534.0, FP = 12535.0, pca, smote \n",
    "# FN = 577.0, FP = 22223.0, smote and downsampling only\n",
    "# FN = 360, FP = 30387.0, smote only\n",
    "# FN = 2103.0, FP = 36.0, undersampling only \n",
    "# FN = 508.0, FP = 8724.0, full, pca, smote\n",
    "# FN = 508, FP = 8745, full, pca, smote, undersample, 50,000 samples\n",
    "# FN = 510, FP = 8701, full, pca, smote, undersample, 25,000 samples\n",
    "# FN = 1093, FP = 734, full, pca, smote, undersample, 50,000 samples, correction\n",
    "# FN = 360, FP = 30387, smote\n",
    "# FN = 354, FP = 31090, smote, undersample\n",
    "# FN = 572, FP = 30741, smote, undersample, pca\n",
    "# FN = 485, FP = 30076, smote, undersample, pca, 100000\n",
    "# FN = , FP = , smote, undersample, pca, 95000, full\n",
    "# FN = 360, FP = 30387, no downsample, no pca, smote addr1\n",
    "# FN = 529, FP = 18023, smote, full\n",
    "# FN = 429, FP = 23106, smote, downsample, full\n",
    "# FN = 549, FP = 30123, smote, downsample, pca, full\n",
    "# FN = 545, FP = 30227, smote, downsample, pca, 290, full\n",
    "\n",
    "# NEXT, we need to fix our tuning dataframe. method, decide how we are going to tune our model, then \n",
    "# we also need to decide how this effects our results. Should we tune on the hold outset? If we applied\n",
    "# smote and undersampling, etc. how does this effect our outcome?\n",
    "\n",
    "# the tuning dataframe we define must upsample and downsample from whole dataset, we use that to \n",
    "# tune our data since during the process of tuning, we will have our hold out set in the CV. \n",
    "# Also, check time elap for non pca. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('X_train', mod.X_train.shape)\n",
    "# print('y_train', mod.y_train.shape)\n",
    "# print('X_test', mod.X_test.shape)\n",
    "# print('y_test', mod.y_test.shape)\n",
    "# print('y_train==1', np.sum(mod.y_train[mod.y_train==0].isnull()))\n",
    "# print('y_train==0', np.sum(mod.y_train[mod.y_train==1].isnull()))\n",
    "# we need to test tuning on the entire dataframe and also the split version we have created. \n",
    "# lets test using logisticregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "mod.X_train = pd.DataFrame(mod.X_train)\n",
    "mod.y_train = pd.DataFrame(mod.y_train)\n",
    "mod.X_test = pd.DataFrame(mod.X_test)\n",
    "mod.y_test = pd.DataFrame(mod.y_test)\n",
    "mod.X_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv')\n",
    "mod.y_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv')\n",
    "mod.X_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv')\n",
    "mod.y_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv')\n",
    "mod.X_features = pd.DataFrame(mod.X_features)\n",
    "mod.y_target = pd.DataFrame(mod.y_target)\n",
    "mod.X_features.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv')\n",
    "mod.y_target.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "mod.X_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.X_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv').drop('Unnamed: 0',axis=1)\n",
    "mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv').drop('Unnamed: 0',axis=1)\n",
    "\n",
    "# mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv').drop('Unnamed: 0', axis=1)\n",
    "# mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv').drop('Unnamed: 0', axis=1)\n",
    "# mod.X_features.info(memory_usage='deep')\n",
    "# mod.y_target.info(memory_usage='deep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2', random_state=42,\n",
      "                   solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "threshold:  0.1\n",
      "roc auc score: 0.5211168088662994\n",
      "confusion matrix:\n",
      " [[ 2594 54351]\n",
      " [    7  2102]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.5437876989521929\n",
      "confusion matrix:\n",
      " [[ 5527 51418]\n",
      " [   20  2089]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.5641492350288002\n",
      "confusion matrix:\n",
      " [[ 8548 48397]\n",
      " [   46  2063]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.58450200735647\n",
      "confusion matrix:\n",
      " [[11514 45431]\n",
      " [   70  2039]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.6020273819484507\n",
      "confusion matrix:\n",
      " [[14509 42436]\n",
      " [  107  2002]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.6204658767302316\n",
      "confusion matrix:\n",
      " [[17743 39202]\n",
      " [  149  1960]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.6391150262240095\n",
      "confusion matrix:\n",
      " [[21244 35701]\n",
      " [  200  1909]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.6543307970086348\n",
      "confusion matrix:\n",
      " [[25137 31808]\n",
      " [  280  1829]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.7008829487463073\n",
      "confusion matrix:\n",
      " [[35434 21511]\n",
      " [  465  1644]]\n",
      "roc auc score: 0.7008829487463073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76     56945\n",
      "           1       0.07      0.78      0.13      2109\n",
      "\n",
      "    accuracy                           0.63     59054\n",
      "   macro avg       0.53      0.70      0.45     59054\n",
      "weighted avg       0.95      0.63      0.74     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "391         NaN  437.0  31562.0   0.050310  0.792793            1.039733   \n",
      "392         NaN  357.0   4560.0   0.277567  0.830725            4.303023   \n",
      "393         NaN  367.0   2881.0   0.376812  0.825984           10.411890   \n",
      "394         NaN  916.0  13612.0   0.080581  0.565671            0.131308   \n",
      "0           NaN  465.0  21511.0   0.071000  0.779516            1.050254   \n",
      "\n",
      "         tn       tp  \n",
      "391  1672.0  25383.0  \n",
      "392  1752.0  52385.0  \n",
      "393  1742.0  54064.0  \n",
      "394  1193.0  43333.0  \n",
      "0    1644.0  35434.0  \n",
      "\n",
      "model does not have _feature_importance attribute.\n"
     ]
    }
   ],
   "source": [
    "bool_predict_proba = True\n",
    "model_current = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
      "                   intercept_scaling=0.1, l1_ratio=1e-06, max_iter=150,\n",
      "                   multi_class='multinomial', n_jobs=-1, penalty='none',\n",
      "                   random_state=42, solver='lbfgs', tol=1e-05, verbose=0,\n",
      "                   warm_start=False)\n",
      "roc auc score: 0.6192693439773956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.45      0.61     56945\n",
      "           1       0.05      0.79      0.09      2109\n",
      "\n",
      "    accuracy                           0.46     59054\n",
      "   macro avg       0.52      0.62      0.35     59054\n",
      "weighted avg       0.95      0.46      0.59     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "386  model score  357.0   4560.0   0.277567  0.830725            4.032026   \n",
      "387          NaN  359.0   3402.0   0.339674  0.829777            7.586321   \n",
      "388          NaN  359.0   3402.0   0.339674  0.829777            7.044470   \n",
      "389          NaN  359.0   3402.0   0.339674  0.829777            6.538398   \n",
      "0            NaN  437.0  31562.0   0.050310  0.792793            0.917979   \n",
      "\n",
      "         tn       tp  \n",
      "386  1752.0  52385.0  \n",
      "387  1750.0  53543.0  \n",
      "388  1750.0  53543.0  \n",
      "389  1750.0  53543.0  \n",
      "0    1672.0  25383.0  \n",
      "\n",
      "model does not have _feature_importance attribute.\n"
     ]
    }
   ],
   "source": [
    "# testing LogisticRegression\n",
    "model_current =  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
    "                                   intercept_scaling=0.1, l1_ratio=1e-06, max_iter=150,\n",
    "                                   multi_class='multinomial', n_jobs=-1, penalty='none',\n",
    "                                   random_state=42, solver='lbfgs', tol=1e-05, verbose=0,\n",
    "                                   warm_start=False)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n",
      "roc auc score: 0.8753240973827782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96     56945\n",
      "           1       0.28      0.83      0.42      2109\n",
      "\n",
      "    accuracy                           0.92     59054\n",
      "   macro avg       0.64      0.88      0.69     59054\n",
      "weighted avg       0.97      0.92      0.94     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "388         NaN  359.0   3402.0   0.339674  0.829777            7.044470   \n",
      "389         NaN  359.0   3402.0   0.339674  0.829777            6.538398   \n",
      "390         NaN  437.0  31562.0   0.050310  0.792793            0.917979   \n",
      "391         NaN  437.0  31562.0   0.050310  0.792793            1.039733   \n",
      "0           NaN  357.0   4560.0   0.277567  0.830725            4.303023   \n",
      "\n",
      "         tn       tp  \n",
      "388  1750.0  53543.0  \n",
      "389  1750.0  53543.0  \n",
      "390  1672.0  25383.0  \n",
      "391  1672.0  25383.0  \n",
      "0    1752.0  52385.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col  feat_rank\n",
      "0          V67   0.139434\n",
      "1          V88   0.057244\n",
      "2  ProductCD_R   0.056509\n",
      "3         V300   0.024647\n",
      "4     card5_fe   0.023890\n",
      "5         V301   0.022385\n",
      "6           V9   0.020888\n",
      "7          V47   0.019428\n",
      "8     card6_fe   0.018848\n",
      "9         V302   0.015161\n"
     ]
    }
   ],
   "source": [
    "# TESTING XGBClassifier n_estimators=100\n",
    "model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "                              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "                              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "                              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
    "                              nthread=None, objective='binary:logistic', random_state=42,\n",
    "                              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "                              silent=None, subsample=0.5, verbosity=1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n",
      "threshold:  0.1\n",
      "roc auc score: 0.8095789024880347\n",
      "confusion matrix:\n",
      " [[36932 20013]\n",
      " [   62  2047]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.8444455879644959\n",
      "confusion matrix:\n",
      " [[41848 15097]\n",
      " [   97  2012]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.865026534175436\n",
      "confusion matrix:\n",
      " [[45218 11727]\n",
      " [  135  1974]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.8743948235844848\n",
      "confusion matrix:\n",
      " [[47581  9364]\n",
      " [  183  1926]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.8821739684515864\n",
      "confusion matrix:\n",
      " [[49412  7533]\n",
      " [  218  1891]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.8850623876923494\n",
      "confusion matrix:\n",
      " [[50821  6124]\n",
      " [  258  1851]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.886712820190645\n",
      "confusion matrix:\n",
      " [[51927  5018]\n",
      " [  292  1817]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.8861769242288763\n",
      "confusion matrix:\n",
      " [[52811  4134]\n",
      " [  327  1782]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.8850176446948032\n",
      "confusion matrix:\n",
      " [[53543  3402]\n",
      " [  359  1750]]\n",
      "roc auc score: 0.8850176446948032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     56945\n",
      "           1       0.34      0.83      0.48      2109\n",
      "\n",
      "    accuracy                           0.94     59054\n",
      "   macro avg       0.67      0.89      0.72     59054\n",
      "weighted avg       0.97      0.94      0.95     59054\n",
      "\n",
      "\n",
      "df_scores:\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "394         NaN  916.0  13612.0   0.080581  0.565671            0.131308   \n",
      "395         NaN  465.0  21511.0   0.071000  0.779516            1.050254   \n",
      "396         NaN  859.0   2014.0   0.382966  0.592698            1.038110   \n",
      "397         NaN  840.0   2227.0   0.362986  0.601707            1.648225   \n",
      "0           NaN  359.0   3402.0   0.339674  0.829777            7.588302   \n",
      "\n",
      "         tn       tp  \n",
      "394  1193.0  43333.0  \n",
      "395  1644.0  35434.0  \n",
      "396  1250.0  54931.0  \n",
      "397  1269.0  54718.0  \n",
      "0    1750.0  53543.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAGECAYAAADX+oDHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5RdZX3/8fdHLiHcryIhaFSCihgCDIIgqEQLWhSsYoNYAW3j5dcqVRQsrhYv9Fcs2ta60MYKiPqLgSAB6wVilgEvIA4hJISb3DRcyiVRIdwDn98f+xk5GWaSM8mcOTM8n9daZ2WfZ9+++wx85jnP3rO3bBMREc9tz+t2ARER0XkJ+4iICiTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLCPMUnSHZIelbSy5TVhPbf5ekl3DleNbe7zHEmfH8l9DkbSqZK+3e06ojMS9jGWvdX25i2vu7tZjKQNu7n/9TGWa4/2JOzjOUfS/pJ+KekPkq6V9PqWecdLukHSQ5Juk/SB0r4Z8CNgQus3hf497/69//IN4yRJi4GHJW1Y1rtA0v2Sbpf0kTbrniTJpcZlkn4v6YOS9pW0uBzPV1qWP07SLyT9p6Q/SrpR0rSW+RMkXSxphaRbJP1Ny7xTJc2R9G1JDwIfBP4B+Mty7Neu6fNq/SwkfVzSfZLukXR8y/zxkr4o6belvp9LGr+2n1F0Rn6bx3OKpJ2BHwB/BfwYmAZcIOnltu8H7gMOB24DDgZ+JOnXthdKejPwbdsTW7bXzm6PBv4ceAB4Gvg+cFFpnwj8RNJNti9p8zD2AyaX+i4ux/FGYCPgGknn276sZdk5wPbAXwDfk/Ri2yuAWcBSYALwcmCepNtszy/rHgEcBbwXGFe2savt97TUMujnVea/ANgK2Bl4EzBH0lzbvwfOAF4JHAD8b6n16TZ+RtEB6dnHWDa39Az/IGluaXsP8EPbP7T9tO15QC/wFgDbP7B9qxuXAZcCB61nHV+2vcz2o8C+wA62P2v7Cdu3AV8Hpg9he5+z/ZjtS4GHgVm277N9F/AzYK+WZe8D/t32k7ZnAzcBfy5pF+C1wEllW4uA/6YJ2D5X2J5bPqdHByqkjc/rSeCzZf8/BFYCL5P0POB9wEdt32X7Kdu/tP04a/kZRWekZx9j2ZG2f9Kv7UXAUZLe2tK2EfBTgNJ7/ydgN5rOzqbAkvWsY1m//U+Q9IeWtg1oQrpd97ZMPzrA+81b3t/l1e9m+FuanvwEYIXth/rN6xmk7gG18Xktt72q5f0jpb7tgU2AWwfY7Bp/RtEZCft4rlkGfMv23/SfIWkccAHNsMVFtp8s3wj6xmoGugXswzQB1+cFAyzTut4y4Hbbk9el+HWwsyS1BP4LaYZ+7ga2lbRFS+C/ELirZd3+x7va+zY+rzV5AHgMeClwbb95g/6MonMyjBPPNd8G3irpUEkbSNqknEicCGxMMzZ9P7Cq9Fr/rGXde4HtJG3V0rYIeIukbSW9ADhhLfu/CniwnLQdX2rYQ9K+w3aEq3s+8BFJG0k6CngFzRDJMuCXwP8tn8EU4P3Ad9awrXuBSWUIBtb+eQ3K9tPAWcCXyoniDSS9pvwCWdPPKDokYR/PKSXkjqC5suR+ml7kJ4DnlR7uR4DzgN8D76bpBfeteyPNSc3bynmACcC3aHqmd9CMV89ey/6fAt4KTAVup+nh/jfNScxO+BXNydwHgNOAd9peXuYdDUyi6eVfCPxTGR8fzPnl3+WSFq7t82rDiTRDPr8GVgCn0/wcBv0ZDWHbMUTKw0sixiZJxwF/bfu13a4lRr/8Jo2IqEDCPiKiAhnGiYioQHr2EREVSNhHRFQgf1TVBdtvv70nTZrU7TIi4jnm6quvfsD2DgPNS9h3waRJk+jt7e12GRHxHCPpt4PNyzBOREQFcjVOF2y+5VbeY98Du11GRIxiV87/4ZDXkXS17Z6B5qVnHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9IWmBpEP7tZ0g6UxJL5R0qaQbJF0vaVKZ/zNJi8rrbklzu1F7RMTa5EZoz5gFTAcuaWmbTvMg5HOB02zPk7Q58DSA7YP6FpR0AXDRyJUbEdG+9OyfMQc4XNI4gNJ7nwCsADa0PQ/A9krbj7SuKGkL4BAgPfuIGJUS9oXt5cBVwGGlaTowG5gM/EHS9yRdI+lfJW3Qb/W3A/NtPzjY9iXNkNQrqffJJ57oxCFERAwqYb+6vqEcyr+zaIa6DgJOBPYFXgIc12+9o8uyg7I903aP7Z6NNt54OGuOiFirhP3q5gLTJO0NjLe9ELgTuMb2bbZXlWX27ltB0nbAq4EfdKPgiIh2JOxb2F4JLADO4pme+q+BbST1PerrEOD6ltWOAv7H9mMjVWdExFAl7J9tFrAn8F0A20/RDOHMl7QEEPD1luX7hnsiIkatXHrZj+0LaQK9tW0eMGWQ5V8/AmVFRKyX9OwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiArnOvgtevttkrpz/w26XEREVSc8+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiArk0ssuuPHWO3jt24/vdhkxRD+/8OxulxCxztKzj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMfCXtJTkhZJuk7S+ZI2XY9tHSfpK+ux7oSW9xtJ+hdJvym1XSXpzWXeHZKWlNf1kj4vadwatj1J0qPlOK+XdK6kjdalzoiITupkz/5R21Nt7wE8AXywdaYaI/HN4jhgQsv7zwE7AXuU2t4KbNEy/w22XwW8GngJMHMt27/V9lTgVcBE4F3DVHdExLAZqWGcnwG7lp7wDZLOBBYCu0g6uvSkr5N0et8Kko6XdLOky4ADW9rPkfTOlvcrW6Y/WbZ1bem9vxPoAb5Tet+bAX8D/J3txwFs32v7vP4F215J8wvqSEnbru0AbT8FXAXsPNQPJyKi0zoe9pI2BN4MLClNLwPOtb0X8CRwOnAIMBXYV9KRknYCPkMT8m8Cdm9jP28GjgT2s70n8AXbc4Be4JjS+34p8DvbD7ZTe1nudmByG/vfBNgP+PEg82dI6pXUu+rxx9rZfUTEsOlk2I+XtIgmbH8HfKO0/9b2lWV6X2CB7fttrwK+AxxME5p97U8As9vY3xuBs20/AmB7xTAdh9Yy/6XlOJfT/CJZPNBCtmfa7rHds+G4TYaptIiI9nTyFsePlt70n0gCeLi1aQ3re5D2VZRfUmo2uHHLtgZbp88twAslbWH7obUsi6QtgEnAzWtY7FbbU8u3kQWS3mb74rVtOyJiJHX70stfAa+TtL2kDYCjgctK++slbVeubjmqZZ07gH3K9BFA39UvlwLv67vqp2Wc/SHKCdjS6/8G8GVJG5fldpL0nv6FSdocOBOYa/v3azsQ2/cAJwOfavPYIyJGTFfDvgTkp4CfAtcCC21fVNpPBa4AfkJzMrfP12l+QVxFM9zzcNnWj4GLgd4yrHJiWf4c4GvlBO144NPA/cD1kq4D5pb3fX5a2q+iGX76wBAOaS6wqaSDhrBORETHyV7byEcMt8232d5TX//WbpcRQ5QnVcVoJ+lq2z0Dzev2ME5ERIyAPIO2DZJeBXyrX/PjtvfrRj0REUOVsG+D7SU0fwcQETEmZRgnIqICCfuIiAok7CMiKpCwj4ioQE7QdsHLXzop12xHxIhKzz4iogIJ+4iICiTsIyIqkLCPiKhAwj4iogK5GqcLbrrjTl533EndLiOG6LJzTl/7QhGjVHr2EREVSNhHRFQgYR8RUYGEfUREBRL2EREVSNhHRFQgYR8RUYGEfUREBRL2EREVSNi3kLRA0qH92k6QdKakL0haKukGSV+WpDL/aElLJC2W9GNJ23en+oiIwSXsVzcLmN6vbTowGzgQmALsAewLvE7ShsB/AG+wPQVYDPztyJUbEdGehP3q5gCHSxoHIGkSMAF4AtgE2BgYB2wE3AuovDYrPf0tgbtHvOqIiLVI2LewvRy4CjisNE0HZtu+AvgpcE95XWL7BttPAh8CltCE/O7ANwbatqQZknol9T752KMdPpKIiNUl7J+tdShnOjBL0q7AK4CJwM7AIZIOlrQRTdjvRfMNYDHwqYE2anum7R7bPRttMr7TxxARsZqE/bPNBaZJ2hsYb3sh8HbgStsrba8EfgTsD0wFsH2rbQPnAQd0qe6IiEEl7PspYb4AOIumlw/wO8oJ2dKbfx1wA3AXsLukHcpybyrtERGjSh5eMrBZwPd4ZjhnDnAIzdi8gR/b/j6ApM8Al0t6EvgtcNyIVxsRsRYJ+wHYvpDmKpu+908BHxhk2a8BXxuh0iIi1kmGcSIiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICuc6+C142aSKXnXN6t8uIiIqkZx8RUYGEfUREBRL2EREVSNhHRFQgYR8RUYGEfUREBXLpZRfcvOxepn3037pdxjqZ/x9/3+0SImIdpGcfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUoLqwl7RA0qH92k6QdLakqyUtkrRU0gdb5u8jaYmkWyR9WZJK+1Fl2acl9Yz0sUREtKu6sAdmAdP7tU0HzgEOsD0V2A84WdKEMv+rwAxgcnkdVtqvA/4CuLzDNUdErJcaw34OcLikcQCSJgETgMttP16WGUf5bCTtBGxp+wrbBs4FjgSwfYPtm0a2/IiIoasu7G0vB67imd75dGC2bUvaRdJiYBlwuu27gZ2BO1s2cWdpi4gYM6oL+6J1KGd6eY/tZbanALsCx0raEdAA63uoO5Q0Q1KvpN4nHn14HcuOiFg3tYb9XGCapL2B8bYXts4sPfqlwEE0PfmJLbMnAncPdYe2Z9rusd2z8fjN1r3yiIh1UGXY214JLADOovTqJU2UNL5MbwMcCNxk+x7gIUn7l6tw3gtc1JXCIyLWUZVhX8wC9gS+W96/AviVpGuBy4AzbC8p8z4E/DdwC3Ar8CMASW+XdCfwGuAHki4ZwfojItpW7cNLbF9Iy3i87XnAlEGW7QX2GGQbF3aqxoiI4VJzzz4iohoJ+4iICiTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAtX9U1U277bIj8//j77tdRkRUJD37iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgK5GqcLvjN3Ss49B+/0+0yhuySzx7T7RIiYh2lZx8RUYGEfUREBRL2EREVSNhHRFQgYR8RUYGEfUREBRL2EREVSNhHRFQgYR8RUYHqwl7SAkmH9ms7QdLZkq6WtEjSUkkfbJm/j6Qlkm6R9GVJKu3bSpon6Tfl321G+ngiItpRXdgDs4Dp/dqmA+cAB9ieCuwHnCxpQpn/VWAGMLm8DivtJwPzbU8G5pf3ERGjTo1hPwc4XNI4AEmTgAnA5bYfL8uMo3w2knYCtrR9hW0D5wJHluWOAL5Zpr/Z0h4RMapUF/a2lwNX8UzvfDow27Yl7SJpMbAMON323cDOwJ0tm7iztAHsaPuest17gOePxDFERAxVdWFftA7lTC/vsb3M9hRgV+BYSTsCGmB9D3WHkmZI6pXU+8QjD65j2RER66bWsJ8LTJO0NzDe9sLWmaVHvxQ4iKYnP7Fl9kTg7jJ9bxnm6RvuuW+wHdqeabvHds/Gm245fEcSEdGGKsPe9kpgAXAWpVcvaaKk8WV6G+BA4KYyPPOQpP3LVTjvBS4qm7oYOLZMH9vSHhExqtT88JJZwPd4ZjjnFcAXJZlm6OYM20vKvA/RXK0zHvhReQH8C3CepPcDvwOOGpnSIyKGptqwt30hLePxtucBUwZZthfYY4D25cC0TtUYETFcqhzGiYioTcI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICa7z0UtLH1jTf9peGt5yIiOiEtV1nv8WIVBERER2l5q69MZJ6enrc29vb7TIi4jlG0tW2ewaa19aYfblvzIWS7pN0r6QLJE1c+5oRETEatHuC9myam35NoLmX+/dLW0REjAHthv0Ots+2vaq8zgF26GBdERExjNoN+wckvUfSBuX1HmB5JwuLiIjh027Yvw94F/C/wD3AO4HjO1VUREQMr3Zvcfw54FjbvweQtC1wBs0vgRiiW+/9I+8443+6XcZaXXDi4d0uISKGSbs9+yl9QQ9gewWwV2dKioiI4dZu2D+vPKoP+FPPvtoHn0REjDXtBvYXgV9KmgOYZvz+tI5VFRERw6qtsLd9rqRe4BCaR/n9he3rO1pZREQMm7aHYkq4J+AjIsag3OI4IqICCfuIiAok7CMiKpCwj4ioQLVhL2mBpJ6W6ZskLSqv569hvR0k/UrSNZIOGrmKIyLWXRV/GCVpQ9ur1rLYMbbbeaLINOBG28cOQ2kRESNizIW9pPcCJ9L8cddi4Dzg08DGNHfiPMb2vZJOpbn//iSau3a+n+Ye/LsDNwDj12HfU4EvAOMlLQJeAxwEfAYYB9wKHG975XocYkTEsBtTYS/plcApwIG2Hyi3bTCwv21L+mvgk8DHyyr7AK+1/Wh5ePojtqdImgIs7Lf5syU9BVwAfN4DPK/R9iJJ/wj02P5bSdvT/KJ5o+2HJZ0EfAz47AC1zwBmAIzfOo8CiIiRNabCnuYveOfYfgCaG7JJehUwW9JONL3721uWv9j2o2X6YODLZb3Fkha3LHeM7bskbUET9n8FnNtGPfvTfFP4hSTK/q8YaEHbM4GZANvsMjkP/o2IETXWTtCKpiff6j+Br9h+FfABYJOWeQ/3W3bAkLV9V/n3IeD/Aa8eQj3zbE8tr91tv7/NdSMiRsxYC/v5wLskbQd/uvvmVsBdZf6aTppeDhxT1tsDmFKmNyzDMUjaCDgcuK7Neq4EDpS0a1l/U0m7DemIIiJGwJgaxrG9VNJpwGVlfP0a4FTgfEl30YTviwdZ/as04/KLgUXAVaV9HHBJCfoNgJ8AX2+znvslHQfMkjSuNH8auHmoxxYR0Uka4DxkdNg2u0z2IR/9t26XsVZ5UlXE2CLpats9A80ba8M4ERGxDsbUMM5IknQKcFS/5vNt56EtETHmJOwHUUI9wR4RzwkZxomIqEDCPiKiAgn7iIgKJOwjIiqQE7Rd8NIdt8o17BExotKzj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICufSyC357/0PM+K/53S5jNTM/MK3bJUREB6VnHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgWrDXtICST1lemNJMyXdLOlGSe9Yw3o7SPqVpGskHTRyFUdErLsqbpcgaUPbq9awyCnAfbZ3k/Q8YNs1LDsNuNH2scNaZEREB425sJf0XuBEwMBi4Dzg08DGwHLgGNv3SjoVmABMAh6Q9H7gbGB34AZgfMtm3we8HMD208ADg+x7KvAFYLykRcBrgIOAzwDjgFuB422vHL4jjohYf2NqGEfSK2l64YfY3hP4KPBzYH/bewHfBT7Zsso+wBG23w18CHjE9hTgtDIPSVuXZT8naaGk8yXtOND+bS8C/hGYbXsqsBnNL5o32t4b6AU+NkjtMyT1Sup9bOUf1uNTiIgYujEV9sAhwBzbDwDYXgFMBC6RtAT4BPDKluUvtv1omT4Y+HZZbzHNtwJovt1MBH5RAvsK4Iw269mf5pvCL0pP/1jgRQMtaHum7R7bPZtsvvVAi0REdMxYC3vRDN+0+k/gK7ZfBXwA2KRl3sP9lu2/LjRDP48AF5b35wN7D6Geebanltfutt/f5roRESNmrIX9fOBdkrYDkLQtsBVwV5m/ppOmlwPHlPX2AKYA2DbwfeD1ZblpwPVt1nMlcKCkXct2N5W0W7sHExExUsbUCVrbSyWdBlwm6SngGuBU4HxJd9GE74sHWf2rwNmSFgOLgKta5p0EfEvSvwP3A8e3Wc/9ko4DZkkaV5o/Ddw8pAOLiOgwNR3bGEk7vOhlfvs/nNntMlaTJ1VFjH2SrrbdM9C8sTaMExER62BMDeOMJEmnAEf1az7f9mndqCciYn0k7AdRQj3BHhHPCRnGiYioQMI+IqICCfuIiAok7CMiKpATtF3woh22yHXtETGi0rOPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhArsbpgrtWrOQfZv2y22UA8M9HH9DtEiJiBKRnHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9C0kLJB3ar+0ESWeW6S0l3SXpK+X9FpIWtbwekPTv3ag9ImJNEvarmwVM79c2vbQDfA64rG+G7YdsT+17Ab8FvjcilUZEDEHCfnVzgMMljQOQNAmYAPxc0j7AjsClA60oaTLwfOBnI1JpRMQQJOxb2F4OXAUcVpqmA7MBAV8EPrGG1Y8GZtv2QDMlzZDUK6n3kYf+MIxVR0SsXcL+2VqHcvqGcD4M/ND2sjWs1zrc8yy2Z9rusd2z6RZbD1uxERHtyC2On20u8CVJewPjbS+U9HHgIEkfBjYHNpa00vbJAJL2BDa0fXX3yo6IGFzCvh/bKyUtAM6i9NRtH9M3X9JxQE9f0BdHs4ZefUREt2UYZ2CzgD2B77a5/LtI2EfEKJae/QBsX0hzUnageecA5/Rre0nnq4qIWHfp2UdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFcp19F+y87eb889EHdLuMiKhIevYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCCXXnbBvX98hC/+z8JulwHAxw/fu9slRMQISM8+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMK+DZIWSDq0X9sJks6UdLqk68rrL7tVY0TEmiTs2zMLmN6vbTpwL7A3MBXYD/iEpC1HuLaIiLVK2LdnDnC4pHEAkiYBE4BHgMtsr7L9MHAtcFi3ioyIGEzCvg22lwNX8UyQTwdm04T7myVtKml74A3ALt2pMiJicLkRWvv6hnIuKv++z/ZCSfsCvwTuB64AVg20sqQZwAyAbXZ4wYgUHBHRJz379s0FpknaGxhveyGA7dNsT7X9JkDAbwZa2fZM2z22ezbbapuRqzoigoR922yvBBYAZ9H08pG0gaTtyvQUYApwabdqjIgYTIZxhmYW8D2euTJnI+BnkgAeBN5je8BhnIiIbkrYD4HtC2mGavrePwbs3r2KIiLak2GciIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAvmjqi7YcatN+fjhe3e7jIioSHr2EREVSNhHRFQgYR8RUYGEfUREBRL2EREVSNhHRFQgl152wfKVj3HO5Td0Zd/HHfyKruw3IrorPfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqIC1YW9pAWSDu3XdoKksyVdLWmRpKWSPtgyfx9JSyTdIunLklTa/1XSjZIWS7pQ0tYjfTwREe2oLuyBWcD0fm3TgXOAA2xPBfYDTpY0ocz/KjADmFxeh5X2ecAetqcANwOf6mzpERHrpsawnwMcLmkcgKRJwATgctuPl2XGUT4bSTsBW9q+wraBc4EjAWxfantVWedKYOJIHURExFBUF/a2lwNX8UzvfDow27Yl7SJpMbAMON323cDOwJ0tm7iztPX3PuBHnas8ImLdVRf2RetQzvTyHtvLypDMrsCxknYENMD6bn0j6RRgFfCdwXYoaYakXkm9D/1hxTAcQkRE+2oN+7nANEl7A+NtL2ydWXr0S4GDaHryrcMzE4G7+95IOhY4HDimDPMMyPZM2z22e7bYetvhO5KIiDZUGfa2VwILgLMovXpJEyWNL9PbAAcCN9m+B3hI0v7lKpz3AheV5Q4DTgLeZvuRET+QiIg21Xw/+1nA93hmOOcVwBclmWbo5gzbS8q8D9FcrTOeZly+b2z+KzQnc+eVqzGvtP2nSzYjIkaLasPe9oW0jMfbngdMGWTZXmCPAdp37ViBERHDqMphnIiI2iTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAwj4iogLV/lFVN223+SYcd/Arul1GRFQkPfuIiAok7CMiKqA13JU3OkTSQ8BN3a6jDdsDD3S7iLUYCzXC2KhzLNQIY6PObtX4Its7DDQjY/bdcZPtnm4XsTaSekd7nWOhRhgbdY6FGmFs1Dkaa8wwTkREBRL2EREVSNh3x8xuF9CmsVDnWKgRxkadY6FGGBt1jroac4I2IqIC6dlHRFQgYT/MJB0m6SZJt0g6eYD54yTNLvN/JWlSy7xPlfabJB062mqU9CZJV0taUv49pFM1rk+dLfNfKGmlpBNHY42Spki6QtLS8pluMtrqlLSRpG+W+m6Q9Kku1niwpIWSVkl6Z795x0r6TXkd26ka16dOSVNbft6LJf1lJ+t8Ftt5DdML2AC4FXgJsDFwLbB7v2U+DHytTE8HZpfp3cvy44AXl+1sMMpq3AuYUKb3AO4ajZ9ly/wLgPOBE0dbjTSXPS8G9izvt+vEz3sY6nw38N0yvSlwBzCpSzVOonlO9LnAO1vatwVuK/9uU6a36eJnOViduwGTy/QE4B5g607UOdArPfvh9WrgFtu32X4C+C5wRL9ljgC+WabnANMkqbR/1/bjtm8HbinbGzU12r7G9t2lfSmwiaRxHahxveoEkHQkzf/0SztU3/rW+GfAYtvXAthebvupUVingc0kbQiMB54AHuxGjbbvsL0YeLrfuocC82yvsP17YB5wWAdqXK86bd9s+zdl+m7gPmDAP4DqhIT98NoZWNby/s7SNuAytlcBf6Tp1bWzbrdrbPUO4Brbj3egxvWqU9JmwEnAZzpU23rXSNPLs6RLylf+T47SOucAD9P0Qn8HnGF7RZdq7MS6QzUs+5L0appvBrcOU11rlb+gHV4aoK3/5U6DLdPOusNhfWpsZkqvBE6n6Z12ygTo7lUAAAQGSURBVPrU+Rng32yvLB39TlmfGjcEXgvsCzwCzJd0te35w1viGmtoZ5lXA0/RDDtsA/xM0k9s3za8Ja7Xf/8j9f/OsOxL0k7At4Bjbff/ltIx6dkPrzuBXVreTwTuHmyZ8tV4K2BFm+t2u0YkTQQuBN5ru5O9kvWpcz/gC5LuAE4A/kHS346yGu8ELrP9gO1HgB8Ce3egxvWt893Aj20/afs+4BdAJ24DsD7//Y/U/zvrvS9JWwI/AD5t+8phrm3NRurkQA0vmt7abTQnWPtO3ryy3zL/h9VPhJ1Xpl/J6idob6MzJ2jXp8aty/LvGM2fZb9lTqVzJ2jX57PcBlhIc9JzQ+AnwJ+PwjpPAs6m6dFuBlwPTOlGjS3LnsOzT9DeXj7Tbcr0tt36LNdQ58bAfOCETtS21tq7sdPn8gt4C3AzzVjcKaXts8DbyvQmNFeI3AJcBbykZd1Tyno3AW8ebTUCn6YZv13U8nr+aKuz3zZOpUNhPww/7/fQnEC+DvjCaPzvEti8tC+lCfpPdLHGfWl61g8Dy4GlLeu+r9R+C3B8lz/LAessP+8n+/3/M7WTtba+8he0EREVyJh9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EcMA0kfKbcA/s4Q15sk6d2dqqtlPwskjaoHYMfISthHDI8PA2+xfcwQ15tEc0uCtpRbGUQMWcI+Yj1J+hrN/c0vlnSKpLMk/VrSNZKOKMtMkvSzcofLhZIOKKv/C3CQpEWS/n6Q7R8n6XxJ3wculbS5pPllO0v67eMGSV8vD8i4VNL4ftt6XnkYyec79oHEqJS/oI0YBuWmaz3Ax4DrbX9b0tY0tx7Yi+bOiE/bfkzSZGCW7R5Jr6e5ncPha9j2ccDnae5Js6L07je1/aCk7YErgcnAi2huF9Bje5Gk84CLSy0LgJOBjwLX2T6tAx9DjGL5ShgxvP4MeJueeRTiJsALae6M+BVJU2luGbzbELc7z8/cR17AP0s6mOYBGTsDO5Z5t9teVKavphkm6vNfNDc4S9BXKGEfMbxEc1fQm1ZrlE4F7gX2pBk+fWyI2324ZfoYmicc7WP7yfKtou/5ta0Pk3mK5ulSfX4JvEHSF20Pdf8xxmXMPmJ4XQL8XcvjEfcq7VsB97h5WMVf0TzLFOAhYIsh7mMr4L4S9G+gGb5pxzdo7pt/fk701idhHzG8PgdsBCyWdF15D3AmcKykK2mGcPp66ouBVZKuHewE7QC+A/RI6qXp5d/YbnG2v0RzH/1vScr//xXJCdqIiArkN3tERAUybhcxSkg6lOZB7q1ut/32btQTzy0ZxomIqECGcSIiKpCwj4ioQMI+IqICCfuIiAok7CMiKvD/AWwRLaxvEdnBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col  feat_rank\n",
      "0          V67   0.127891\n",
      "1  ProductCD_R   0.053158\n",
      "2          V88   0.053002\n",
      "3         V301   0.022626\n",
      "4         V300   0.021941\n",
      "5     card5_fe   0.020173\n",
      "6     card6_fe   0.018008\n",
      "7          V47   0.016993\n",
      "8           V9   0.016852\n",
      "9         V302   0.014930\n"
     ]
    }
   ],
   "source": [
    "# testing XGBClassifier 200 n_estimators\n",
    "model_current = XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
    "                              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
    "                              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
    "                              min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,\n",
    "                              nthread=None, objective='binary:logistic', random_state=42,\n",
    "                              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
    "                              silent=None, subsample=0.5, verbosity=1)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=9,\n",
      "                       min_impurity_decrease=0.1, min_impurity_split=None,\n",
      "                       min_samples_leaf=4, min_samples_split=3,\n",
      "                       min_weight_fraction_leaf=0.3, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=1,\n",
      "                       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score: 0.6633166330833978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.76      0.86     56945\n",
      "           1       0.08      0.57      0.14      2109\n",
      "\n",
      "    accuracy                           0.75     59054\n",
      "   macro avg       0.53      0.66      0.50     59054\n",
      "weighted avg       0.95      0.75      0.83     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "390         NaN  437.0  31562.0   0.050310  0.792793            0.917979   \n",
      "391         NaN  437.0  31562.0   0.050310  0.792793            1.039733   \n",
      "392         NaN  357.0   4560.0   0.277567  0.830725            4.303023   \n",
      "393         NaN  367.0   2881.0   0.376812  0.825984           10.411890   \n",
      "0           NaN  916.0  13612.0   0.080581  0.565671            0.131308   \n",
      "\n",
      "         tn       tp  \n",
      "390  1672.0  25383.0  \n",
      "391  1672.0  25383.0  \n",
      "392  1752.0  52385.0  \n",
      "393  1742.0  54064.0  \n",
      "0    1193.0  43333.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAGECAYAAADX+oDHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxcVZ3+8c8jgSTsq0ASNIowgIgBOqIiCmYYUFFhBjUoyjISnXFGGcXB7fUTRecnijr64+cwUVlEBpAlgIoIIkEcNjsQEkJkCaCEIDtC2APP/FGnh0rbnVSnu6qSnOf9etUrVecu51vVnadPn3v7XtkmIiJWby/pdgEREdF+CfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj1WSpLskPSVpcdNj3DD3uaekhSNVY4t9niLpK53sczCSjpH0427XEe2RsI9V2Tttr9v0WNTNYiSN6mb/w7Eq1x6tSdjHakfS6yVdJelRSTdK2rNp2WGS5kt6XNIdkj5S2tcBfgGMa/5Nof/Iu//ov/yGcbSkOcATkkaV7c6V9ICkOyV9vMW6J0pyqfFuSY9I+qikyZLmlPdzQtP6h0r6b0n/T9KfJf1e0pSm5eMkXSjpYUm3Szqiadkxks6R9GNJjwEfBT4HvK+89xuX9Xk1fxaSPiXpfkn3SjqsaflYSd+U9IdS328ljV3e1yjaIz/NY7UiaTzwc+CDwMXAFOBcSdvZfgC4H9gPuAN4M/ALSb+zfb2ktwE/tj2haX+tdHsQ8A7gQeAF4KfABaV9AvArSbfY/mWLb2M3YJtS34Xlffw1sCZwg6SzbV/RtO45wKbA3wLnSXqF7YeBM4B5wDhgO+BSSXfYvqxs+27gPcCHgNFlH6+yfXBTLYN+XmX5FsAGwHhgb+AcSefbfgQ4Hng18EbgT6XWF1r4GkUbZGQfq7Lzy8jwUUnnl7aDgYtsX2T7BduXAr3A2wFs/9z2AjdcAVwC7DHMOr5r+27bTwGTgc1sf9n2s7bvAL4PTB3C/o61/bTtS4AngDNs32/7HuBKYOemde8H/t32c7bPAm4B3iFpK+BNwNFlX7OBH9AI2D5X2z6/fE5PDVRIC5/Xc8CXS/8XAYuBv5L0EuBw4BO277H9vO2rbD/Dcr5G0R4Z2ceqbH/bv+rX9nLgPZLe2dS2JnA5QBm9fxHYlsZgZ21g7jDruLtf/+MkPdrUtgaNkG7VfU3Pnxrg9bpNr+/x0lcz/AONkfw44GHbj/db1jNI3QNq4fN6yPaSptdPlvo2BcYACwbY7TK/RtEeCftY3dwNnGb7iP4LJI0GzqUxbXGB7efKbwR9czUDXQL2CRoB12eLAdZp3u5u4E7b26xI8StgvCQ1Bf7LaEz9LAI2lrReU+C/DLinadv+73ep1y18XsvyIPA0sDVwY79lg36Non0yjROrmx8D75S0j6Q1JI0pBxInAGvRmJt+AFhSRq1/07TtfcAmkjZoapsNvF3SxpK2AI5cTv/XAY+Vg7ZjSw07Spo8Yu9waS8FPi5pTUnvAbanMUVyN3AV8H/LZ7AT8PfA6cvY133AxDIFA8v/vAZl+wXgJOBb5UDxGpLeUH6ALOtrFG2SsI/VSgm5d9M4s+QBGqPITwMvKSPcjwM/AR4B3k9jFNy37e9pHNS8oxwHGAecRmNkeheN+eqzltP/88A7gUnAnTRGuD+gcRCzHa6lcTD3QeCrwIG2HyrLDgIm0hjlzwC+WObHB3N2+fchSdcv7/NqwVE0pnx+BzwMHEfj6zDo12gI+44hUm5eErFqknQo8GHbb+p2LbHyy0/SiIgKJOwjIiqQaZyIiApkZB8RUYGEfUREBfJHVV2w6aabeuLEid0uIyJWM7NmzXrQ9mYDLUvYd8HEiRPp7e3tdhkRsZqR9IfBlmUaJyKiAjkbpwvWXX8D7zh5926XERErsWsuu2jI20iaZbtnoGUZ2UdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVKBtYS/peUmzJd0k6WxJaw9jX4dKOmEY245rer2mpK9Juq3Udl25kTKS7pI0tzxulvSVcoPkwfY9UdJT5X3eLOlHktZckTojItqpnSP7p2xPsr0j8Czw0eaFaujEbxaHAuOaXh8LbAnsWGp7J7Be0/K9bL8GeB3wSmD6cva/wPYk4DXABOC9I1R3RMSI6dQ0zpXAq8pIeL6k7wHXA1tJOqiMpG+SdFzfBpIOk3SrpCuA3ZvaT5F0YNPrxU3P/7Xs68Yyej8Q6AFOL6PvdYAjgH+2/QyA7fts/6R/wbYX0/gBtb+kjZf3Bm0/D1wHjB/qhxMR0W5tD3tJo4C3AXNL018BP7K9M/AccBzwVmASMFnS/pK2BL5EI+T3BnZooZ+3AfsDu9l+LfB12+cAvcAHyuh7a+CPth9rpfay3p3ANi30PwbYDbh4kOXTJPVK6n3u2Wdb6T4iYsS0M+zHSppNI2z/CPywtP/B9jXl+WRgpu0HbC8BTgfeTCM0+9qfBc5qob+/Bk62/SSA7YdH6H1oOcu3Lu/zIRo/SOYMtJLt6bZ7bPesudZaI1RaRERr2nnzkqfKaPp/SQJ4orlpGdsPdu3lJZQfUmrssC85tYxt+twOvEzSerYfX866SFoPmAjcuozVFtieVH4bmSnpXbYvXN6+IyI6qdunXl4LvEXSppLWAA4Crijte0rapJzd8p6mbe4Cdi3P3w30nf1yCXB431k/TfPsj1MOwJZR/w+B70paq6y3paSD+xcmaV3ge8D5th9Z3huxfS/wGeCzLb73iIiO6WrYl4D8LHA5cCNwve0LSvsxwNXAr2gczO3zfRo/IK6jMd3zRNnXxcCFQG+ZVjmqrH8KcGI5QDsW+ALwAHCzpJuA88vrPpeX9utoTD99ZAhv6XxgbUl7DGGbiIi2y52quiB3qoqI5cmdqiIiYsjaeYB2tSHpNcBp/Zqfsb1bN+qJiBiqhH0LbM+l8XcAERGrpEzjRERUIGEfEVGBhH1ERAUS9hERFcgB2i7YbtttVugc2oiIFZWRfUREBRL2EREVSNhHRFQgYR8RUYGEfUREBRL2EREVyKmXXfD7BXfxpgMO63YZEauN3844udslrPQyso+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIipQXdhLmilpn35tR0o6WdIsSbMlzZP00ablu0qaK+l2Sd+VpNL+nrLuC5J6Ov1eIiJaVV3YA2cAU/u1TQVOAd5oexKwG/AZSePK8v8ApgHblMe+pf0m4G+B37S55oiIYakx7M8B9pM0GkDSRGAc8Bvbz5R1RlM+G0lbAuvbvtq2gR8B+wPYnm/7ls6WHxExdNWFve2HgOt4cXQ+FTjLtiVtJWkOcDdwnO1FwHhgYdMuFpa2iIhVRnVhXzRP5Uwtr7F9t+2dgFcBh0jaHNAA23uoHUqaJqlXUu+SZ55ewbIjIlZMrWF/PjBF0i7AWNvXNy8sI/p5wB40RvITmhZPABYNtUPb02332O4ZNXrMilceEbECqgx724uBmcBJlFG9pAmSxpbnGwG7A7fYvhd4XNLry1k4HwIu6ErhERErqMqwL84AXgucWV5vD1wr6UbgCuB423PLsn8AfgDcDiwAfgEg6QBJC4E3AD+X9MsO1h8R0bJqb15iewZN8/G2LwV2GmTdXmDHQfYxo101RkSMlJpH9hER1UjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBhH1ERAWq/aOqbtpu64n8dsbJ3S4jIiqSkX1ERAUS9hERFUjYR0RUIGEfEVGBhH1ERAVyNk4X3HLXQt5y6NHdLiOira445bhulxBNMrKPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAwr4fSVtIOlPSAkk3S7pI0raSLpb0qKSf9Vv/FZKulXSbpLMkrdWt2iMiBpOwbyJJwAxgpu2tbe8AfA7YHPgG8MEBNjsO+LbtbYBHgL/vVL0REa1K2C9tL+A52yf2NdiebftK25cBjzevXH44vBU4pzSdCuzfqWIjIlqVsF/ajsCsIay/CfCo7SXl9UJg/EArSpomqVdS73NPPzXMMiMihiZhPzwaoM0DrWh7uu0e2z1rjhnb5rIiIpaWsF/aPGDXIaz/ILChpL5LRU8AFo14VRERw5SwX9qvgdGSjuhrkDRZ0lsGWtm2gcuBA0vTIcAFba8yImKIEvZNSngfAOxdTr2cBxwDLJJ0JXA2MEXSQkn7lM2OBj4p6XYac/g/7ELpERHLlDtV9WN7EfDeARbtMcj6dwCva2tRERHDlJF9REQFEvYRERVI2EdEVCBhHxFRgYR9REQFEvYRERVI2EdEVCDn2XfBX02cwBWnHNftMiKiIhnZR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBnHrZBbfefR9TPvHtbpcRFbrsO//S7RKiSzKyj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwLyTNlLRPv7YjJX1P0nGSbiqP9zUtP13SLaX9JElrdr7yiIjlS9i/6Axgar+2qcB9wC7AJGA34NOS1i/LTwe2A14DjAU+3JlSIyKGJmH/onOA/SSNBpA0ERgHPAlcYXuJ7SeAG4F9AWxf5AK4DpjQjcIjIpYnYV/YfohGYO9bmqYCZ9EI97dJWlvSpsBewFbN25bpmw8CF3eu4oiI1uVCaEvrm8q5oPx7uO3rJU0GrgIeAK4GlvTb7nvAb2xfOdiOJU0DpgGMXm+jNpQeETG4jOyXdj4wRdIuwFjb1wPY/qrtSbb3BgTc1reBpC8CmwGfXNaObU+33WO7Z62x67TvHUREDCAj+ya2F0uaCZxEY5SPpDWADW0/JGknYCfgkrLsw8A+wBTbL3Sn6oiI5UvY/6UzgPN48cycNYErJQE8Bhxsu28a50TgD8DVZfl5tr/c2XIjIpYvYd+P7Rk0pmr6Xj8N7DDIuvn8ImKVkDn7iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAvmjoC7YdqvNuew7/9LtMiKiIhnZR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIGfjdMFtix5mn/9zerfLiJXIL7/8gW6XEKu5jOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsG+RpC0knSlpgaSbJV0kaVtJL5N0iaT5pX1it2uNiOgvl0togSQBM4BTbU8tbZOAzYFjga/avlTSusAL3as0ImJgGdm3Zi/gOdsn9jXYng08BIyyfWlpW2z7yS7VGBExqIR9a3YEZg3Qvi3wqKTzJN0g6RuS1uhwbRERy5WwH55RwB7AUcBk4JXAoQOtKGmapF5Jvc8++VjnKoyIIGHfqnnArgO0LwRusH2H7SXA+cAuA+3A9nTbPbZ71lp7/TaWGhHxlxL2rfk1MFrSEX0NkiYDo4GNJG1Wmt8K3NyF+iIililh3wLbBg4A9i6nXs4DjgEW0ZjCuUzSXEDA97tWaETEIHLqZYtsLwLeO8Ci24CdOlxORMSQZGQfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBhH1ERAWWeeqlpE8ua7ntb41sORER0Q7LO89+vY5UERERbaXGH4dGJ/X09Li3t7fbZUTEakbSLNs9Ay1rac5e0gRJMyTdL+k+SedKmjCyZUZERLu0eoD2ZOBCYBwwHvhpaYuIiFVAq2G/me2TbS8pj1OAzZa3UURErBxaDfsHJR0saY3yOJjGLfkiImIV0GrYH07jio9/Au4FDgQOa1dRERExslq9xPGxwCG2HwGQtDFwPI0fAjFEC+77M393/M+6XUYsx7lH7dftEiJGTKsj+536gh7A9sPAzu0pKSIiRlqrYf8SSRv1vSgj+9z4JCJiFdFqYH8TuErSOYBpzN9/tW1VRUTEiGop7G3/SFIvjRtqC/hb27mxdkTEKqLlqZgS7gn4iIhVUC5xHBFRgYR9REQFEvYRERVI2EdEVKDKsJdkSac1vR4l6QFJP2tq21PSbEnzJF0xyH42KevMlvQnSfc0vV6rE+8lIqIVtf5h1BPAjpLG2n4K2Bu4p2+hpA2B7wH72v6jpJcOtBPbDwGTyjbHAIttH9/u4iMihqrKkX3xC+Ad5flBwBlNy94PnGf7jwC27+9wbRERI6rmsD8TmCppDLATcG3Tsm2BjSTNlDRL0oeG25mkaZJ6JfU+s/jPw91dRMSQ1DqNg+05kibSGNVf1G/xKGBXYAowFrha0jW2bx1Gf9OB6QAbbbVNbvwbER1VbdgXF9K4VPOewCZN7QuBB20/ATwh6TfAa4EVDvuIiG6qeRoH4CTgy7bn9mu/ANijnKWzNrAbML/j1UVEjJCqR/a2FwLfGaB9vqSLgTnAC8APbN/U6foiIkZKlWFve90B2mYCM5tefwP4xhD2ecwIlBYR0Ra1T+NERFShypH9ipC0CXDZAIumlD+uiohYaSXsW9T817IREauaTONERFQgYR8RUYGEfUREBRL2EREVyAHaLth68w0496j9ul1GRFQkI/uIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpBTL7vgDw88zrT/HOiaarEymf6RKd0uIWLEZGQfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUoLqwlzRT0j792o6UdLKkWZJmS5on6aNNy3eVNFfS7ZK+K0ml/RuSfi9pjqQZkjbs9PuJiGhFdWEPnAFM7dc2FTgFeKPtScBuwGckjSvL/wOYBmxTHvuW9kuBHW3vBNwKfLa9pUdErJgaw/4cYD9JowEkTQTGAb+x/UxZZzTls5G0JbC+7attG/gRsD+A7UtsLynbXANM6NSbiIgYiurC3vZDwHW8ODqfCpxl25K2kjQHuBs4zvYiYDywsGkXC0tbf4cDv2hf5RERK666sC+ap3KmltfYvrtMybwKOETS5oAG2N7NLyR9HlgCnD5Yh5KmSeqV1Pv04kdH4C1ERLSu1rA/H5giaRdgrO3rmxeWEf08YA8aI/nm6ZkJwKK+F5IOAfYDPlCmeQZke7rtHts9Y9bNcdyI6Kwqw972YmAmcBJlVC9pgqSx5flGwO7ALbbvBR6X9PpyFs6HgAvKevsCRwPvsv1kx99IRESLar6e/RnAebw4nbM98E1JpjF1c7ztuWXZP9A4W2csjXn5vrn5E2gczL20nI15je3/PWUzImJlUW3Y255B03y87UuBnQZZtxfYcYD2V7WtwIiIEVTlNE5ERG0S9hERFUjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIGEfEVGBav+oqptevtl6TP/IlG6XEREVycg+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICORunC+55eDGfO+OqbpcRy/FvB72x2yVEjJiM7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwj4ioQMI+IqICCfuIiAok7CMiKpCwb5GkLSSdKWmBpJslXSRpW0lflzRP0nxJ35WkbtcaEdFfLpfQghLgM4BTbU8tbZOALYHdgZ3Kqr8F3gLM7EKZERGDysi+NXsBz9k+sa/B9mzgWWAMsBYwGlgTuK8rFUZELEPCvjU7ArP6N9q+GrgcuLc8fml7/kA7kDRNUq+k3icff7StxUZE9JewHwZJrwK2ByYA44G3SnrzQOvanm67x3bP2utt2MkyIyIS9i2aB+w6QPsBwDW2F9teDPwCeH1HK4uIaEHCvjW/BkZLOqKvQdJkYG3gLZJGSVqTxsHZAadxIiK6KWfjtMC2JR0A/LukzwBPA3cBnwTGAXMBAxfb/mnXCo2IGETCvkW2FwHvHWDRRzpdS0TEUGUaJyKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQ8+y7YPzG6/JvB72x22VEREUyso+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAjn1sgvu+/OTfPNn13e7jFXWp/bbpdslRKxyMrKPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLBvkaQtJJ0paYGkmyVdJGlbSc9Lml0eF3a7zoiIgeQvaFsgScAM4FTbU0vbJGBz4Cnbk7pZX0TE8iTsW7MX8JztE/sabM8GaPwciIhYuWUapzU7ArMGWTZGUq+kayTt38miIiJalZH98L3M9iJJrwR+LWmu7QX9V5I0DZgGsNFmW3S6xoioXEb2rZkH7DrQAtuLyr93ADOBnQdZb7rtHts962ywUbvqjIgYUMK+Nb8GRks6oq9B0mRJb5E0urzeFNgduLlLNUZEDCph3wLbBg4A9i6nXs4DjimLeyXdCFwOfM12wj4iVjqZs29Rma557wCLXtPpWiIihioj+4iICiTsIyIqkLCPiKhAwj4iogIJ+4iICiTsIyIqkLCPiKhAwj4iogL5o6ou2HyDtfnUfrt0u4yIqEhG9hERFUjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIKdedsFDi5/mlN/M73YZI+rQN2/f7RIiYhkyso+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqEDCPiKiAgn7iIgKJOwjIiqQsI+IqMBqFfaSDpV0wiDLFg/SvpWkyyXNlzRP0ieW08d2kmZLukHS1iNRd0REu61WYT9UktYAlgCfsr098HrgY5J2WMZm+wMX2N7Z9oJO1BkRMVyrVNhLOl/SrDICn1baDpN0q6QrgN2b1n2FpKsl/U7SsU3te5aR/H8Bc23fa/t6ANuPA/OB8YP0/3bgSODDki4vbQdLuq6M9v+z/ACJiFiprFJhDxxue1egB/i4pPHAl2iE/N5A84j8O8B/2J4M/Knffl4HfN72UiN4SROBnYFrB+rc9kXAicC3be8laXvgfcDuticBzwMfGGhbSdMk9UrqffzRh4fwliMihm9VC/uPS7oRuAbYCvggMNP2A7afBc5qWnd34Izy/LR++7nO9p3NDZLWBc4FjrT9WIv1TAF2BX4naXZ5/cqBVrQ93XaP7Z71Nty4xd1HRIyMVeYSx5L2BP4aeIPtJyXNBH4PLOvauh6k/Yl++16TRtCfbvu8oZQFnGr7s0PYJiKi41alkf0GwCMl6LejcTB1LLCnpE1KYL+naf3/BqaW5wNOrQBIEvBDYL7tbw2xpsuAAyW9tOxrY0kvH+I+IiLablUK+4uBUZLmAMfSmMq5FzgGuBr4FXB90/qfoHFmze9o/KAYzO40poPeWg6yzi4HYpfL9s3AF4BLSl2XAlsO6V1FRHSA7MFmOqJdXrHdjv7i9LO7XcaIyp2qIrpP0izbPQMtW5VG9hERsYJWmQO0nSbp/9N03n7xHdsnd6OeiIjhSNgPwvbHul1DRMRIyTROREQFEvYRERVI2EdEVCBhHxFRgRyg7YJN1h2T89IjoqMyso+IqEDCPiKiArlcQhdIehy4pYslbAo82MX+V4Yaut3/ylBD7f2vDDWMdP8vt73ZQAsyZ98dtwx2/YpOkNTbzf5Xhhq63f/KUEPt/a8MNXSy/0zjRERUIGEfEVGBhH13TK+8f+h+Dd3uH7pfQ+39Q/dr6Fj/OUAbEVGBjOwjIiqQsB9hkvaVdIuk2yV9ZoDloyWdVZZfK2li07LPlvZbJO3Tyf4l7S1plqS55d+3drL/puUvk7RY0lEr0v9wa5C0k6SrJc0rn8WYTvUvaU1Jp5Z+50ta4RvZt1DDmyVdL2mJpAP7LTtE0m3lcUgn+5c0qenznyPpfZ3sv2n5+pLukXTCivQ/3BrK/4NLyvfBzf3/n6wQ23mM0ANYA1gAvBJYC7gR2KHfOv8InFieTwXOKs93KOuPBl5R9rNGB/vfGRhXnu8I3NPJ99+0/FzgbOCoLnwNRgFzgNeW15t0+GvwfuDM8nxt4C5gYps+g4nATsCPgAOb2jcG7ij/blSeb9TB/rcFtinPx9G4z/SGneq/afl3gP8CTmjj9+GgNQAzgb3L83WBtVekjuZHRvYj63XA7bbvsP0scCbw7n7rvBs4tTw/B5giSaX9TNvP2L4TuL3sryP9277B9qLSPg8YI2l0p/oHkLQ/jXCZN8R+R6qGvwHm2L4RwPZDtp/vYP8G1pE0ChgLPAs8NsT+W6rB9l225wAv9Nt2H+BS2w/bfgS4FNi3U/3bvtX2beX5IuB+YMA/EmpH/wCSdgU2By4ZYr8jUoOkHYBRti8t6y22/eQwagEyjTPSxgN3N71eWNoGXMf2EuDPNEaQrWzbzv6b/R1wg+1nOtW/pHWAo4EvDbHPEauBxqjSkn5Zfr3+1w73fw7wBI3R7B+B420/3KYa2rHtSO4DSa+jMSpe0Kn+Jb0E+Cbw6SH2OWI10Pg+fFTSeZJukPQNSWsMs578Be0I0wBt/U93GmydVrZtZ/+NhdKrgeNojHKHajj9fwn4tu3FZaC/ooZTwyjgTcBk4EngMkmzbF/Wof5fBzxPY/piI+BKSb+yfccQ+m+1hnZsO2L7kLQlcBpwiO2/GH23sf9/BC6yfXcHvg8HMwrYg8bU6h+Bs4BDgR8Op6CM7EfWQmCrptcTgEWDrVN+Xd8AeLjFbdvZP5ImADOAD9ke6mhquP3vBnxd0l3AkcDnJP1Th2tYCFxh+8Hya/NFwC4d7P/9wMW2n7N9P/DfwIr8Kf1wvpc69X04KEnrAz8HvmD7miH2Pdz+3wD8U/k+PB74kKSvdbiGhTR+s76j/OZ3PkP/PvxLw530z2OpgyqjaMw5v4IXD8q8ut86H2Ppg3M/Kc9fzdIHaO9g6AcHh9P/hmX9v+vG+++3zjGs+AHa4XwGGwHX0zg4Ogr4FfCODvZ/NHAyjVHhOsDNwE7t+Aya1j2FvzxAe2f5LDYqzzfuYP9rAZcBR7bz+3Cw/vstO5QVP0A7nM9gjbL+ZuX1ycDHVvTz+N/9DncHefzFF+7twK005hk/X9q+DLyrPB9D42yT24HrgFc2bfv5st0twNs62T/wBRrzxbObHpiAlBkAAAK4SURBVC/t5Ptv2scxrGDYj8DX4GAaB4hvAr7e4a/BuqV9Ho2g/3QbP4PJNEaQTwAPAfOatj281HY7cFgn+y+f/3P9vg8ndfL9N+3jUFYw7Efga7A3jTPD5tL4YbDWitbR98hf0EZEVCBz9hERFUjYR0RUIGEfEVGBhH1ERAUS9hERFUjYR0RUIGEfMQIkfbxcjvb0IW43UdL721VXUz8zJXX15t7RXQn7iJHxj8DbbX9giNtNpHGZhJaUyytEDFnCPmKYJJ1I47rlF0r6vKSTJP2uXLHw3WWdiZKuLFfTvF7SG8vmXwP2kDRb0r8Msv9DJZ0t6afAJZLWlXRZ2c/cfn3Ml/T9cvOPSySN7bevl6hxg5SvtO0DiZVS/oI2YgSUC2f1AJ8Ebrb9Y0kb0rgcws40rnj4gu2nJW0DnGG7R9KeNC4Nsd8y9n0o8BUa18l5uIzu17b9mKRNgWuAbYCX07jEQY/t2ZJ+AlxYapkJfAb4BHCT7a+24WOIlVh+JYwYWX8DvEsv3lZxDPAyGlc8PEHSJBqXMd52iPu91C9e217Av0l6M40bX4yncbMNgDttzy7PZ9GYJurznzQuupagr1DCPmJkicaVQ29ZqlE6BrgPeC2N6dOnh7jfJ5qef4DG3Zt2tf1c+a2i7165zTeceZ7GHa/6XAXsJembtofaf6ziMmcfMbJ+Cfxz060Wdy7tGwD3unEjjg/SuIwtwOPAekPsYwPg/hL0e9GYvmnFD2lco//sHOitT8I+YmQdC6wJzJF0U3kN8D3gEEnX0JjC6RupzwGWSLpxsAO0Azgd6JHUS2OU//tWi7P9LRrX7D+t3IIvKpEDtBERFchP9oiICmTeLmIlIWkfGjd7b3an7QO6UU+sXjKNExFRgUzjRERUIGEfEVGBhH1ERAUS9hERFUjYR0RU4H8AIWAOloZYINEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col  feat_rank\n",
      "0  ProductCD_R   0.163934\n",
      "1         V301   0.131148\n",
      "2          C10   0.114754\n",
      "3          V92   0.098361\n",
      "4           C6   0.081967\n",
      "5         M6_T   0.065574\n",
      "6         V302   0.065574\n",
      "7           C8   0.065574\n",
      "8           C5   0.049180\n",
      "9     addr2_fe   0.032787\n"
     ]
    }
   ],
   "source": [
    "# testing RandomForestClassifier\n",
    "model_current = RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                       criterion='gini', max_depth=5, max_features='auto',\n",
    "                                       max_leaf_nodes=9,\n",
    "                                       min_impurity_decrease=0.1, min_impurity_split=None,\n",
    "                                       min_samples_leaf=4, min_samples_split=3,\n",
    "                                       min_weight_fraction_leaf=0.3, n_estimators=100,\n",
    "                                       n_jobs=-1, oob_score=False, random_state=42, verbose=1,\n",
    "                                       warm_start=False)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=11, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=9, min_samples_split=5,\n",
      "                       min_weight_fraction_leaf=0, presort='auto',\n",
      "                       random_state=42, splitter='best')\n",
      "threshold:  0.1\n",
      "roc auc score: 0.7896280386009625\n",
      "confusion matrix:\n",
      " [[40465 16480]\n",
      " [  277  1832]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.8083025051290831\n",
      "confusion matrix:\n",
      " [[47290  9655]\n",
      " [  451  1658]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.813868805471044\n",
      "confusion matrix:\n",
      " [[49463  7482]\n",
      " [  508  1601]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.8144657562442961\n",
      "confusion matrix:\n",
      " [[49909  7036]\n",
      " [  522  1587]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.8068523232531902\n",
      "confusion matrix:\n",
      " [[51715  5230]\n",
      " [  621  1488]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.7953842271087443\n",
      "confusion matrix:\n",
      " [[53298  3647]\n",
      " [  728  1381]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.7908356290816744\n",
      "confusion matrix:\n",
      " [[53914  3031]\n",
      " [  770  1339]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.7821775946868949\n",
      "confusion matrix:\n",
      " [[54737  2208]\n",
      " [  837  1272]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.7786652506446767\n",
      "confusion matrix:\n",
      " [[54931  2014]\n",
      " [  859  1250]]\n",
      "roc auc score: 0.7786652506446767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     56945\n",
      "           1       0.38      0.59      0.47      2109\n",
      "\n",
      "    accuracy                           0.95     59054\n",
      "   macro avg       0.68      0.78      0.72     59054\n",
      "weighted avg       0.96      0.95      0.96     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "392         NaN  357.0   4560.0   0.277567  0.830725            4.303023   \n",
      "393         NaN  367.0   2881.0   0.376812  0.825984           10.411890   \n",
      "394         NaN  916.0  13612.0   0.080581  0.565671            0.131308   \n",
      "395         NaN  465.0  21511.0   0.071000  0.779516            1.050254   \n",
      "0           NaN  859.0   2014.0   0.382966  0.592698            1.038110   \n",
      "\n",
      "         tn       tp  \n",
      "392  1752.0  52385.0  \n",
      "393  1742.0  54064.0  \n",
      "394  1193.0  43333.0  \n",
      "395  1644.0  35434.0  \n",
      "0    1250.0  54931.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                col  feat_rank\n",
      "0       ProductCD_R   0.299810\n",
      "1          card2_fe   0.188458\n",
      "2  P_emaildomain_fe   0.140255\n",
      "3           V294_fe   0.061730\n",
      "4          card5_fe   0.044080\n",
      "5          addr1_fe   0.035400\n",
      "6           V279_fe   0.025616\n",
      "7          card6_fe   0.021235\n",
      "8            D10_fe   0.021186\n",
      "9               C11   0.017006\n"
     ]
    }
   ],
   "source": [
    "# testing DecisionTreeClassifier max_depth=11\n",
    "model_current = DecisionTreeClassifier(class_weight='balanced',\n",
    "                                       criterion='entropy', max_depth=11, max_features=None,\n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                       min_impurity_split=None, min_samples_leaf=9,\n",
    "                                       min_samples_split=5, min_weight_fraction_leaf=0,\n",
    "                                       presort='auto', random_state=42, splitter='best')\n",
    "\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=13, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=9, min_samples_split=5,\n",
      "                       min_weight_fraction_leaf=0, presort='auto',\n",
      "                       random_state=42, splitter='best')\n",
      "threshold:  0.1\n",
      "roc auc score: 0.8028863292635816\n",
      "confusion matrix:\n",
      " [[42353 14592]\n",
      " [  291  1818]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.8175924620268423\n",
      "confusion matrix:\n",
      " [[47403  9542]\n",
      " [  416  1693]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.8182677536379862\n",
      "confusion matrix:\n",
      " [[50072  6873]\n",
      " [  512  1597]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.8171961282464953\n",
      "confusion matrix:\n",
      " [[51300  5645]\n",
      " [  562  1547]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.8081604199871596\n",
      "confusion matrix:\n",
      " [[52458  4487]\n",
      " [  643  1466]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.8035590063215983\n",
      "confusion matrix:\n",
      " [[53500  3445]\n",
      " [  701  1408]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.8022769676895773\n",
      "confusion matrix:\n",
      " [[53678  3267]\n",
      " [  713  1396]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.7875251426961064\n",
      "confusion matrix:\n",
      " [[54428  2517]\n",
      " [  803  1306]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.7812995294928462\n",
      "confusion matrix:\n",
      " [[54718  2227]\n",
      " [  840  1269]]\n",
      "roc auc score: 0.7812995294928462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     56945\n",
      "           1       0.36      0.60      0.45      2109\n",
      "\n",
      "    accuracy                           0.95     59054\n",
      "   macro avg       0.67      0.78      0.71     59054\n",
      "weighted avg       0.96      0.95      0.95     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "     feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "393         NaN  367.0   2881.0   0.376812  0.825984           10.411890   \n",
      "394         NaN  916.0  13612.0   0.080581  0.565671            0.131308   \n",
      "395         NaN  465.0  21511.0   0.071000  0.779516            1.050254   \n",
      "396         NaN  859.0   2014.0   0.382966  0.592698            1.038110   \n",
      "0           NaN  840.0   2227.0   0.362986  0.601707            1.648225   \n",
      "\n",
      "         tn       tp  \n",
      "393  1742.0  54064.0  \n",
      "394  1193.0  43333.0  \n",
      "395  1644.0  35434.0  \n",
      "396  1250.0  54931.0  \n",
      "0    1269.0  54718.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAGECAYAAAAGMUbyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dfbxmc73/8dc7N2PcTRjJGNkpEtLIJiVyV+IQitKhkFKnG5WcnOJXSs7p7tRJKk03FI40HFI5IRlJNGbGGPc06MR0MKjMGHfj/fvj+u7OZbv23tfes/f32nt7Px+P6zHrWuv7Xeuz1uZ67+9aa69LtomIiKjleZ0uICIinlsSPBERUVWCJyIiqkrwREREVQmeiIioKsETERFVJXgiIqKqBE9EPyTdLWmppMVNrynLuc6dJd0zXDW2uc3TJX2+5jb7IukESWd2uo7onARPxMD2sb1602thJ4uRtGInt788xnLtMXwSPBFDJGl7Sb+T9BdJ10vauWnZ4ZJukfSIpDslva/MXw34b2BK8wiq94ik96iojLyOlTQfWCJpxdLvPEkPSLpL0lFt1t0lyaXGP0l6WNL7JW0raX7Zn1Oa2h8m6SpJ35D0V0m3StqtafkUSRdKekjSHyS9t2nZCZLOlXSmpL8B7wc+Bby97Pv1/R2v5mMh6eOS7pf0Z0mHNy2fKOnfJf2x1PdbSRMH+hlF5+S3j4ghkLQB8AvgncAvgd2A8yRtZvsB4H5gb+BOYCfgvyVda3uupD2BM21PbVpfO5t9B/APwCLgaeBnwE/L/KnAryTdZvviNnfj1cAmpb4Ly37sDqwEXCdphu0rmtqeC0wG3gL8l6QX234IOBu4CZgCbAZcKulO25eVvvsCBwLvAiaUdbzU9iFNtfR5vMryFwKTgA2ANwDnSrrA9sPAV4AtgNcC/1tqfbqNn1F0SEY8EQO7oPzG/BdJF5R5hwAX2b7I9tO2LwVmA3sB2P6F7QVuuAK4BNhxOes42fafbC8FtgXWtf0520/YvhP4LnDQINZ3ou3HbF8CLAHOtn2/7XuBK4Gtm9reD/yH7SdtnwPcBvyDpA2B1wHHlnXNA75H48O+x9W2LyjHaWmrQto4Xk8CnyvbvwhYDLxM0vOAdwMfsX2v7WW2f2f7cQb4GUXnZMQTMbD9bP+q17yNgAMl7dM0byXgcoAyqvkMsCmNX/BWBW5Yzjr+1Gv7UyT9pWneCjQCo133NU0vbfF+9ab39/qZTxT+I40RzhTgIduP9FrW3UfdLbVxvB60/VTT+0dLfZOBVYAFLVbb788oOifBEzE0fwLOsP3e3gskTQDOo3Fq6ae2nywjpZ7zaa0eCb+Exodtjxe2aNPc70/AXbY3GUrxQ7CBJDWFz4tonJ5bCKwtaY2m8HkRcG9T3977+4z3bRyv/iwCHgNeAlzfa1mfP6PorJxqixiaM4F9JO0haQVJq5SL4FOBlWlcy3gAeKr8Nv/Gpr73AetImtQ0bx6wl6S1Jb0Q+OgA258F/K3ccDCx1LClpG2HbQ+f6QXAUZJWknQg8HIap7H+BPwO+LdyDLYCjgDO6mdd9wFd5TQZDHy8+mT7aeAHwFfLTQ4rSHpNCbP+fkbRQQmeiCEoH7j70rhD6wEav13/M/C88pv/UcBPgIeBf6QxOujpeyuNC/J3lutGU4AzaPzGfjeN6xvnDLD9ZcA+wDTgLhq/+X+PxgX4kfB7GjciLAJOAg6w/WBZ9g6gi8bo53zgM+V6Sl9mlH8flDR3oOPVhmNonJa7FngI+CKNn0OfP6NBrDtGgPJFcBHRH0mHAe+x/bpO1xLjQ5I/IiKqSvBERERVOdUWERFVZcQTERFVJXgiIqKq/AHpODZ58mR3dXV1uoyIGGfmzJmzyPa6Q+2f4BnHurq6mD17dqfLiIhxRtIfl6d/TrVFRERVuattHFt9zUnectsdOl1GRIxi11x20aD7SJpju3vglq1lxBMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvBERERVCZ6IiKgqwRMREVWN+eCRtEzSPEk3SpohadXlWNdhkk5Zjr5Tmt6vJOkLku4otc2StGdZdrekG8rrZkmflzShn3V3SVpa9vNmST+StNJQ6oyI6LQxHzzAUtvTbG8JPAG8v3mhGmrs52HAlKb3JwLrA1uW2vYB1mhavovtVwDbARsD0wdY/wLb04BXAFOBtw1T3RERVY2H4Gl2JfDSMkK4RdK3gLnAhpLeUUYYN0r6Yk8HSYdLul3SFcAOTfNPl3RA0/vFTdOfKOu6voxqDgC6gbPKqGQ14L3Ah20/DmD7Pts/6V2w7cU0wnI/SWsPtIO2lwGzgA0Ge3AiIkaDcRM8klYE9gRuKLNeBvzI9tbAk8AXgV2BacC2kvaTtD7wWRqB8wZg8za2syewH/Bq268EvmT7XGA2cHAZlbwE+B/bf2un9tLuLmCTNra/CvBq4Jd9LD9S0mxJs5984ol2Nh8RUdV4CJ6JkubR+OD/H+D7Zf4fbV9TprcFZtp+wPZTwFnATjQ+wHvmPwGc08b2dgdOs/0ogO2Hhmk/NMDyl5T9fJBGqM1v1cj2dNvdtrtXWnnlYSotImL4jIcvgltaRhl/JwlgSfOsfvr39b0QT1GCWY0V9nyKq58+Pf4AvEjSGrYfGaAtktYAuoDb+2m2wPa0MkqbKenNti8caN0REaPNeBjxtOP3wOslTZa0AvAO4Ioyf2dJ65S7xA5s6nM3sE2Z3hfouYvsEuDdPXfPNV2XeYRy80AZDX0fOFnSyqXd+pIO6V2YpNWBbwEX2H54oB2x/WfgX4BPtrnvERGjynMieMqH9SeBy4Hrgbm2f1rmnwBcDfyKxo0IPb5LI6xm0Tglt6Ss65fAhcDscurrmNL+dODUcnPBROB44AHgZkk3AheU9z0uL/Nn0ThF+L5B7NIFwKqSdhxEn4iIUSHfQDqO5RtII2Ig+QbSiIgY98bDzQXjhqRXAGf0mv247Vd3op6IiJGQ4BlFbN9A4++MIiLGrZxqi4iIqhI8ERFRVYInIiKqSvBERERVublgHNts002GdI9+RMRIyognIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqoET0REVJXbqcexWxfczev2P7zTZUQ8y2/PP63TJUQHZcQTERFVJXgiIqKqBE9ERFSV4ImIiKoSPBERUVWCJyIiqkrwREREVQmeiIioKsETERFVJXhGmKSZkrolrSrpF5JulXSTpC8M0G9dSb+XdJ2kHWvVGxEx0hI8w0jSQI8g+ortzYCtgR0k7dlP292AW21vbfvKYSsyIqLD8qy2Pkh6F3AMYGA+8BPgeGBl4EHgYNv3SToBmAJ0AYskHQGcBmwO3AJMBLD9KHB5mX5C0lxgah/bngZ8CZgoaR7wGmBH4LPABGABcLjtxcO+4xERIyzB04KkLYDjgB1sL5K0No0A2t62Jb0H+ATw8dJlG+B1tpdKOhp41PZWkrYC5rZY//OBfYCvt9q+7XmSPg102/6QpMk0Qm9320skHQscDXyuxbqPBI4EmDBxteU5DBERIyLB09quwLm2FwHYfkjSK4BzJK1PY9RzV1P7C20vLdM7ASeXfvMlzW9ecTkddzZwsu0726xnexojqKskUbZ/dauGtqcD0wFWX2uy21x/REQ1CZ7WRGOE0+wbwFdtXyhpZ+CEpmVLerXt7wN/OnCH7f8YZD2X2n7HIPpERIxKubmgtcuAt0laB6CcapsE3FuWH9pP398AB5d+WwJb9SyQ9Pmyno8Osp5raNyM8NKynlUlbTrIdUREjAoJnhZs3wScBFwh6XrgqzRGODMkXQks6qf7t4HVyym2TwCzACRNpXHdaHNgrqR55VpRO/U8ABwGnF3Wew2w2RB2LSKi42TnMsB4tfpakz1t5306XUbEs+QbSMc2SXNsdw+1f0Y8ERFRVW4u6DBJxwEH9po9w/ZJnagnImKkJXg6rARMQiYinjNyqi0iIqpK8ERERFUJnoiIqCrBExERVeXmgnFss5d05e8lImLUyYgnIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqrc1TaO3Xb3Pbz+sGM7XUZUcsXpX+x0CRFtyYgnIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvBERERVIxY8kpZJmifpRkkzJK06UtvqY/tTJJ1bpneW9PM+2t0tafII1tEt6eQh9j1K0i2SzhruuiIiOmUkRzxLbU+zvSXwBPD+EdzWs9heaPuAmtvso47Zto8aYvcPAHvZPng4a4qI6KRap9quBF7a10JJh0iaVUZI35G0Qpm/WNIXJc2R9CtJ20maKelOSW8ubbokXSlpbnm9tmn+jS22tY6kSyRdJ+k7gJqWHV1GaDdK+mjTem6V9L0y/yxJu0u6StIdkrYr7baT9Luy3t9JelmZ//fRlqQTJP2gaR/6DCRJpwIbAxdK+pik1Urfa8s29u2j35GSZkua/eRjS/v/qUREdMCIB4+kFYE9gRv6WP5y4O3ADranAcuAnt/wVwNm2t4GeAT4PPAGYH/gc6XN/cAbbL+qrGeg01qfAX5re2vgQuBFpY5tgMOBVwPbA++VtHXp81Lg68BWwGbAPwKvA44BPlXa3ArsVNb7aeBf+9j+ZsAewHbAZySt1KqR7fcDC4FdbH8NOA74te1tgV2AL0tarUW/6ba7bXevtMrEAQ5FRER9I/m1CBMlzSvTVwLf76PdbsA2wLWSACbSCBNonKL7ZZm+AXjc9pOSbgC6yvyVgFMk9YTWpgPUtRPwFgDbv5D0cJn/OuB820sAJP0XsCONcLrL9g1l/k3AZbbdq45JwA8lbQK41NXKL2w/Djwu6X5gPeCeAWoGeCPwZknHlPer0AjNW9roGxExaoxk8CwtI5iBCPih7U+2WPakbZfpp4HHAWw/XUZSAB8D7gNeSWME91gb23SLeWoxr8fjTdNPN71/mv87hicCl9veX1IXMLONdS2j/Z+BgLfavq3N9hERo9JouJ36MuAASS8AkLS2pI0G0X8S8GfbTwPvBFYYoP1vKKfyJO0JrNU0fz9Jq5ZTWPvTGKkNpo57y/Rhg+jXrouBD6sMC5tOA0ZEjCkdDx7bNwPHA5dImg9cCqw/iFV8CzhU0jU0TrMtGaD9Z4GdJM2lcfrqf0odc4HTgVnA74Hv2b5uEHV8Cfg3SVcxcPgNxYk0Tt/NLzdNnDgC24iIGHH6vzNZMd6sMfmFftXeh3a6jKgkX30dtUiaY7t7qP07PuKJiIjnlpG8ueAZJK1D43pOb7vZfrBWHaNNjktEPNdUC57yIdrOXW7PKTkuEfFck1NtERFRVYInIiKqSvBERERV1a7xRH0v65qaW2wjYtTJiCciIqpK8ERERFUJnoiIqCrBExERVSV4IiKiqgRPRERUldupx7Hb/3Qfu33ka50uY0Rc9vWPdbqEiBiijHgiIqKqBE9ERFSV4ImIiKoSPBERUVWCJyIiqkrwREREVQmeiIioKsETERFVJXgiIqKqBE8/JM2UtEeveR+VdJGkqyXdJGm+pLc3Ld9V0lxJN0r6oaQVe/XfVtIySQcMsO0vl/V/eXj3KiKis/LInP6dDRwEXNw07yDgWGCh7TskTQHmSLoY+BvwQ2A327dL+hxwKPB9AEkrAF/stb6+vA9Y1/bjw7Y3ERGjQEY8/TsX2FvSBABJXcAU4De27wCwvRC4H1gXWAd43Pbtpf+lwFub1vdh4LzSvk+SLgRWA34v6e2S1pV0nqRry2uH4drBiIjaEjz9sP0gMAt4U5l1EHCObfe0kbQdsDKwAFgErCSpuyw+ANiwtNsA2B84tY3tvhlYanua7XOArwNfs70tjSD7Xl99JR0pabak2U8sXTKo/Y2IqCGn2gbWc7rtp+Xfd/cskLQ+cAZwqO2ny7yDgK+VUdIlwFOl+X8Ax9peJmmwNewObN7Ub01Ja9h+pHdD29OB6QBrrrehey+PiOi0BM/ALgC+KulVwETbcwEkrQn8Ajje9jU9jW1fDexY2rwR2LQs6gZ+XMJjMrCXpKdsX9BGDc8DXmN76TDtU0REx+RU2wBsLwZmAj+gMfpB0srA+cCPbM9obi/pBeXfCTRuQji1rOfFtrtsd9G4dvSBNkMHGiOnDzVtY9py7FJEREcleNpzNvBK4Mfl/duAnYDDJM0rr54w+GdJtwDzgZ/Z/vUwbP8ooLvcun0z8P5hWGdEREeo6Tp5jDNrrrehtz3o6E6XMSLyDaQRnSNpju3ugVu2lhFPRERUlZsLOkjSK2jcFdfscduv7kQ9ERE1JHg6yPYNQG4UiIjnlJxqi4iIqhI8ERFRVYInIiKqSvBERERVublgHNt0w/Xy9y4RMepkxBMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVe5qG8fuWPgQe3z6rE6X0dLFnzu40yVERIdkxBMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqoEzwiTNFNSd9P0bZLmldcL+um3rqTfS7pO0o71Ko6IGFl5VtswkrSi7acGaHaw7dltrG434Fbbhw5DaRERo0aCpw+S3gUcAxiYD/wEOB5YGXiQRoDcJ+kEYArQBSySdARwGrA5cAswcQjbngZ8CZgoaR7wGmBH4LPABGABcLjtxcuxixERHZHgaUHSFsBxwA62F0lam0YAbW/bkt4DfAL4eOmyDfA620slHQ08ansrSVsBc3ut/jRJy4DzgM/bdu/t254n6dNAt+0PSZpMI/R2t71E0rHA0cDnWtR+JHAkwCqT1lnuYxERMdwSPK3tCpxrexGA7YckvQI4R9L6NEY9dzW1v9D20jK9E3By6Tdf0vymdgfbvlfSGjSC553Aj9qoZ3saI6irJFG2f3WrhranA9MBJk3Z+FmhFhHRabm5oDXRGOE0+wZwiu1XAO8DVmlatqRX25Yf+LbvLf8+AvwnsN0g6rnU9rTy2tz2EW32jYgYVRI8rV0GvE3SOgDlVNsk4N6yvL8L/r8BDi79tgS2KtMrllNmSFoJ2Bu4sc16rgF2kPTS0n9VSZsOao8iIkaJnGprwfZNkk4CrijXY64DTgBmSLqXRhC8uI/u36ZxHWc+MA+YVeZPAC4uobMC8Cvgu23W84Ckw4CzJU0os48Hbh/svkVEdJpaXNuOcWLSlI29/XtO7HQZLeWrryPGLklzbHcPtX9OtUVERFU51dZhko4DDuw1e4btkzpRT0TESEvwdFgJmIRMRDxn5FRbRERUleCJiIiq+j3VVh7/0ifbXx3eciIiYrwb6BrPGlWqiIiI54z8Hc841t3d7dmz2/kGhoiI9lX5Ox5JUyWdL+l+SfdJOk/S1KFuNCIinrvavbngNOBCGt87swHwszIvIiJiUNoNnnVtn2b7qfI6HVh3BOuKiIhxqt3gWSTpEEkrlNchNL6FMyIiYlDaDZ53A28D/hf4M3AAcPhIFRUREeNXu4/MORE41PbD8Pfvp/kKjUCKUWrBfX/lrV/5eafLAOC8Y/budAkRMUq0O+LZqid0oPFV0MDWI1NSRESMZ+0Gz/MkrdXzpox48oDRiIgYtHbD49+B30k6FzCN6z15onJERAxaW8Fj+0eSZgO7AgLeYvvmEa0sIiLGpbZPl5WgSdhERMRyydciREREVQmeiIioKsETERFVJXgiIqKqBM8QSDpM0il9LFvcT78flK+WuLGNbWwmaZ6k6yS9ZHnqjYgYTRI8FUhaoUyeDrypzW77AT+1vbXtBSNSWEREByR4WpB0gaQ5km6SdGSZd7ik2yVdAezQ1PbFkq6WdK2kE5vm7yzpckn/CdwAYPs3wENtbH8v4KPAeyRdXuYdImlWGQV9pynMIiLGlARPa++2vQ3QDRwlaQPgszQC5w3A5k1tvw582/a2NJ7e3Ww74DjbmzMIti8CTgW+ZnsXSS8H3g7sYHsasAw4uFVfSUdKmi1p9uOL/zqYzUZEVJHgae0oSdcD1wAbAu8EZtp+wPYTwDlNbXcAzi7TZ/Razyzbdw1DPbsB2wDXSppX3m/cqqHt6ba7bXdPWH3SMGw6ImJ45UGfvUjaGdgdeI3tRyXNBG4FXt5PN/cxf8lwlQX80PYnh2l9EREdkxHPs00CHi6hsxmwPTAR2FnSOpJWAg5san8VcFCZbnn6axhcBhwg6QXQeDq4pI1GaFsRESMqwfNsvwRWlDSfxhfgXUPjW1dPAK4GfgXMbWr/EeCDkq6lEVp9knR2WcfLJN0j6Yh2CirPyTseuKTUdSmw/mB2KiJitJDd11miGOvW2nAT7/qRr3W6DCDfQBoxnkiaY7t7qP0z4omIiKpyc0GHSfomTX8XVHzd9mmdqCciYqQleDrM9gc7XUNERE051RYREVUleCIioqoET0REVJXgiYiIqnJzwTj2kvUm5e9nImLUyYgnIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqoET0REVJXbqcexPz7wCEd+57IRW//09+02YuuOiPErI56IiKgqwRMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvBERERVCZ6IiKgqwdMPSTMl7dFr3kclXSTpakk3SZov6e1Ny6+UNK+8Fkq6oMxfS9L5pf0sSVsOsO2jJN0i6ayR2buIiM7II3P6dzZwEHBx07yDgGOBhbbvkDQFmCPpYtt/sb1jT0NJ5wE/LW8/Bcyzvb+kzYBvAv09c+YDwJ627xrG/YmI6LiMePp3LrC3pAkAkrqAKcBvbN8BYHshcD+wbnNHSWsAuwIXlFmbA5eVPrcCXZLWa7VRSacCGwMXSvqYpNUk/UDStZKuk7Tv8O5mREQ9CZ5+2H4QmAW8qcw6CDjHtnvaSNoOWBlY0Kv7/sBltv9W3l8PvKWpz0bA1D62+35gIbCL7a8BxwG/tr0tsAvwZUmrteor6UhJsyXNfmzxXwa7yxERIy7BM7Ce022Uf8/uWSBpfeAM4HDbT/fq947mtsAXgLUkzQM+DFwHPNVmDW8E/qX0nQmsAryoVUPb02132+5eZfXnt7n6iIh6co1nYBcAX5X0KmCi7bkAktYEfgEcb/ua5g6S1gG2ozHqAaCMfA4vywXcVV7tEPBW27ct575ERHRcRjwDsL2YxijjB5QRjKSVgfOBH9me0aLbgcDPbT/WM0PS80s/gPfQuE70txZ9W7kY+HAJLCRtPZR9iYgYDRI87TkbeCXw4/L+bcBOwGFNt05Pa2r/jFNyxcuBmyTdCuwJfGQQ2z8RWAmYL+nG8j4iYkzKqbY22D6fxumunvdnAmf2037nFvOuBjYZxDa7mqaXAu9rt29ExGiWEU9ERFSVEU8HlZsQLmuxaLdyK3dExLiT4OmgEi7TBmwYETGO5FRbRERUleCJiIiqEjwREVFVgiciIqrKzQXj2EbrrsH09/X3zQsREfVlxBMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVe5qG8fufWgxnzr7d8O2vn99x2uHbV0R8dyVEU9ERFSV4ImIiKoSPBERUVWCJyIiqkrwREREVQmeiIioKsETERFVJXgiIqKqBE9ERFSV4ImIiKoSPCNM0kxJ3WV6ZUnTJd0u6VZJb+2n37qSfi/pOkk71qs4ImJk5Vltw0jSiraf6qfJccD9tjeV9Dxg7X7a7gbcavvQYS0yIqLDEjx9kPQu4BjAwHzgJ8DxwMrAg8DBtu+TdAIwBegCFkk6AjgN2By4BZjYtNp3A5sB2H4aWNTHtqcBXwImSpoHvAbYEfgsMAFYABxue3GLvkcCRwKsOXm9Ie9/RMRIyam2FiRtQWN0sqvtVwIfAX4LbG97a+DHwCeaumwD7Gv7H4F/Ah61vRVwUlmGpOeXtidKmitphqSWyWB7HvBp4Bzb04DVaITe7rZfBcwGju6j73Tb3ba7V13j+a2aRER0VIKntV2Bc20vArD9EDAVuFjSDcA/A1s0tb/Q9tIyvRNwZuk3n8ZoCRqjy6nAVSU8rga+0mY929MYQV1VRkCHAhsNcd8iIjoqwdOaaJxia/YN4BTbrwDeB6zStGxJr7a9+0Lj9NyjwPnl/QzgVYOo51Lb08prc9tHtNk3ImJUSfC0dhnwNknrAEhaG5gE3FuW93fB/zfAwaXflsBWALYN/AzYubTbDbi5zXquAXaQ9NKy3lUlbdruzkREjCa5uaAF2zdJOgm4QtIy4DrgBGCGpHtpBMGL++j+beA0SfOBecCspmXHAmdI+g/gAeDwNut5QNJhwNmSJpTZxwO3D2rHIiJGATV+EY/xaP2NN/PhJ/1g2NaXr76OCABJc2x3D7V/TrVFRERVOdXWYZKOAw7sNXuG7ZM6UU9ExEhL8HRYCZiETEQ8Z+RUW0REVJXgiYiIqhI8ERFRVa7xjGMbrL16boGOiFEnI56IiKgqwRMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVW6nHsfu++uj/PvP5w7Luj6+d7vfWRcR0b+MeCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvBERERVCZ6IiKgqwRMREVUleCIioqoEzxBJWiZpnqSbJF0v6WhJzyvL1pF0uaTFkk7p1W8bSTdI+oOkkyWpn21sVrZxnaSXjPQ+RUTUkOAZuqW2p9neAngDsBfwmbLsMeD/Ace06Pdt4Ehgk/J6Uz/b2A/4qe2tbS8YtsojIjoowTMMbN9PI0w+JEm2l9j+LY0A+jtJ6wNr2r7atoEf0QiXZ5G0F/BR4D2SLi/zDpE0q4yCviNphZHcr4iIkZDgGSa276RxPF/QT7MNgHua3t9T5rVa30XAqcDXbO8i6eXA24EdbE8DlgEH9+4n6UhJsyXNXvLXh4e2MxERIyhPpx5efV6v6We521z3bsA2wLXlstBE4P5nrcyeDkwH2HCTzdtdd0RENQmeYSJpYxqjkGeFQZN7gKlN76cCC9vdBPBD258cWoUREaNDTrUNA0nr0jgtdkq5dtOS7T8Dj0javtzN9i7gp21u5jLgAEkvKNtcW9JGy1l6RER1GfEM3URJ84CVgKeAM4Cv9iyUdCiKMzgAAAiYSURBVDewJrCypP2AN9q+Gfgn4HQap8r+u7wGZPtmSccDl5Tbtp8EPgj8cbh2KCKihgTPENnu944y2119zJ8NbNnmNk7o9f4c4Jz2KoyIGJ1yqi0iIqrKiGcUkPRNYIdes79u+7RO1BMRMZISPKOA7Q92uoaIiFpyqi0iIqpK8ERERFUJnoiIqCrBExERVeXmgnFsvUmr8vG9X9XpMiIiniEjnoiIqCrBExERVSV4IiKiqgRPRERUleCJiIiqEjwREVFVbqcexx5c/Bin/+aWIfc/bKeXD2M1ERENGfFERERVCZ6IiKgqwRMREVUleCIioqoET0REVJXgiYiIqhI8ERFRVYInIiKqSvB0iKQXSvqxpAWSbpZ0kaRNJf1S0l8k/bxX+w9J+oMkS5rcqbojIpZXgqcDJAk4H5hp+yW2Nwc+BawHfBl4Z4tuVwG7A3+sVmhExAjII3M6YxfgSdun9sywPa9nWtLOvTvYvq4sq1FfRMSIyYinM7YE5nS6iIiITkjwjDOSjpQ0W9LsR/7yUKfLiYh4lgRPZ9wEbDMSK7Y93Xa37e41nr/2SGwiImK5JHg649fABEnv7ZkhaVtJr+9gTRERVSR4OsC2gf2BN5TbqW8CTgAWSroSmAHsJukeSXsASDpK0j3AVGC+pO91qPyIiOWSu9o6xPZC4G0tFu3YR/uTgZNHtKiIiAoy4omIiKoSPBERUVWCJyIiqkrwREREVQmeiIioKsETERFVJXgiIqKqBE9ERFSVPyAdx9ZZfRUO2+nlnS4jIuIZMuKJiIiqEjwREVGVGs+rjPFI0iPAbZ2uYwgmA4s6XcQQjdXaU3ddY7VuaNS+mu11h7qCXOMZ326z3d3pIgZL0uyxWDeM3dpTd11jtW74e+1dy7OOnGqLiIiqEjwREVFVgmd8m97pAoZorNYNY7f21F3XWK0bhqH23FwQERFVZcQTERFVJXjGKElvknSbpD9I+pcWyydIOqcs/72krqZlnyzzb5O0x1ioW1KXpKWS5pXXqaOs7p0kzZX0lKQDei07VNId5XVovaqXu+5lTcf7wnpV/337A9V+tKSbJc2XdJmkjZqWjeZj3l/dHTvmbdT9fkk3lNp+K2nzpmWD+0yxndcYewErAAuAjYGVgeuBzXu1+QBwapk+CDinTG9e2k8AXlzWs8IYqLsLuHEUH+8uYCvgR8ABTfPXBu4s/65Vptca7XWXZYs7cbwHUfsuwKpl+p+a/lsZ7ce8Zd2dPOZt1r1m0/SbgV+W6UF/pmTEMzZtB/zB9p22nwB+DOzbq82+wA/L9LnAbpJU5v/Y9uO27wL+UNY32uvupAHrtn237fnA07367gFcavsh2w8DlwJvqlE0y1d3p7VT++W2Hy1vrwGmlunRfsz7qruT2qn7b01vVwN6bhAY9GdKgmds2gD4U9P7e8q8lm1sPwX8FVinzb4jZXnqBnixpOskXSFpx5EutlVNxWCO2Wg/3v1ZRdJsSddI2m94SxvQYGs/AvjvIfYdTstTN3TumLdVt6QPSloAfAk4ajB9m+XJBWNTqxFA79sT+2rTTt+Rsjx1/xl4ke0HJW0DXCBpi16/hY2U5Tlmo/149+dFthdK2hj4taQbbC8YptoG0nbtkg4BuoHXD7bvCFieuqFzx7ytum1/E/impH8EjgcObbdvs4x4xqZ7gA2b3k8FFvbVRtKKwCTgoTb7jpQh112G8Q8C2J5D4zzypiNeca+aisEcs9F+vPtke2H5905gJrD1cBY3gLZql7Q7cBzwZtuPD6bvCFmeujt5zAd7zH4M9IzIBn+8O3EhK6/lvhC4Io0Lpi/m/y4EbtGrzQd55kX6n5TpLXjmhcA7qXdzwfLUvW5PnTQugN4LrD1a6m5qezrPvrngLhoXudcq02Oh7rWACWV6MnAHvS42d7p2Gh/KC4BNes0f1ce8n7o7dszbrHuTpul9gNlletCfKVX+I8prRP5D2Qu4vfwHfFyZ9zkav0EBrALMoHGhbxawcVPf40q/24A9x0LdwFuBm8p/4HOBfUZZ3dvS+M1vCfAgcFNT33eX/fkDcPhYqBt4LXBDOd43AEeMwv/GfwXcB8wrrwvHyDFvWXenj3kbdX+9/D84D7icpmAa7GdKnlwQERFV5RpPRERUleCJiIiqEjwREVFVgiciIqpK8ERERFUJnoiIqCrBEzGGSDpK0i2Szhpkv67ymJMRJWmmpO6R3k6MbQmeiLHlA8Betg8eZL8uoO3gKY8rihgRCZ6IMaJ8+d3GwIWSjpP0A0nXlid271vadEm6sny521xJry3dvwDsWL7E62N9rP8wSTMk/Qy4RNLq5YvK5pYvAGvexi2SvivpJkmXSJrYa13Pk/RDSZ8fsQMSY1aeXBAxhki6m8YTjY8GbrZ9pqTn03i80NY0ngr8tO3HJG0CnG27W9LOwDG29+5n3YcBnwe2sv1QGfWsavtvkibT+O6YTYCNaDyKptv2PEk/ofHYlzMlzQT+BfgIjS/uO2kEDkOMcRlOR4xNbwTeLOmY8n4V4EU0ngp8iqRpwDIG/wTvS20/VKYF/KuknWh8UdwGwHpl2V2255XpOTRO5fX4Do2HuyZ0oqUET8TYJOCttm97xkzpBBoPoHwljVPpjw1yvUuapg+m8VTwbWw/WUZbq5Rljze1WwY0n2r7HbCLpH+3Pdjtx3NArvFEjE0XAx/u+VpwST3f2zIJ+LPtp4F3AiuU+Y8AawxyG5OA+0vo7ELjFFs7vg9cBMzITQrRSoInYmw6EVgJmC/pxvIe4FvAoZKuoXGarWcEMx94StL1fd1c0MJZQLek2TRGP7e2W5ztr9L4+oozJOVzJp4hNxdERERV+U0kIiKqyvnXiOcYSXsAX+w1+y7b+3einnjuyam2iIioKqfaIiKiqgRPRERUleCJiIiqEjwREVFVgiciIqr6/9fm4RRl7ttAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                col  feat_rank\n",
      "0       ProductCD_R   0.287072\n",
      "1          card2_fe   0.180950\n",
      "2  P_emaildomain_fe   0.134429\n",
      "3           V294_fe   0.059769\n",
      "4          card5_fe   0.042207\n",
      "5          addr1_fe   0.037437\n",
      "6           V279_fe   0.025849\n",
      "7          card6_fe   0.021462\n",
      "8            D10_fe   0.020395\n",
      "9               C11   0.017218\n"
     ]
    }
   ],
   "source": [
    "# testing DecisionTreeClassifier max_depth=13\n",
    "model_current = DecisionTreeClassifier(class_weight='balanced',\n",
    "                                       criterion='entropy', max_depth=13, max_features=None,\n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                       min_impurity_split=None, min_samples_leaf=9,\n",
    "                                       min_samples_split=5, min_weight_fraction_leaf=0,\n",
    "                                       presort='auto', random_state=42, splitter='best')\n",
    "\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bool_predict_proba = True\n",
    "# model_current = LogisticRegression(random_state=42)\n",
    "# mod.create_df_score_model(model_current)\n",
    "# bool_predict_proba = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# model_current = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
    "# mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# model_current = LogisticRegression(random_state=42)\n",
    "# mod.create_df_score_model(model_current) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.X_train = pd.DataFrame()\n",
    "        self.y_train = pd.DataFrame()\n",
    "        self.X_test = pd.DataFrame()\n",
    "        self.y_test = pd.DataFrame()\n",
    "        self.X_features = pd.DataFrame()\n",
    "        self.y_target = pd.DataFrame()\n",
    "        \n",
    "    def create_df_score_model(self, model_current):\n",
    "        '''scores model'''\n",
    "        print(\"Fitting model:\\n\", model_current)\n",
    "        y_pred, elapsed_time = self.add_model(model_current) \n",
    "        df_scores, df_temp, y_pred = self._score_model(y_pred, \n",
    "                                                       elapsed_time)\n",
    "        self._save_results(df_scores, df_temp, y_pred)\n",
    "        self._feature_importance(model_current)\n",
    "        fe.col_fe = []\n",
    "        \n",
    "    def add_model(self, model):        \n",
    "        '''fitting model and calculating time elapsed'''\n",
    "        start_time = time.time()\n",
    "        model.fit(mod.X_train, mod.y_train)\n",
    "        y_pred = self._predict(model)\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        return y_pred, elapsed_time\n",
    "    \n",
    "    def _predict(self, model):\n",
    "        '''make prediction'''\n",
    "        if bool_predict_proba:\n",
    "            y_pred = self._predict_proba(model)\n",
    "            return y_pred \n",
    "        else:\n",
    "            y_pred = model.predict(mod.X_test)\n",
    "            return y_pred\n",
    "        \n",
    "    def _predict_proba(self, model):\n",
    "        try:\n",
    "            y_pred_prob = model.predict_proba(mod.X_test)\n",
    "            y_pred_class = self._predict_proba_threshold(y_pred_prob)\n",
    "            return y_pred_class\n",
    "        except:\n",
    "            print(\"Model does not have predict_proba attribute.\")\n",
    "            \n",
    "    def _predict_proba_threshold(self, y_pred_prob):\n",
    "        for threshold in [.1, .15, .2, .25, .3, .35, .4, .45, .5]:\n",
    "            print('threshold: ', threshold)\n",
    "            y_pred_class = binarize(y_pred_prob, threshold)[:,1]\n",
    "            print('roc auc score:', roc_auc_score(mod.y_test, y_pred_class))\n",
    "            print('confusion matrix:\\n', confusion_matrix(mod.y_test, y_pred_class))\n",
    "        return y_pred_class\n",
    "            \n",
    "#     def _create_roc_curve(self, y_pred_class):\n",
    "#         fpr, tpr, thresholds = roc_curve(mod.y_test, y_pred_class)\n",
    "#         plt.plot(fpr, tpr)\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.0])\n",
    "#         plt.title(\"ROC curve for classifier\")\n",
    "#         plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "#         plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "        \n",
    "    def _score_model(self, y_pred, elapsed_time):      \n",
    "        '''creating dataframe with score results'''\n",
    "        col_recall, col_precision, col_time = self._calc_scores(y_pred, \n",
    "                                                                  elapsed_time)        \n",
    "        df_conf_matrix = self._confusion_matrix(y_pred)\n",
    "        df_temp = pd.concat([col_recall, col_precision, df_conf_matrix, col_time], axis=1)\n",
    "        if fe.col_fe:\n",
    "            df_temp = self._concat_new_feat(df_temp)\n",
    "        df_scores = self._read_create_score_file(df_temp)\n",
    "        return df_scores, df_temp, y_pred\n",
    "\n",
    "    def _calc_scores(self, y_pred, elapsed_time):\n",
    "        '''calculating recall, precision and elapsed time'''\n",
    "        col_recall = pd.Series(recall_score(mod.y_test, y_pred), name='recall')\n",
    "        col_precision = pd.Series(precision_score(mod.y_test, y_pred), name='precision')\n",
    "        col_time = pd.Series(elapsed_time, name='time_elapsed (min)')\n",
    "        print(\"roc auc score:\", roc_auc_score(mod.y_test, y_pred))\n",
    "        return col_recall, col_precision, col_time\n",
    "    \n",
    "    def _confusion_matrix(self, y_pred):\n",
    "        '''creating confusion matrix dataframe'''\n",
    "        df_conf_matrix = pd.DataFrame(confusion_matrix(mod.y_test, y_pred))\n",
    "        val_tp = pd.Series(df_conf_matrix[0][0], name='tp')\n",
    "        val_fn = pd.Series(df_conf_matrix[0][1], name='fn')\n",
    "        val_fp = pd.Series(df_conf_matrix[1][0], name='fp')\n",
    "        val_tn = pd.Series(df_conf_matrix[1][1], name='tn')\n",
    "        return pd.concat([val_fn, val_fp, val_tp, val_tn], axis=1)\n",
    "\n",
    "    def _concat_new_feat(self, df_temp):\n",
    "        '''concatenate scoring results'''        \n",
    "        print(\"\\nThe following new features have been created:\", fe.col_fe, '\\n')\n",
    "        if len(fe.col_fe) > 1: \n",
    "            fe.col_fe = \"model score\"\n",
    "        col_fe = pd.Series(fe.col_fe, name='feat_tested')\n",
    "        return pd.concat([col_fe, df_temp], axis=1)\n",
    "    \n",
    "    def _read_create_score_file(self, df_temp):\n",
    "        '''reading or creating df_scores file'''\n",
    "        try: \n",
    "            df_scores = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "            df_scores = df_scores.drop('Unnamed: 0', axis=1)\n",
    "        except:\n",
    "            print(\"\\nCreating df_scores.csv file.\") \n",
    "            df_scores = df_temp\n",
    "        return df_scores\n",
    "            \n",
    "    def _save_results(self, df_scores, df_temp, y_pred):\n",
    "        '''printing scores for new features'''            \n",
    "        df_scores = pd.concat([df_scores, df_temp], axis=0)\n",
    "        df_scores.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "        classif_report = classification_report(mod.y_test, y_pred)\n",
    "        self._print_summary(classif_report, df_scores)\n",
    "        self._save_summary(classif_report)\n",
    "\n",
    "    def _print_summary(self, classif_report, df_scores):\n",
    "        '''print last 5 rows of previous score results'''\n",
    "        print(classif_report)\n",
    "        print('\\ndf_scores:\\n\\n', df_scores.tail(5))\n",
    "    \n",
    "    def _save_summary(self, classif_report):\n",
    "        '''save score result summary to text file'''\n",
    "        file_summary = open('/Users/krahman/work/fraud_detection/saved_files/df_scores_summary.txt', \"a\")\n",
    "        file_summary.write('New features created from: ' \n",
    "                           + fe.str_list_col_fe \n",
    "                           + '\\n')\n",
    "        file_summary.write(classif_report)\n",
    "        file_summary.close()\n",
    "        \n",
    "    def _feature_importance(self, model):\n",
    "        '''create feature importance dataframe and bar plot'''\n",
    "        try:\n",
    "            df_feat_rank = self._feat_import_create_df(model)\n",
    "            self._feat_import_create_plot(df_feat_rank)\n",
    "            print(df_feat_rank[0:10].reset_index(drop=True))\n",
    "        except:\n",
    "            print(\"\\nmodel does not have _feature_importance attribute.\")\n",
    "        \n",
    "    def _feat_import_create_df(self, model):\n",
    "        '''creating dataframe of important features'''\n",
    "        col_name = pd.Series(fe.df_feat.columns, name='col')\n",
    "        col_feat_rank = pd.Series(model.feature_importances_, \n",
    "                                  name='feat_rank')\n",
    "        df_feat_rank = pd.concat([col_name, col_feat_rank], axis=1)\n",
    "        df_feat_rank = df_feat_rank.sort_values('feat_rank', ascending=False)\n",
    "        return df_feat_rank\n",
    "    \n",
    "    def _feat_import_create_plot(self, df_feat_rank):\n",
    "        '''create feature importance bar plot'''\n",
    "        plt.figure(figsize=(5,6))\n",
    "        sns.barplot(df_feat_rank.feat_rank[0:10],\n",
    "                    df_feat_rank.col[0:10],\n",
    "                    palette='Blues_d')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "        \n",
    "mod = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_xgbc = XGBClassifier()\n",
    "# model_xgbc.fit(mod.X_train, mod.y_train)\n",
    "# y_pred_xgbc = model_xgbc.predict(mod.X_test)\n",
    "# print(classification_report(mod.y_test, y_pred_xgbc))\n",
    "# print(confusion_matrix(mod.y_test, y_pred_xgbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cbc = CatBoostClassifier()\n",
    "# X_test\n",
    "# model_cbc.fit(mod.X_train, mod.y_train)\n",
    "# # y_pred_cbc = model_cbc.predict(mod.y_test)\n",
    "# # print(classification_report(mod.y_test, y_pred_cbc))\n",
    "# # print(confusion_matrix(mod.y_test, y_pred_cbc))\n",
    "# test_data = Pool(mod.X_test)\n",
    "# y_pred_cbc = model_cbc.predict(test_data)\n",
    "# print(classification_report(mod.y_test, y_pred_cbc))\n",
    "# print(confusion_matrix(mod.y_test, y_pred_cbc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base model\n",
    "# model_lr_temp = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_temp.fit(mod.X_train, mod.y_train)\n",
    "# pred_lr = model_lr_temp.predict(mod.X_test)\n",
    "# print(confusion_matrix(mod.y_test, pred_lr))\n",
    "# print(roc_auc_score(mod.y_test, pred_lr))\n",
    "# # NEXT, we fixed our downsample and upsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr_temp.fit(mod.X_train, mod.y_train)\n",
    "# pred_lr = model_lr_temp.predict(mod.X_test)\n",
    "# print(confusion_matrix(mod.y_test, pred_lr))\n",
    "# print(roc_auc_score(mod.y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Tuning - XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning xgbc\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 7.5min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.986, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 14.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.985, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 28.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 34.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 40.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 46.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 53.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 59.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 3.1min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 2.1min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.2min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.966, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 2.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.800, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.796, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.799, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.805, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.798, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.807, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.990, total= 3.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.1min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 4.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.984, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total=14.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.987, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.3min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.3min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.2min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.7min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.961, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.959, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.961, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.964, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.960, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.963, total= 1.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.960, total= 1.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=7, reg_alpha=3, max_depth=3, colsample_bytree=0.3, colsample_bynode=0.1, colsample_bylevel=0.5, booster=gbtree, base_score=0.1, score=0.962, total= 1.7min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 5.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.985, total= 5.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 5.8min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.985, total= 7.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.3min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.5min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.986, total= 4.4min\n",
      "[CV] subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1 \n",
      "[CV]  subsample=0.1, scale_pos_weight=3, reg_lambda=3, reg_alpha=0, max_depth=13, colsample_bytree=0.5, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.1, score=0.987, total= 4.5min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.982, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.982, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.985, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.983, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=7, max_depth=11, colsample_bytree=0.1, colsample_bynode=0.3, colsample_bylevel=0.9, booster=gbtree, base_score=0.5, score=0.984, total= 1.7min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 3.0min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 3.1min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.983, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.985, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.982, total= 2.9min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.8min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.983, total= 2.8min\n",
      "[CV] subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.3, scale_pos_weight=5, reg_lambda=7, reg_alpha=5, max_depth=7, colsample_bytree=0.3, colsample_bynode=0.3, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.984, total= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 315.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 13\n",
      "Best learning_rate: 0.1\n",
      "Best booster: gbtree\n",
      "Best subsample: 0.5\n",
      "Best colsample_bytree: 0.7\n",
      "Best colsample_bylevel: 0.3\n",
      "Best colsample_bynode: 0.5\n",
      "Best reg_alpha: 3\n",
      "Best reg_lambda: 5\n",
      "Best scale_pos_weight: 5\n",
      "Best base_score: 0.4\n"
     ]
    }
   ],
   "source": [
    "### Tuning XGBClassifier READY ###\n",
    "print('tuning xgbc')\n",
    "xgbc = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "# learning_rate = [0,.1,.3,.5,.7,.9]\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "subsample = [.1,.3,.5,.7]\n",
    "colsample_bytree = [.1,.3,.5,.7]\n",
    "colsample_bylevel = [0,.1,.3,.5,.7,.9,1]\n",
    "colsample_bynode = [.1,.3,.5,.7]\n",
    "reg_alpha = [0,1,3,5,7]\n",
    "reg_lambda = [1,3,5,7]\n",
    "scale_pos_weight = [1,3,5,7]\n",
    "base_score = [.1,.2,.3,.4,.5]\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "#                        learning_rate=learning_rate, \n",
    "                       booster=booster, \n",
    "                       subsample=subsample, \n",
    "                       colsample_bytree=colsample_bytree, colsample_bylevel=colsample_bylevel, \n",
    "                       colsample_bynode=colsample_bynode, reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                       scale_pos_weight=scale_pos_weight,\n",
    "                       base_score=base_score\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(xgbc, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best booster:', best_model.best_estimator_.get_params()['booster'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n",
    "print('Best colsample_bytree:', best_model.best_estimator_.get_params()['colsample_bytree'])\n",
    "print('Best colsample_bylevel:', best_model.best_estimator_.get_params()['colsample_bylevel'])\n",
    "print('Best colsample_bynode:', best_model.best_estimator_.get_params()['colsample_bynode'])\n",
    "print('Best reg_alpha:', best_model.best_estimator_.get_params()['reg_alpha'])\n",
    "print('Best reg_lambda:', best_model.best_estimator_.get_params()['reg_lambda'])\n",
    "print('Best scale_pos_weight:', best_model.best_estimator_.get_params()['scale_pos_weight'])\n",
    "print('Best base_score:', best_model.best_estimator_.get_params()['base_score'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>395.304133</td>\n",
       "      <td>25.412146</td>\n",
       "      <td>0.587299</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985380</td>\n",
       "      <td>0.986789</td>\n",
       "      <td>0.987938</td>\n",
       "      <td>0.986694</td>\n",
       "      <td>0.987347</td>\n",
       "      <td>0.986711</td>\n",
       "      <td>0.987547</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>167.217239</td>\n",
       "      <td>11.165350</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.964751</td>\n",
       "      <td>0.967276</td>\n",
       "      <td>0.963401</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.966658</td>\n",
       "      <td>0.965024</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>120.322799</td>\n",
       "      <td>8.478442</td>\n",
       "      <td>0.271042</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961926</td>\n",
       "      <td>0.962940</td>\n",
       "      <td>0.965645</td>\n",
       "      <td>0.961338</td>\n",
       "      <td>0.963113</td>\n",
       "      <td>0.962412</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.963005</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>99.684576</td>\n",
       "      <td>5.295310</td>\n",
       "      <td>0.226211</td>\n",
       "      <td>0.090092</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>0.798554</td>\n",
       "      <td>0.805074</td>\n",
       "      <td>0.801674</td>\n",
       "      <td>0.804025</td>\n",
       "      <td>0.797907</td>\n",
       "      <td>0.806804</td>\n",
       "      <td>0.801659</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>203.073417</td>\n",
       "      <td>25.312860</td>\n",
       "      <td>0.677408</td>\n",
       "      <td>0.173201</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987628</td>\n",
       "      <td>0.988995</td>\n",
       "      <td>0.989606</td>\n",
       "      <td>0.988777</td>\n",
       "      <td>0.989180</td>\n",
       "      <td>0.988744</td>\n",
       "      <td>0.989257</td>\n",
       "      <td>0.988861</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>217.663624</td>\n",
       "      <td>223.572931</td>\n",
       "      <td>0.553119</td>\n",
       "      <td>0.219257</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984488</td>\n",
       "      <td>0.985504</td>\n",
       "      <td>0.986885</td>\n",
       "      <td>0.985129</td>\n",
       "      <td>0.985695</td>\n",
       "      <td>0.984936</td>\n",
       "      <td>0.986337</td>\n",
       "      <td>0.985598</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>103.800450</td>\n",
       "      <td>5.746303</td>\n",
       "      <td>0.394553</td>\n",
       "      <td>0.301245</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959114</td>\n",
       "      <td>0.960575</td>\n",
       "      <td>0.963978</td>\n",
       "      <td>0.959773</td>\n",
       "      <td>0.963111</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.962418</td>\n",
       "      <td>0.961344</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>305.320971</td>\n",
       "      <td>59.342621</td>\n",
       "      <td>0.606575</td>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985453</td>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.987005</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.986576</td>\n",
       "      <td>0.985673</td>\n",
       "      <td>0.987112</td>\n",
       "      <td>0.986233</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>102.538558</td>\n",
       "      <td>3.363380</td>\n",
       "      <td>0.421863</td>\n",
       "      <td>0.036955</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982312</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.984753</td>\n",
       "      <td>0.982607</td>\n",
       "      <td>0.983767</td>\n",
       "      <td>0.982724</td>\n",
       "      <td>0.984011</td>\n",
       "      <td>0.983344</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>173.911546</td>\n",
       "      <td>5.071884</td>\n",
       "      <td>0.338486</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982008</td>\n",
       "      <td>0.983266</td>\n",
       "      <td>0.984789</td>\n",
       "      <td>0.982414</td>\n",
       "      <td>0.983546</td>\n",
       "      <td>0.982560</td>\n",
       "      <td>0.983857</td>\n",
       "      <td>0.983167</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     395.304133     25.412146         0.587299        0.052609   \n",
       "1     167.217239     11.165350         0.489047        0.155241   \n",
       "2     120.322799      8.478442         0.271042        0.108111   \n",
       "3      99.684576      5.295310         0.226211        0.090092   \n",
       "4     203.073417     25.312860         0.677408        0.173201   \n",
       "5     217.663624    223.572931         0.553119        0.219257   \n",
       "6     103.800450      5.746303         0.394553        0.301245   \n",
       "7     305.320971     59.342621         0.606575        0.120821   \n",
       "8     102.538558      3.363380         0.421863        0.036955   \n",
       "9     173.911546      5.071884         0.338486        0.032476   \n",
       "\n",
       "  param_subsample param_scale_pos_weight param_reg_lambda param_reg_alpha  \\\n",
       "0             0.5                      3                5               5   \n",
       "1             0.7                      1                7               7   \n",
       "2             0.1                      7                3               7   \n",
       "3             0.5                      7                1               5   \n",
       "4             0.5                      5                5               3   \n",
       "5             0.7                      3                1               7   \n",
       "6             0.1                      7                7               3   \n",
       "7             0.1                      3                3               0   \n",
       "8             0.5                      3                5               7   \n",
       "9             0.3                      5                7               5   \n",
       "\n",
       "  param_max_depth param_colsample_bytree  ... split3_test_score  \\\n",
       "0               9                    0.5  ...          0.985380   \n",
       "1              11                    0.7  ...          0.963229   \n",
       "2               2                    0.7  ...          0.961926   \n",
       "3               9                    0.7  ...          0.802181   \n",
       "4              13                    0.7  ...          0.987628   \n",
       "5               9                    0.7  ...          0.984488   \n",
       "6               3                    0.3  ...          0.959114   \n",
       "7              13                    0.5  ...          0.985453   \n",
       "8              11                    0.1  ...          0.982312   \n",
       "9               7                    0.3  ...          0.982008   \n",
       "\n",
       "  split4_test_score split5_test_score split6_test_score split7_test_score  \\\n",
       "0          0.986789          0.987938          0.986694          0.987347   \n",
       "1          0.964751          0.967276          0.963401          0.966066   \n",
       "2          0.962940          0.965645          0.961338          0.963113   \n",
       "3          0.798554          0.805074          0.801674          0.804025   \n",
       "4          0.988995          0.989606          0.988777          0.989180   \n",
       "5          0.985504          0.986885          0.985129          0.985695   \n",
       "6          0.960575          0.963978          0.959773          0.963111   \n",
       "7          0.986445          0.987005          0.985547          0.986576   \n",
       "8          0.983491          0.984753          0.982607          0.983767   \n",
       "9          0.983266          0.984789          0.982414          0.983546   \n",
       "\n",
       "   split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.986711           0.987547         0.986829        0.000743   \n",
       "1           0.964305           0.966658         0.965024        0.001343   \n",
       "2           0.962412           0.964037         0.963005        0.001308   \n",
       "3           0.797907           0.806804         0.801659        0.003228   \n",
       "4           0.988744           0.989257         0.988861        0.000556   \n",
       "5           0.984936           0.986337         0.985598        0.000679   \n",
       "6           0.959996           0.962418         0.961344        0.001475   \n",
       "7           0.985673           0.987112         0.986233        0.000623   \n",
       "8           0.982724           0.984011         0.983344        0.000777   \n",
       "9           0.982560           0.983857         0.983167        0.000871   \n",
       "\n",
       "   rank_test_score  \n",
       "0                2  \n",
       "1                7  \n",
       "2                8  \n",
       "3               10  \n",
       "4                1  \n",
       "5                4  \n",
       "6                9  \n",
       "7                3  \n",
       "8                5  \n",
       "9                6  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### testing manual tuning ######\n",
    "### manual xbgc tuning\n",
    "max_depth = [3,5,7,9,11,13,15]\n",
    "# max_depth = [1,3,5]\n",
    "list_time_elapsed = []\n",
    "list_roc_auc_score = []\n",
    "for val in max_depth:\n",
    "    model_xgbc = XGBClassifier(max_depth=val, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_xgbc.fit(mod.X_train_test, mod.y_train)\n",
    "    y_pred_xgbc = model_xgbc.predict(mod.X_test)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    score_roc_auc = roc_auc_score(mod.y_test, y_pred_xgbc)\n",
    "    print(score_roc_auc) #delete\n",
    "    list_time_elapsed.append(elapsed_time)\n",
    "    list_roc_auc_score.append(score_roc_auc)\n",
    "    print('max depth: ', val)\n",
    "    print(confusion_matrix(mod.y_test, y_pred_xgbc))\n",
    "\n",
    "col_time_elapsed = pd.Series(list_time_elapsed)\n",
    "col_roc_score = pd.Series(list_roc_auc_score)\n",
    "col_max_depth = pd.Series(max_depth)\n",
    "df_results_xgbc = pd.concat([col_max_depth, col_roc_score,col_time_elapsed], \n",
    "                            keys=['max_depth', 'roc_auc_score', 'time_elap'], \n",
    "                            axis=1)\n",
    "print(df_results_xgbc)\n",
    "\n",
    "sns.lineplot(x='max_depth', y='roc_auc_score', data=df_results_xgbc)\n",
    "plt.title(\"XGBC manual tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Tuning RandomForestClassifier READY ####\n",
    "rfc = RandomForestClassifier(oob_score=False, n_jobs=1, random_state=42, verbose=1)\n",
    "\n",
    "n_estimators = [50,75,100,125,150,200]\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [3,5,7,9,11,13,15]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,2,4,6,8,10]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "max_leaf_nodes = [2,3,5,7,9,None]\n",
    "min_impurity_decrease = [0,.1,.3,.5,.7,.9]\n",
    "\n",
    "# n_estimators = [50,75,100,125]\n",
    "# criterion = ['gini']\n",
    "# max_depth = [2,3,4,5,6,7,None]\n",
    "# min_samples_split = [6,7,8,9]\n",
    "# min_samples_leaf = [1,2]\n",
    "# min_weight_fraction_leaf = [0]\n",
    "# max_features = ['auto', 'sqrt', 'log2', None]\n",
    "# max_leaf_nodes = [None]\n",
    "# min_impurity_decrease = [0]\n",
    "\n",
    "hyperparameters = dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split,\n",
    "                       min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                       min_impurity_decrease=min_impurity_decrease\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(rfc, hyperparameters, random_state=42, cv=5, verbose=5, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best max_leaf_nodes:', best_model.best_estimator_.get_params()['max_leaf_nodes'])\n",
    "print('Best min_impurity_decrease:', best_model.best_estimator_.get_params()['min_impurity_decrease'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### testing manual tuning ######\n",
    "# ### manual lr tuning\n",
    "\n",
    "# penalty = ['l1', 'l2', 'elasticnet','none']\n",
    "\n",
    "# list_time_elapsed = []\n",
    "# list_roc_auc_score = []\n",
    "# for val in penalty:\n",
    "#     model = LogisticRegression(penalty=val, n_jobs=1, random_state=42)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     model.fit(mod.X_train, mod.y_train)\n",
    "#     y_pred = model.predict(mod.X_test)\n",
    "#     elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "#     score_roc_auc = roc_auc_score(mod.y_test, y_pred)\n",
    "#     print(score_roc_auc) #delete\n",
    "#     list_time_elapsed.append(elapsed_time)\n",
    "#     list_roc_auc_score.append(score_roc_auc)\n",
    "#     print('penalty: ', val)\n",
    "#     print(confusion_matrix(mod.y_test, y_pred))\n",
    "\n",
    "# col_time_elapsed = pd.Series(list_time_elapsed)\n",
    "# col_roc_score = pd.Series(list_roc_auc_score)\n",
    "# col_penalty = pd.Series(penalty) ### UPDATE\n",
    "# df_results = pd.concat([col_max_depth, col_roc_score,col_time_elapsed], \n",
    "#                             keys=['penalty', 'roc_auc_score', 'time_elap'], \n",
    "#                             axis=1)\n",
    "# print(df_results)\n",
    "\n",
    "# sns.lineplot(x='penalty', y='roc_auc_score', data=df_results_xgbc)\n",
    "# plt.title(\"lr manual tuning\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### LR Tuning ####\n",
    "lr = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "\n",
    "penalty = ['l1', 'l2', 'elasticnet','none']\n",
    "tol = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "C = [1e-1,.2,.3,.5,.7,1]\n",
    "fit_intercept = [True,False]\n",
    "intercept_scaling = [1,.1,.01,.001]\n",
    "class_weight = ['balanced', None]\n",
    "solver = ['newton-cg', 'lbfgs', 'sag']#, 'liblinear','saga']\n",
    "max_iter = [50,75,100,150,200]\n",
    "multi_class = ['auto', 'ovr', 'multinomial']\n",
    "l1_ratio = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "\n",
    "\n",
    "hyperparameters = dict(penalty=penalty, \n",
    "                       tol=tol, \n",
    "                       C=C, \n",
    "                       fit_intercept=fit_intercept,\n",
    "                       intercept_scaling=intercept_scaling, \n",
    "                       class_weight=class_weight,\n",
    "                       solver=solver, \n",
    "                       max_iter=max_iter\n",
    "                       multi_class=multi_class, \n",
    "                       l1_ratio=l1_ratio\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best tol:', best_model.best_estimator_.get_params()['tol'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best fit_intercept:', best_model.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best intercept_scaling:', best_model.best_estimator_.get_params()['intercept_scaling'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best max_iter:', best_model.best_estimator_.get_params()['max_iter'])\n",
    "print('Best multi_class:', best_model.best_estimator_.get_params()['multi_class'])\n",
    "print('Best l1_ratio:', best_model.best_estimator_.get_params()['l1_ratio'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(mod.y_target[mod.y_target==0].isnull())\n",
    "df_temp = pd.concat([mod.X_features, mod.y_target], keys=['features','target'],axis=1).sample(n=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.X_features_test = df_temp.features\n",
    "mod.y_target_test = df_temp.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Tuning DTC READY ####\n",
    "dt = DecisionTreeClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_depth = [3,5,7,9,11]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,3,5,7,9]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "hyperparameters = dict(criterion=criterion, splitter=splitter, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                       min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, class_weight=class_weight\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(dt, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    " \n",
    "# best hyper parameters\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best splitter:', best_model.best_estimator_.get_params()['splitter'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.096819\n",
      "0:\tlearn: 0.5974147\ttotal: 368ms\tremaining: 6m 7s\n",
      "1:\tlearn: 0.5392678\ttotal: 630ms\tremaining: 5m 14s\n",
      "2:\tlearn: 0.4915173\ttotal: 784ms\tremaining: 4m 20s\n",
      "3:\tlearn: 0.4398323\ttotal: 929ms\tremaining: 3m 51s\n",
      "4:\tlearn: 0.4039320\ttotal: 1.55s\tremaining: 5m 8s\n",
      "5:\tlearn: 0.3829470\ttotal: 1.72s\tremaining: 4m 44s\n",
      "6:\tlearn: 0.3573045\ttotal: 1.87s\tremaining: 4m 25s\n",
      "7:\tlearn: 0.3330280\ttotal: 2.03s\tremaining: 4m 12s\n",
      "8:\tlearn: 0.3187945\ttotal: 2.42s\tremaining: 4m 27s\n",
      "9:\tlearn: 0.3050905\ttotal: 2.61s\tremaining: 4m 18s\n",
      "10:\tlearn: 0.2945899\ttotal: 2.96s\tremaining: 4m 25s\n",
      "11:\tlearn: 0.2855216\ttotal: 3.36s\tremaining: 4m 36s\n",
      "12:\tlearn: 0.2754627\ttotal: 3.92s\tremaining: 4m 57s\n",
      "13:\tlearn: 0.2699601\ttotal: 4.07s\tremaining: 4m 46s\n",
      "14:\tlearn: 0.2635532\ttotal: 4.38s\tremaining: 4m 47s\n",
      "15:\tlearn: 0.2591132\ttotal: 4.54s\tremaining: 4m 39s\n",
      "16:\tlearn: 0.2549411\ttotal: 4.68s\tremaining: 4m 30s\n",
      "17:\tlearn: 0.2482291\ttotal: 4.82s\tremaining: 4m 22s\n",
      "18:\tlearn: 0.2446053\ttotal: 4.97s\tremaining: 4m 16s\n",
      "19:\tlearn: 0.2412611\ttotal: 5.11s\tremaining: 4m 10s\n",
      "20:\tlearn: 0.2383919\ttotal: 5.4s\tremaining: 4m 11s\n",
      "21:\tlearn: 0.2359875\ttotal: 5.58s\tremaining: 4m 7s\n",
      "22:\tlearn: 0.2335099\ttotal: 5.74s\tremaining: 4m 4s\n",
      "23:\tlearn: 0.2316011\ttotal: 5.87s\tremaining: 3m 58s\n",
      "24:\tlearn: 0.2294379\ttotal: 6.02s\tremaining: 3m 54s\n",
      "25:\tlearn: 0.2275914\ttotal: 6.14s\tremaining: 3m 50s\n",
      "26:\tlearn: 0.2257007\ttotal: 6.45s\tremaining: 3m 52s\n",
      "27:\tlearn: 0.2220940\ttotal: 6.6s\tremaining: 3m 48s\n",
      "28:\tlearn: 0.2203052\ttotal: 6.73s\tremaining: 3m 45s\n",
      "29:\tlearn: 0.2168747\ttotal: 6.85s\tremaining: 3m 41s\n",
      "30:\tlearn: 0.2126009\ttotal: 7s\tremaining: 3m 38s\n",
      "31:\tlearn: 0.2112563\ttotal: 7.19s\tremaining: 3m 37s\n",
      "32:\tlearn: 0.2103443\ttotal: 7.79s\tremaining: 3m 48s\n",
      "33:\tlearn: 0.2083814\ttotal: 7.96s\tremaining: 3m 46s\n",
      "34:\tlearn: 0.2070996\ttotal: 8.19s\tremaining: 3m 45s\n",
      "35:\tlearn: 0.2059587\ttotal: 8.59s\tremaining: 3m 49s\n",
      "36:\tlearn: 0.2049042\ttotal: 8.73s\tremaining: 3m 47s\n",
      "37:\tlearn: 0.2021214\ttotal: 8.87s\tremaining: 3m 44s\n",
      "38:\tlearn: 0.2012388\ttotal: 8.99s\tremaining: 3m 41s\n",
      "39:\tlearn: 0.2004438\ttotal: 9.11s\tremaining: 3m 38s\n",
      "40:\tlearn: 0.1987920\ttotal: 9.37s\tremaining: 3m 39s\n",
      "41:\tlearn: 0.1965363\ttotal: 9.52s\tremaining: 3m 37s\n",
      "42:\tlearn: 0.1956851\ttotal: 9.63s\tremaining: 3m 34s\n",
      "43:\tlearn: 0.1949848\ttotal: 9.75s\tremaining: 3m 31s\n",
      "44:\tlearn: 0.1941251\ttotal: 9.92s\tremaining: 3m 30s\n",
      "45:\tlearn: 0.1934626\ttotal: 10s\tremaining: 3m 28s\n",
      "46:\tlearn: 0.1918788\ttotal: 10.3s\tremaining: 3m 29s\n",
      "47:\tlearn: 0.1910792\ttotal: 10.6s\tremaining: 3m 29s\n",
      "48:\tlearn: 0.1904645\ttotal: 10.7s\tremaining: 3m 28s\n",
      "49:\tlearn: 0.1899533\ttotal: 10.8s\tremaining: 3m 26s\n",
      "50:\tlearn: 0.1892106\ttotal: 11s\tremaining: 3m 23s\n",
      "51:\tlearn: 0.1886490\ttotal: 11.1s\tremaining: 3m 21s\n",
      "52:\tlearn: 0.1872483\ttotal: 11.4s\tremaining: 3m 23s\n",
      "53:\tlearn: 0.1867921\ttotal: 12s\tremaining: 3m 30s\n",
      "54:\tlearn: 0.1862311\ttotal: 12.2s\tremaining: 3m 29s\n",
      "55:\tlearn: 0.1855280\ttotal: 12.5s\tremaining: 3m 30s\n",
      "56:\tlearn: 0.1847381\ttotal: 12.7s\tremaining: 3m 29s\n",
      "57:\tlearn: 0.1843034\ttotal: 12.9s\tremaining: 3m 29s\n",
      "58:\tlearn: 0.1838649\ttotal: 13s\tremaining: 3m 27s\n",
      "59:\tlearn: 0.1826363\ttotal: 13.4s\tremaining: 3m 30s\n",
      "60:\tlearn: 0.1821428\ttotal: 13.6s\tremaining: 3m 28s\n",
      "61:\tlearn: 0.1815356\ttotal: 13.7s\tremaining: 3m 27s\n",
      "62:\tlearn: 0.1811719\ttotal: 13.8s\tremaining: 3m 25s\n",
      "63:\tlearn: 0.1805213\ttotal: 14.5s\tremaining: 3m 32s\n",
      "64:\tlearn: 0.1799528\ttotal: 14.7s\tremaining: 3m 31s\n",
      "65:\tlearn: 0.1793772\ttotal: 14.8s\tremaining: 3m 30s\n",
      "66:\tlearn: 0.1789399\ttotal: 15s\tremaining: 3m 28s\n",
      "67:\tlearn: 0.1784458\ttotal: 15.1s\tremaining: 3m 27s\n",
      "68:\tlearn: 0.1781352\ttotal: 15.7s\tremaining: 3m 32s\n",
      "69:\tlearn: 0.1777118\ttotal: 15.8s\tremaining: 3m 30s\n",
      "70:\tlearn: 0.1761632\ttotal: 16s\tremaining: 3m 28s\n",
      "71:\tlearn: 0.1757768\ttotal: 16.1s\tremaining: 3m 27s\n",
      "72:\tlearn: 0.1754701\ttotal: 16.3s\tremaining: 3m 26s\n",
      "73:\tlearn: 0.1750342\ttotal: 16.4s\tremaining: 3m 25s\n",
      "74:\tlearn: 0.1747301\ttotal: 16.5s\tremaining: 3m 24s\n",
      "75:\tlearn: 0.1743965\ttotal: 16.7s\tremaining: 3m 22s\n",
      "76:\tlearn: 0.1740934\ttotal: 16.9s\tremaining: 3m 22s\n",
      "77:\tlearn: 0.1736384\ttotal: 17s\tremaining: 3m 20s\n",
      "78:\tlearn: 0.1733267\ttotal: 17.3s\tremaining: 3m 21s\n",
      "79:\tlearn: 0.1729972\ttotal: 17.5s\tremaining: 3m 21s\n",
      "80:\tlearn: 0.1715890\ttotal: 17.6s\tremaining: 3m 20s\n",
      "81:\tlearn: 0.1711535\ttotal: 17.8s\tremaining: 3m 18s\n",
      "82:\tlearn: 0.1709210\ttotal: 17.9s\tremaining: 3m 17s\n",
      "83:\tlearn: 0.1705091\ttotal: 18s\tremaining: 3m 16s\n",
      "84:\tlearn: 0.1702137\ttotal: 18.1s\tremaining: 3m 14s\n",
      "85:\tlearn: 0.1699296\ttotal: 18.9s\tremaining: 3m 21s\n",
      "86:\tlearn: 0.1696074\ttotal: 19s\tremaining: 3m 19s\n",
      "87:\tlearn: 0.1693427\ttotal: 19.6s\tremaining: 3m 23s\n",
      "88:\tlearn: 0.1690289\ttotal: 19.8s\tremaining: 3m 22s\n",
      "89:\tlearn: 0.1687006\ttotal: 19.9s\tremaining: 3m 21s\n",
      "90:\tlearn: 0.1683796\ttotal: 20s\tremaining: 3m 20s\n",
      "91:\tlearn: 0.1681888\ttotal: 20.2s\tremaining: 3m 19s\n",
      "92:\tlearn: 0.1679253\ttotal: 20.4s\tremaining: 3m 19s\n",
      "93:\tlearn: 0.1676538\ttotal: 20.6s\tremaining: 3m 18s\n",
      "94:\tlearn: 0.1673396\ttotal: 20.8s\tremaining: 3m 17s\n",
      "95:\tlearn: 0.1670924\ttotal: 20.9s\tremaining: 3m 17s\n",
      "96:\tlearn: 0.1666612\ttotal: 21s\tremaining: 3m 15s\n",
      "97:\tlearn: 0.1660442\ttotal: 21.3s\tremaining: 3m 16s\n",
      "98:\tlearn: 0.1657754\ttotal: 21.5s\tremaining: 3m 15s\n",
      "99:\tlearn: 0.1655306\ttotal: 21.6s\tremaining: 3m 14s\n",
      "100:\tlearn: 0.1651446\ttotal: 21.7s\tremaining: 3m 13s\n",
      "101:\tlearn: 0.1648770\ttotal: 21.9s\tremaining: 3m 12s\n",
      "102:\tlearn: 0.1646894\ttotal: 22s\tremaining: 3m 11s\n",
      "103:\tlearn: 0.1644829\ttotal: 22.8s\tremaining: 3m 16s\n",
      "104:\tlearn: 0.1642456\ttotal: 24.1s\tremaining: 3m 25s\n",
      "105:\tlearn: 0.1640627\ttotal: 24.5s\tremaining: 3m 26s\n",
      "106:\tlearn: 0.1638724\ttotal: 24.9s\tremaining: 3m 27s\n",
      "107:\tlearn: 0.1636659\ttotal: 25.3s\tremaining: 3m 28s\n",
      "108:\tlearn: 0.1634403\ttotal: 25.6s\tremaining: 3m 29s\n",
      "109:\tlearn: 0.1631823\ttotal: 26.2s\tremaining: 3m 32s\n",
      "110:\tlearn: 0.1627908\ttotal: 26.6s\tremaining: 3m 32s\n",
      "111:\tlearn: 0.1624420\ttotal: 26.9s\tremaining: 3m 33s\n",
      "112:\tlearn: 0.1622046\ttotal: 27.1s\tremaining: 3m 33s\n",
      "113:\tlearn: 0.1620071\ttotal: 27.6s\tremaining: 3m 34s\n",
      "114:\tlearn: 0.1617991\ttotal: 27.8s\tremaining: 3m 33s\n",
      "115:\tlearn: 0.1615468\ttotal: 28.2s\tremaining: 3m 34s\n",
      "116:\tlearn: 0.1613376\ttotal: 28.3s\tremaining: 3m 33s\n",
      "117:\tlearn: 0.1610066\ttotal: 28.6s\tremaining: 3m 33s\n",
      "118:\tlearn: 0.1607932\ttotal: 28.9s\tremaining: 3m 33s\n",
      "119:\tlearn: 0.1600555\ttotal: 29.3s\tremaining: 3m 34s\n",
      "120:\tlearn: 0.1597351\ttotal: 29.6s\tremaining: 3m 34s\n",
      "121:\tlearn: 0.1595239\ttotal: 29.9s\tremaining: 3m 35s\n",
      "122:\tlearn: 0.1592334\ttotal: 30.2s\tremaining: 3m 35s\n",
      "123:\tlearn: 0.1588922\ttotal: 30.6s\tremaining: 3m 35s\n",
      "124:\tlearn: 0.1586662\ttotal: 30.9s\tremaining: 3m 36s\n",
      "125:\tlearn: 0.1584707\ttotal: 31.2s\tremaining: 3m 36s\n",
      "126:\tlearn: 0.1581715\ttotal: 31.6s\tremaining: 3m 37s\n",
      "127:\tlearn: 0.1579768\ttotal: 31.9s\tremaining: 3m 37s\n",
      "128:\tlearn: 0.1578215\ttotal: 32.3s\tremaining: 3m 37s\n",
      "129:\tlearn: 0.1575792\ttotal: 32.4s\tremaining: 3m 36s\n",
      "130:\tlearn: 0.1574042\ttotal: 32.5s\tremaining: 3m 35s\n",
      "131:\tlearn: 0.1566614\ttotal: 33.1s\tremaining: 3m 37s\n",
      "132:\tlearn: 0.1564462\ttotal: 33.3s\tremaining: 3m 37s\n",
      "133:\tlearn: 0.1562204\ttotal: 33.5s\tremaining: 3m 36s\n",
      "134:\tlearn: 0.1560241\ttotal: 33.8s\tremaining: 3m 36s\n",
      "135:\tlearn: 0.1557848\ttotal: 33.9s\tremaining: 3m 35s\n",
      "136:\tlearn: 0.1555831\ttotal: 34s\tremaining: 3m 34s\n",
      "137:\tlearn: 0.1548319\ttotal: 34.3s\tremaining: 3m 34s\n",
      "138:\tlearn: 0.1546218\ttotal: 34.4s\tremaining: 3m 33s\n",
      "139:\tlearn: 0.1543601\ttotal: 34.6s\tremaining: 3m 32s\n",
      "140:\tlearn: 0.1541231\ttotal: 34.9s\tremaining: 3m 32s\n",
      "141:\tlearn: 0.1538508\ttotal: 35s\tremaining: 3m 31s\n",
      "142:\tlearn: 0.1536259\ttotal: 35.6s\tremaining: 3m 33s\n",
      "143:\tlearn: 0.1526311\ttotal: 36s\tremaining: 3m 33s\n",
      "144:\tlearn: 0.1524919\ttotal: 36.1s\tremaining: 3m 32s\n",
      "145:\tlearn: 0.1522810\ttotal: 36.5s\tremaining: 3m 33s\n",
      "146:\tlearn: 0.1520336\ttotal: 36.7s\tremaining: 3m 32s\n",
      "147:\tlearn: 0.1519036\ttotal: 36.9s\tremaining: 3m 32s\n",
      "148:\tlearn: 0.1516133\ttotal: 37.1s\tremaining: 3m 32s\n",
      "149:\tlearn: 0.1514692\ttotal: 37.6s\tremaining: 3m 32s\n",
      "150:\tlearn: 0.1512941\ttotal: 38s\tremaining: 3m 33s\n",
      "151:\tlearn: 0.1510755\ttotal: 38.2s\tremaining: 3m 32s\n",
      "152:\tlearn: 0.1509078\ttotal: 38.3s\tremaining: 3m 32s\n",
      "153:\tlearn: 0.1507434\ttotal: 38.5s\tremaining: 3m 31s\n",
      "154:\tlearn: 0.1504749\ttotal: 38.8s\tremaining: 3m 31s\n",
      "155:\tlearn: 0.1503149\ttotal: 39s\tremaining: 3m 30s\n",
      "156:\tlearn: 0.1501463\ttotal: 39.5s\tremaining: 3m 31s\n",
      "157:\tlearn: 0.1499482\ttotal: 39.7s\tremaining: 3m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158:\tlearn: 0.1497307\ttotal: 39.9s\tremaining: 3m 30s\n",
      "159:\tlearn: 0.1494894\ttotal: 40.1s\tremaining: 3m 30s\n",
      "160:\tlearn: 0.1492489\ttotal: 40.2s\tremaining: 3m 29s\n",
      "161:\tlearn: 0.1490791\ttotal: 40.4s\tremaining: 3m 28s\n",
      "162:\tlearn: 0.1489446\ttotal: 40.6s\tremaining: 3m 28s\n",
      "163:\tlearn: 0.1487970\ttotal: 40.9s\tremaining: 3m 28s\n",
      "164:\tlearn: 0.1486638\ttotal: 41s\tremaining: 3m 27s\n",
      "165:\tlearn: 0.1485293\ttotal: 41.2s\tremaining: 3m 26s\n",
      "166:\tlearn: 0.1483947\ttotal: 41.3s\tremaining: 3m 25s\n",
      "167:\tlearn: 0.1482535\ttotal: 41.8s\tremaining: 3m 26s\n",
      "168:\tlearn: 0.1480183\ttotal: 41.9s\tremaining: 3m 26s\n",
      "169:\tlearn: 0.1478997\ttotal: 42s\tremaining: 3m 25s\n",
      "170:\tlearn: 0.1477164\ttotal: 42.2s\tremaining: 3m 24s\n",
      "171:\tlearn: 0.1475409\ttotal: 42.5s\tremaining: 3m 24s\n",
      "172:\tlearn: 0.1473429\ttotal: 42.7s\tremaining: 3m 24s\n",
      "173:\tlearn: 0.1471793\ttotal: 42.9s\tremaining: 3m 23s\n",
      "174:\tlearn: 0.1470081\ttotal: 43s\tremaining: 3m 22s\n",
      "175:\tlearn: 0.1466012\ttotal: 43.3s\tremaining: 3m 22s\n",
      "176:\tlearn: 0.1464606\ttotal: 43.4s\tremaining: 3m 21s\n",
      "177:\tlearn: 0.1462991\ttotal: 43.7s\tremaining: 3m 21s\n",
      "178:\tlearn: 0.1461685\ttotal: 43.9s\tremaining: 3m 21s\n",
      "179:\tlearn: 0.1460496\ttotal: 44s\tremaining: 3m 20s\n",
      "180:\tlearn: 0.1454466\ttotal: 44.2s\tremaining: 3m 19s\n",
      "181:\tlearn: 0.1453337\ttotal: 44.3s\tremaining: 3m 19s\n",
      "182:\tlearn: 0.1451605\ttotal: 44.5s\tremaining: 3m 18s\n",
      "183:\tlearn: 0.1450512\ttotal: 44.9s\tremaining: 3m 18s\n",
      "184:\tlearn: 0.1449432\ttotal: 45.4s\tremaining: 3m 20s\n",
      "185:\tlearn: 0.1448410\ttotal: 45.8s\tremaining: 3m 20s\n",
      "186:\tlearn: 0.1446888\ttotal: 46s\tremaining: 3m 20s\n",
      "187:\tlearn: 0.1445983\ttotal: 46.5s\tremaining: 3m 20s\n",
      "188:\tlearn: 0.1443339\ttotal: 46.7s\tremaining: 3m 20s\n",
      "189:\tlearn: 0.1441588\ttotal: 46.9s\tremaining: 3m 19s\n",
      "190:\tlearn: 0.1440125\ttotal: 47s\tremaining: 3m 19s\n",
      "191:\tlearn: 0.1438196\ttotal: 47.3s\tremaining: 3m 19s\n",
      "192:\tlearn: 0.1436543\ttotal: 47.5s\tremaining: 3m 18s\n",
      "193:\tlearn: 0.1435081\ttotal: 47.6s\tremaining: 3m 17s\n",
      "194:\tlearn: 0.1433712\ttotal: 47.8s\tremaining: 3m 17s\n",
      "195:\tlearn: 0.1432660\ttotal: 48s\tremaining: 3m 16s\n",
      "196:\tlearn: 0.1431418\ttotal: 48.3s\tremaining: 3m 16s\n",
      "197:\tlearn: 0.1429785\ttotal: 48.8s\tremaining: 3m 17s\n",
      "198:\tlearn: 0.1428631\ttotal: 48.9s\tremaining: 3m 17s\n",
      "199:\tlearn: 0.1427299\ttotal: 49.3s\tremaining: 3m 17s\n",
      "200:\tlearn: 0.1426115\ttotal: 49.4s\tremaining: 3m 16s\n",
      "201:\tlearn: 0.1424951\ttotal: 49.5s\tremaining: 3m 15s\n",
      "202:\tlearn: 0.1423584\ttotal: 49.6s\tremaining: 3m 14s\n",
      "203:\tlearn: 0.1422327\ttotal: 49.8s\tremaining: 3m 14s\n",
      "204:\tlearn: 0.1420714\ttotal: 49.9s\tremaining: 3m 13s\n",
      "205:\tlearn: 0.1419678\ttotal: 50s\tremaining: 3m 12s\n",
      "206:\tlearn: 0.1418310\ttotal: 50.3s\tremaining: 3m 12s\n",
      "207:\tlearn: 0.1416795\ttotal: 50.5s\tremaining: 3m 12s\n",
      "208:\tlearn: 0.1415469\ttotal: 50.7s\tremaining: 3m 11s\n",
      "209:\tlearn: 0.1414718\ttotal: 50.8s\tremaining: 3m 11s\n",
      "210:\tlearn: 0.1413555\ttotal: 50.9s\tremaining: 3m 10s\n",
      "211:\tlearn: 0.1412811\ttotal: 51s\tremaining: 3m 9s\n",
      "212:\tlearn: 0.1411161\ttotal: 51.3s\tremaining: 3m 9s\n",
      "213:\tlearn: 0.1409606\ttotal: 51.5s\tremaining: 3m 9s\n",
      "214:\tlearn: 0.1408133\ttotal: 51.6s\tremaining: 3m 8s\n",
      "215:\tlearn: 0.1407069\ttotal: 51.7s\tremaining: 3m 7s\n",
      "216:\tlearn: 0.1405669\ttotal: 51.8s\tremaining: 3m 7s\n",
      "217:\tlearn: 0.1404664\ttotal: 52s\tremaining: 3m 6s\n",
      "218:\tlearn: 0.1403079\ttotal: 52.2s\tremaining: 3m 6s\n",
      "219:\tlearn: 0.1402145\ttotal: 52.3s\tremaining: 3m 5s\n",
      "220:\tlearn: 0.1400721\ttotal: 52.5s\tremaining: 3m 4s\n",
      "221:\tlearn: 0.1399481\ttotal: 52.6s\tremaining: 3m 4s\n",
      "222:\tlearn: 0.1398522\ttotal: 52.7s\tremaining: 3m 3s\n",
      "223:\tlearn: 0.1397671\ttotal: 52.9s\tremaining: 3m 3s\n",
      "224:\tlearn: 0.1396043\ttotal: 53s\tremaining: 3m 2s\n",
      "225:\tlearn: 0.1395402\ttotal: 53.1s\tremaining: 3m 1s\n",
      "226:\tlearn: 0.1394517\ttotal: 53.4s\tremaining: 3m 1s\n",
      "227:\tlearn: 0.1393813\ttotal: 53.6s\tremaining: 3m 1s\n",
      "228:\tlearn: 0.1392426\ttotal: 53.7s\tremaining: 3m\n",
      "229:\tlearn: 0.1390700\ttotal: 53.8s\tremaining: 3m\n",
      "230:\tlearn: 0.1389432\ttotal: 53.9s\tremaining: 2m 59s\n",
      "231:\tlearn: 0.1388759\ttotal: 54s\tremaining: 2m 58s\n",
      "232:\tlearn: 0.1387802\ttotal: 54.2s\tremaining: 2m 58s\n",
      "233:\tlearn: 0.1386503\ttotal: 54.6s\tremaining: 2m 58s\n",
      "234:\tlearn: 0.1385341\ttotal: 54.8s\tremaining: 2m 58s\n",
      "235:\tlearn: 0.1384084\ttotal: 54.9s\tremaining: 2m 57s\n",
      "236:\tlearn: 0.1383096\ttotal: 55.3s\tremaining: 2m 58s\n",
      "237:\tlearn: 0.1381553\ttotal: 55.5s\tremaining: 2m 57s\n",
      "238:\tlearn: 0.1380479\ttotal: 55.6s\tremaining: 2m 57s\n",
      "239:\tlearn: 0.1379427\ttotal: 55.8s\tremaining: 2m 56s\n",
      "240:\tlearn: 0.1378563\ttotal: 55.9s\tremaining: 2m 56s\n",
      "241:\tlearn: 0.1376723\ttotal: 56s\tremaining: 2m 55s\n",
      "242:\tlearn: 0.1374863\ttotal: 56.3s\tremaining: 2m 55s\n",
      "243:\tlearn: 0.1373809\ttotal: 56.4s\tremaining: 2m 54s\n",
      "244:\tlearn: 0.1372505\ttotal: 56.5s\tremaining: 2m 54s\n",
      "245:\tlearn: 0.1371515\ttotal: 56.7s\tremaining: 2m 53s\n",
      "246:\tlearn: 0.1370041\ttotal: 56.9s\tremaining: 2m 53s\n",
      "247:\tlearn: 0.1368812\ttotal: 57s\tremaining: 2m 52s\n",
      "248:\tlearn: 0.1367834\ttotal: 57.2s\tremaining: 2m 52s\n",
      "249:\tlearn: 0.1366483\ttotal: 57.3s\tremaining: 2m 52s\n",
      "250:\tlearn: 0.1365246\ttotal: 57.5s\tremaining: 2m 51s\n",
      "251:\tlearn: 0.1363754\ttotal: 57.6s\tremaining: 2m 50s\n",
      "252:\tlearn: 0.1362252\ttotal: 57.7s\tremaining: 2m 50s\n",
      "253:\tlearn: 0.1360953\ttotal: 57.9s\tremaining: 2m 49s\n",
      "254:\tlearn: 0.1359971\ttotal: 58s\tremaining: 2m 49s\n",
      "255:\tlearn: 0.1358645\ttotal: 58.4s\tremaining: 2m 49s\n",
      "256:\tlearn: 0.1357774\ttotal: 58.6s\tremaining: 2m 49s\n",
      "257:\tlearn: 0.1355914\ttotal: 58.7s\tremaining: 2m 48s\n",
      "258:\tlearn: 0.1354726\ttotal: 58.8s\tremaining: 2m 48s\n",
      "259:\tlearn: 0.1353432\ttotal: 58.9s\tremaining: 2m 47s\n",
      "260:\tlearn: 0.1352484\ttotal: 59.2s\tremaining: 2m 47s\n",
      "261:\tlearn: 0.1351756\ttotal: 59.3s\tremaining: 2m 47s\n",
      "262:\tlearn: 0.1347057\ttotal: 59.5s\tremaining: 2m 46s\n",
      "263:\tlearn: 0.1346070\ttotal: 59.6s\tremaining: 2m 46s\n",
      "264:\tlearn: 0.1344967\ttotal: 59.7s\tremaining: 2m 45s\n",
      "265:\tlearn: 0.1344061\ttotal: 59.8s\tremaining: 2m 45s\n",
      "266:\tlearn: 0.1343491\ttotal: 59.9s\tremaining: 2m 44s\n",
      "267:\tlearn: 0.1342633\ttotal: 1m\tremaining: 2m 44s\n",
      "268:\tlearn: 0.1341197\ttotal: 1m\tremaining: 2m 43s\n",
      "269:\tlearn: 0.1339737\ttotal: 1m\tremaining: 2m 43s\n",
      "270:\tlearn: 0.1338810\ttotal: 1m\tremaining: 2m 42s\n",
      "271:\tlearn: 0.1337831\ttotal: 1m\tremaining: 2m 42s\n",
      "272:\tlearn: 0.1337230\ttotal: 1m\tremaining: 2m 41s\n",
      "273:\tlearn: 0.1336522\ttotal: 1m\tremaining: 2m 41s\n",
      "274:\tlearn: 0.1335394\ttotal: 1m 1s\tremaining: 2m 41s\n",
      "275:\tlearn: 0.1334538\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "276:\tlearn: 0.1333563\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "277:\tlearn: 0.1332882\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "278:\tlearn: 0.1331706\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "279:\tlearn: 0.1331034\ttotal: 1m 1s\tremaining: 2m 38s\n",
      "280:\tlearn: 0.1329774\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "281:\tlearn: 0.1328679\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "282:\tlearn: 0.1327747\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "283:\tlearn: 0.1327168\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "284:\tlearn: 0.1326333\ttotal: 1m 2s\tremaining: 2m 37s\n",
      "285:\tlearn: 0.1325626\ttotal: 1m 2s\tremaining: 2m 37s\n",
      "286:\tlearn: 0.1324565\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "287:\tlearn: 0.1324029\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "288:\tlearn: 0.1323007\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "289:\tlearn: 0.1322409\ttotal: 1m 3s\tremaining: 2m 35s\n",
      "290:\tlearn: 0.1321341\ttotal: 1m 3s\tremaining: 2m 35s\n",
      "291:\tlearn: 0.1320564\ttotal: 1m 3s\tremaining: 2m 34s\n",
      "292:\tlearn: 0.1319713\ttotal: 1m 3s\tremaining: 2m 34s\n",
      "293:\tlearn: 0.1319025\ttotal: 1m 4s\tremaining: 2m 33s\n",
      "294:\tlearn: 0.1318402\ttotal: 1m 4s\tremaining: 2m 33s\n",
      "295:\tlearn: 0.1317002\ttotal: 1m 4s\tremaining: 2m 33s\n",
      "296:\tlearn: 0.1316482\ttotal: 1m 4s\tremaining: 2m 32s\n",
      "297:\tlearn: 0.1315730\ttotal: 1m 4s\tremaining: 2m 32s\n",
      "298:\tlearn: 0.1315187\ttotal: 1m 4s\tremaining: 2m 31s\n",
      "299:\tlearn: 0.1314675\ttotal: 1m 4s\tremaining: 2m 31s\n",
      "300:\tlearn: 0.1313382\ttotal: 1m 5s\tremaining: 2m 30s\n",
      "301:\tlearn: 0.1312608\ttotal: 1m 5s\tremaining: 2m 31s\n",
      "302:\tlearn: 0.1311509\ttotal: 1m 5s\tremaining: 2m 31s\n",
      "303:\tlearn: 0.1307198\ttotal: 1m 5s\tremaining: 2m 30s\n",
      "304:\tlearn: 0.1305934\ttotal: 1m 5s\tremaining: 2m 30s\n",
      "305:\tlearn: 0.1304879\ttotal: 1m 6s\tremaining: 2m 29s\n",
      "306:\tlearn: 0.1304218\ttotal: 1m 6s\tremaining: 2m 29s\n",
      "307:\tlearn: 0.1303203\ttotal: 1m 6s\tremaining: 2m 29s\n",
      "308:\tlearn: 0.1302397\ttotal: 1m 6s\tremaining: 2m 28s\n",
      "309:\tlearn: 0.1301186\ttotal: 1m 6s\tremaining: 2m 28s\n",
      "310:\tlearn: 0.1300522\ttotal: 1m 6s\tremaining: 2m 28s\n",
      "311:\tlearn: 0.1299421\ttotal: 1m 6s\tremaining: 2m 27s\n",
      "312:\tlearn: 0.1298977\ttotal: 1m 7s\tremaining: 2m 27s\n",
      "313:\tlearn: 0.1298293\ttotal: 1m 7s\tremaining: 2m 27s\n",
      "314:\tlearn: 0.1297546\ttotal: 1m 7s\tremaining: 2m 26s\n",
      "315:\tlearn: 0.1296939\ttotal: 1m 7s\tremaining: 2m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316:\tlearn: 0.1296291\ttotal: 1m 7s\tremaining: 2m 25s\n",
      "317:\tlearn: 0.1295564\ttotal: 1m 7s\tremaining: 2m 25s\n",
      "318:\tlearn: 0.1294825\ttotal: 1m 7s\tremaining: 2m 25s\n",
      "319:\tlearn: 0.1293216\ttotal: 1m 8s\tremaining: 2m 25s\n",
      "320:\tlearn: 0.1292673\ttotal: 1m 8s\tremaining: 2m 24s\n",
      "321:\tlearn: 0.1291817\ttotal: 1m 8s\tremaining: 2m 24s\n",
      "322:\tlearn: 0.1291105\ttotal: 1m 8s\tremaining: 2m 23s\n",
      "323:\tlearn: 0.1290103\ttotal: 1m 8s\tremaining: 2m 23s\n",
      "324:\tlearn: 0.1289503\ttotal: 1m 8s\tremaining: 2m 23s\n",
      "325:\tlearn: 0.1288381\ttotal: 1m 8s\tremaining: 2m 22s\n",
      "326:\tlearn: 0.1287669\ttotal: 1m 9s\tremaining: 2m 22s\n",
      "327:\tlearn: 0.1285659\ttotal: 1m 9s\tremaining: 2m 22s\n",
      "328:\tlearn: 0.1283703\ttotal: 1m 9s\tremaining: 2m 22s\n",
      "329:\tlearn: 0.1282805\ttotal: 1m 9s\tremaining: 2m 21s\n",
      "330:\tlearn: 0.1282083\ttotal: 1m 9s\tremaining: 2m 21s\n",
      "331:\tlearn: 0.1281626\ttotal: 1m 9s\tremaining: 2m 20s\n",
      "332:\tlearn: 0.1280684\ttotal: 1m 10s\tremaining: 2m 21s\n",
      "333:\tlearn: 0.1279591\ttotal: 1m 10s\tremaining: 2m 20s\n",
      "334:\tlearn: 0.1278946\ttotal: 1m 10s\tremaining: 2m 20s\n",
      "335:\tlearn: 0.1277866\ttotal: 1m 10s\tremaining: 2m 19s\n",
      "336:\tlearn: 0.1276944\ttotal: 1m 10s\tremaining: 2m 19s\n",
      "337:\tlearn: 0.1276232\ttotal: 1m 11s\tremaining: 2m 19s\n",
      "338:\tlearn: 0.1275739\ttotal: 1m 11s\tremaining: 2m 19s\n",
      "339:\tlearn: 0.1274832\ttotal: 1m 11s\tremaining: 2m 18s\n",
      "340:\tlearn: 0.1274287\ttotal: 1m 11s\tremaining: 2m 18s\n",
      "341:\tlearn: 0.1273695\ttotal: 1m 11s\tremaining: 2m 17s\n",
      "342:\tlearn: 0.1273194\ttotal: 1m 11s\tremaining: 2m 17s\n",
      "343:\tlearn: 0.1272712\ttotal: 1m 11s\tremaining: 2m 17s\n",
      "344:\tlearn: 0.1272078\ttotal: 1m 12s\tremaining: 2m 17s\n",
      "345:\tlearn: 0.1271215\ttotal: 1m 12s\tremaining: 2m 16s\n",
      "346:\tlearn: 0.1270764\ttotal: 1m 12s\tremaining: 2m 16s\n",
      "347:\tlearn: 0.1269896\ttotal: 1m 12s\tremaining: 2m 16s\n",
      "348:\tlearn: 0.1269243\ttotal: 1m 12s\tremaining: 2m 15s\n",
      "349:\tlearn: 0.1268616\ttotal: 1m 13s\tremaining: 2m 15s\n",
      "350:\tlearn: 0.1268092\ttotal: 1m 13s\tremaining: 2m 15s\n",
      "351:\tlearn: 0.1267140\ttotal: 1m 13s\tremaining: 2m 15s\n",
      "352:\tlearn: 0.1266180\ttotal: 1m 13s\tremaining: 2m 15s\n",
      "353:\tlearn: 0.1265240\ttotal: 1m 13s\tremaining: 2m 14s\n",
      "354:\tlearn: 0.1264529\ttotal: 1m 13s\tremaining: 2m 14s\n",
      "355:\tlearn: 0.1264077\ttotal: 1m 14s\tremaining: 2m 13s\n",
      "356:\tlearn: 0.1263515\ttotal: 1m 14s\tremaining: 2m 13s\n",
      "357:\tlearn: 0.1262851\ttotal: 1m 14s\tremaining: 2m 13s\n",
      "358:\tlearn: 0.1262117\ttotal: 1m 14s\tremaining: 2m 13s\n",
      "359:\tlearn: 0.1261519\ttotal: 1m 14s\tremaining: 2m 12s\n",
      "360:\tlearn: 0.1261015\ttotal: 1m 14s\tremaining: 2m 12s\n",
      "361:\tlearn: 0.1258531\ttotal: 1m 14s\tremaining: 2m 11s\n",
      "362:\tlearn: 0.1257732\ttotal: 1m 14s\tremaining: 2m 11s\n",
      "363:\tlearn: 0.1256950\ttotal: 1m 15s\tremaining: 2m 11s\n",
      "364:\tlearn: 0.1256308\ttotal: 1m 15s\tremaining: 2m 11s\n",
      "365:\tlearn: 0.1255712\ttotal: 1m 15s\tremaining: 2m 10s\n",
      "366:\tlearn: 0.1254597\ttotal: 1m 15s\tremaining: 2m 10s\n",
      "367:\tlearn: 0.1254347\ttotal: 1m 15s\tremaining: 2m 9s\n",
      "368:\tlearn: 0.1254040\ttotal: 1m 15s\tremaining: 2m 9s\n",
      "369:\tlearn: 0.1253424\ttotal: 1m 15s\tremaining: 2m 9s\n",
      "370:\tlearn: 0.1252904\ttotal: 1m 16s\tremaining: 2m 8s\n",
      "371:\tlearn: 0.1252243\ttotal: 1m 16s\tremaining: 2m 8s\n",
      "372:\tlearn: 0.1251265\ttotal: 1m 16s\tremaining: 2m 9s\n",
      "373:\tlearn: 0.1250519\ttotal: 1m 19s\tremaining: 2m 13s\n",
      "374:\tlearn: 0.1249634\ttotal: 1m 21s\tremaining: 2m 15s\n",
      "375:\tlearn: 0.1248708\ttotal: 1m 22s\tremaining: 2m 17s\n",
      "376:\tlearn: 0.1247848\ttotal: 1m 28s\tremaining: 2m 26s\n",
      "377:\tlearn: 0.1246903\ttotal: 1m 29s\tremaining: 2m 26s\n",
      "378:\tlearn: 0.1246006\ttotal: 1m 29s\tremaining: 2m 26s\n",
      "379:\tlearn: 0.1245486\ttotal: 1m 29s\tremaining: 2m 26s\n",
      "380:\tlearn: 0.1244804\ttotal: 1m 29s\tremaining: 2m 25s\n",
      "381:\tlearn: 0.1243746\ttotal: 1m 29s\tremaining: 2m 25s\n",
      "382:\tlearn: 0.1243007\ttotal: 1m 30s\tremaining: 2m 25s\n",
      "383:\tlearn: 0.1242586\ttotal: 1m 30s\tremaining: 2m 25s\n",
      "384:\tlearn: 0.1242124\ttotal: 1m 30s\tremaining: 2m 24s\n",
      "385:\tlearn: 0.1241375\ttotal: 1m 30s\tremaining: 2m 24s\n",
      "386:\tlearn: 0.1240579\ttotal: 1m 30s\tremaining: 2m 24s\n",
      "387:\tlearn: 0.1240154\ttotal: 1m 31s\tremaining: 2m 23s\n",
      "388:\tlearn: 0.1239300\ttotal: 1m 31s\tremaining: 2m 23s\n",
      "389:\tlearn: 0.1238713\ttotal: 1m 31s\tremaining: 2m 23s\n",
      "390:\tlearn: 0.1238141\ttotal: 1m 31s\tremaining: 2m 22s\n",
      "391:\tlearn: 0.1237263\ttotal: 1m 31s\tremaining: 2m 22s\n",
      "392:\tlearn: 0.1236844\ttotal: 1m 31s\tremaining: 2m 22s\n",
      "393:\tlearn: 0.1236476\ttotal: 1m 32s\tremaining: 2m 22s\n",
      "394:\tlearn: 0.1236092\ttotal: 1m 32s\tremaining: 2m 21s\n",
      "395:\tlearn: 0.1235385\ttotal: 1m 32s\tremaining: 2m 21s\n",
      "396:\tlearn: 0.1234962\ttotal: 1m 33s\tremaining: 2m 22s\n",
      "397:\tlearn: 0.1234367\ttotal: 1m 34s\tremaining: 2m 22s\n",
      "398:\tlearn: 0.1233173\ttotal: 1m 34s\tremaining: 2m 22s\n",
      "399:\tlearn: 0.1232309\ttotal: 1m 34s\tremaining: 2m 21s\n",
      "400:\tlearn: 0.1231437\ttotal: 1m 34s\tremaining: 2m 21s\n",
      "401:\tlearn: 0.1230736\ttotal: 1m 34s\tremaining: 2m 21s\n",
      "402:\tlearn: 0.1229883\ttotal: 1m 35s\tremaining: 2m 20s\n",
      "403:\tlearn: 0.1229223\ttotal: 1m 35s\tremaining: 2m 20s\n",
      "404:\tlearn: 0.1228673\ttotal: 1m 35s\tremaining: 2m 20s\n",
      "405:\tlearn: 0.1227601\ttotal: 1m 35s\tremaining: 2m 19s\n",
      "406:\tlearn: 0.1226423\ttotal: 1m 35s\tremaining: 2m 19s\n",
      "407:\tlearn: 0.1225699\ttotal: 1m 35s\tremaining: 2m 19s\n",
      "408:\tlearn: 0.1224967\ttotal: 1m 36s\tremaining: 2m 19s\n",
      "409:\tlearn: 0.1224420\ttotal: 1m 36s\tremaining: 2m 19s\n",
      "410:\tlearn: 0.1223878\ttotal: 1m 37s\tremaining: 2m 19s\n",
      "411:\tlearn: 0.1223023\ttotal: 1m 37s\tremaining: 2m 18s\n",
      "412:\tlearn: 0.1222726\ttotal: 1m 37s\tremaining: 2m 18s\n",
      "413:\tlearn: 0.1222168\ttotal: 1m 37s\tremaining: 2m 18s\n",
      "414:\tlearn: 0.1221226\ttotal: 1m 37s\tremaining: 2m 17s\n",
      "415:\tlearn: 0.1220724\ttotal: 1m 37s\tremaining: 2m 17s\n",
      "416:\tlearn: 0.1219637\ttotal: 1m 38s\tremaining: 2m 17s\n",
      "417:\tlearn: 0.1218872\ttotal: 1m 38s\tremaining: 2m 16s\n",
      "418:\tlearn: 0.1217979\ttotal: 1m 38s\tremaining: 2m 16s\n",
      "419:\tlearn: 0.1217447\ttotal: 1m 38s\tremaining: 2m 15s\n",
      "420:\tlearn: 0.1216461\ttotal: 1m 38s\tremaining: 2m 15s\n",
      "421:\tlearn: 0.1215785\ttotal: 1m 38s\tremaining: 2m 15s\n",
      "422:\tlearn: 0.1215272\ttotal: 1m 38s\tremaining: 2m 14s\n",
      "423:\tlearn: 0.1214504\ttotal: 1m 39s\tremaining: 2m 14s\n",
      "424:\tlearn: 0.1213719\ttotal: 1m 39s\tremaining: 2m 14s\n",
      "425:\tlearn: 0.1213032\ttotal: 1m 39s\tremaining: 2m 13s\n",
      "426:\tlearn: 0.1212231\ttotal: 1m 39s\tremaining: 2m 13s\n",
      "427:\tlearn: 0.1211326\ttotal: 1m 39s\tremaining: 2m 13s\n",
      "428:\tlearn: 0.1210970\ttotal: 1m 39s\tremaining: 2m 12s\n",
      "429:\tlearn: 0.1210268\ttotal: 1m 42s\tremaining: 2m 15s\n",
      "430:\tlearn: 0.1209787\ttotal: 1m 45s\tremaining: 2m 19s\n",
      "431:\tlearn: 0.1209370\ttotal: 1m 47s\tremaining: 2m 21s\n",
      "432:\tlearn: 0.1208815\ttotal: 1m 48s\tremaining: 2m 22s\n",
      "433:\tlearn: 0.1208048\ttotal: 1m 48s\tremaining: 2m 22s\n",
      "434:\tlearn: 0.1207637\ttotal: 1m 49s\tremaining: 2m 21s\n",
      "435:\tlearn: 0.1206602\ttotal: 1m 49s\tremaining: 2m 21s\n",
      "436:\tlearn: 0.1206027\ttotal: 1m 49s\tremaining: 2m 21s\n",
      "437:\tlearn: 0.1205131\ttotal: 1m 49s\tremaining: 2m 20s\n",
      "438:\tlearn: 0.1204653\ttotal: 1m 49s\tremaining: 2m 20s\n",
      "439:\tlearn: 0.1204220\ttotal: 1m 49s\tremaining: 2m 19s\n",
      "440:\tlearn: 0.1203551\ttotal: 1m 50s\tremaining: 2m 19s\n",
      "441:\tlearn: 0.1202844\ttotal: 1m 50s\tremaining: 2m 19s\n",
      "442:\tlearn: 0.1202165\ttotal: 1m 50s\tremaining: 2m 18s\n",
      "443:\tlearn: 0.1201734\ttotal: 1m 50s\tremaining: 2m 18s\n",
      "444:\tlearn: 0.1201017\ttotal: 1m 50s\tremaining: 2m 17s\n",
      "445:\tlearn: 0.1200379\ttotal: 1m 50s\tremaining: 2m 17s\n",
      "446:\tlearn: 0.1200115\ttotal: 1m 50s\tremaining: 2m 17s\n",
      "447:\tlearn: 0.1199324\ttotal: 1m 51s\tremaining: 2m 16s\n",
      "448:\tlearn: 0.1198699\ttotal: 1m 51s\tremaining: 2m 16s\n",
      "449:\tlearn: 0.1198060\ttotal: 1m 51s\tremaining: 2m 16s\n",
      "450:\tlearn: 0.1197721\ttotal: 1m 51s\tremaining: 2m 15s\n",
      "451:\tlearn: 0.1196703\ttotal: 1m 51s\tremaining: 2m 15s\n",
      "452:\tlearn: 0.1196231\ttotal: 1m 51s\tremaining: 2m 14s\n",
      "453:\tlearn: 0.1195574\ttotal: 1m 51s\tremaining: 2m 14s\n",
      "454:\tlearn: 0.1195110\ttotal: 1m 52s\tremaining: 2m 14s\n",
      "455:\tlearn: 0.1194391\ttotal: 1m 52s\tremaining: 2m 14s\n",
      "456:\tlearn: 0.1193858\ttotal: 1m 52s\tremaining: 2m 13s\n",
      "457:\tlearn: 0.1193498\ttotal: 1m 52s\tremaining: 2m 13s\n",
      "458:\tlearn: 0.1192840\ttotal: 1m 53s\tremaining: 2m 13s\n",
      "459:\tlearn: 0.1191831\ttotal: 1m 53s\tremaining: 2m 13s\n",
      "460:\tlearn: 0.1191323\ttotal: 1m 53s\tremaining: 2m 12s\n",
      "461:\tlearn: 0.1190654\ttotal: 1m 53s\tremaining: 2m 12s\n",
      "462:\tlearn: 0.1190170\ttotal: 1m 53s\tremaining: 2m 12s\n",
      "463:\tlearn: 0.1189654\ttotal: 1m 54s\tremaining: 2m 11s\n",
      "464:\tlearn: 0.1189027\ttotal: 1m 54s\tremaining: 2m 11s\n",
      "465:\tlearn: 0.1188506\ttotal: 1m 54s\tremaining: 2m 11s\n",
      "466:\tlearn: 0.1188033\ttotal: 1m 54s\tremaining: 2m 10s\n",
      "467:\tlearn: 0.1187484\ttotal: 1m 54s\tremaining: 2m 10s\n",
      "468:\tlearn: 0.1187074\ttotal: 1m 54s\tremaining: 2m 10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469:\tlearn: 0.1186453\ttotal: 1m 55s\tremaining: 2m 9s\n",
      "470:\tlearn: 0.1186069\ttotal: 1m 55s\tremaining: 2m 9s\n",
      "471:\tlearn: 0.1185489\ttotal: 1m 55s\tremaining: 2m 9s\n",
      "472:\tlearn: 0.1185260\ttotal: 1m 55s\tremaining: 2m 8s\n",
      "473:\tlearn: 0.1184700\ttotal: 1m 55s\tremaining: 2m 8s\n",
      "474:\tlearn: 0.1184296\ttotal: 1m 55s\tremaining: 2m 7s\n",
      "475:\tlearn: 0.1183791\ttotal: 1m 55s\tremaining: 2m 7s\n",
      "476:\tlearn: 0.1183333\ttotal: 1m 56s\tremaining: 2m 7s\n",
      "477:\tlearn: 0.1182541\ttotal: 1m 56s\tremaining: 2m 7s\n",
      "478:\tlearn: 0.1181681\ttotal: 1m 56s\tremaining: 2m 6s\n",
      "479:\tlearn: 0.1181265\ttotal: 1m 56s\tremaining: 2m 6s\n",
      "480:\tlearn: 0.1180292\ttotal: 1m 56s\tremaining: 2m 5s\n",
      "481:\tlearn: 0.1179869\ttotal: 1m 56s\tremaining: 2m 5s\n",
      "482:\tlearn: 0.1179262\ttotal: 1m 57s\tremaining: 2m 5s\n",
      "483:\tlearn: 0.1179018\ttotal: 1m 57s\tremaining: 2m 5s\n",
      "484:\tlearn: 0.1178371\ttotal: 1m 57s\tremaining: 2m 4s\n",
      "485:\tlearn: 0.1178089\ttotal: 1m 57s\tremaining: 2m 4s\n",
      "486:\tlearn: 0.1177494\ttotal: 1m 57s\tremaining: 2m 4s\n",
      "487:\tlearn: 0.1176763\ttotal: 1m 57s\tremaining: 2m 3s\n",
      "488:\tlearn: 0.1175601\ttotal: 1m 58s\tremaining: 2m 3s\n",
      "489:\tlearn: 0.1175320\ttotal: 1m 58s\tremaining: 2m 3s\n",
      "490:\tlearn: 0.1174775\ttotal: 1m 58s\tremaining: 2m 2s\n",
      "491:\tlearn: 0.1174207\ttotal: 1m 58s\tremaining: 2m 2s\n",
      "492:\tlearn: 0.1173477\ttotal: 1m 58s\tremaining: 2m 2s\n",
      "493:\tlearn: 0.1173202\ttotal: 1m 58s\tremaining: 2m 1s\n",
      "494:\tlearn: 0.1172286\ttotal: 1m 59s\tremaining: 2m 1s\n",
      "495:\tlearn: 0.1171725\ttotal: 1m 59s\tremaining: 2m 1s\n",
      "496:\tlearn: 0.1171266\ttotal: 1m 59s\tremaining: 2m\n",
      "497:\tlearn: 0.1171054\ttotal: 1m 59s\tremaining: 2m\n",
      "498:\tlearn: 0.1170490\ttotal: 1m 59s\tremaining: 2m\n",
      "499:\tlearn: 0.1169770\ttotal: 1m 59s\tremaining: 1m 59s\n",
      "500:\tlearn: 0.1169155\ttotal: 1m 59s\tremaining: 1m 59s\n",
      "501:\tlearn: 0.1168393\ttotal: 2m\tremaining: 1m 59s\n",
      "502:\tlearn: 0.1167867\ttotal: 2m\tremaining: 1m 58s\n",
      "503:\tlearn: 0.1167247\ttotal: 2m\tremaining: 1m 58s\n",
      "504:\tlearn: 0.1166516\ttotal: 2m\tremaining: 1m 58s\n",
      "505:\tlearn: 0.1166119\ttotal: 2m\tremaining: 1m 57s\n",
      "506:\tlearn: 0.1165548\ttotal: 2m\tremaining: 1m 57s\n",
      "507:\tlearn: 0.1164523\ttotal: 2m 1s\tremaining: 1m 57s\n",
      "508:\tlearn: 0.1164300\ttotal: 2m 1s\tremaining: 1m 57s\n",
      "509:\tlearn: 0.1163797\ttotal: 2m 1s\tremaining: 1m 56s\n",
      "510:\tlearn: 0.1163397\ttotal: 2m 1s\tremaining: 1m 56s\n",
      "511:\tlearn: 0.1162656\ttotal: 2m 1s\tremaining: 1m 56s\n",
      "512:\tlearn: 0.1161767\ttotal: 2m 2s\tremaining: 1m 55s\n",
      "513:\tlearn: 0.1161297\ttotal: 2m 2s\tremaining: 1m 55s\n",
      "514:\tlearn: 0.1160770\ttotal: 2m 2s\tremaining: 1m 55s\n",
      "515:\tlearn: 0.1160234\ttotal: 2m 2s\tremaining: 1m 54s\n",
      "516:\tlearn: 0.1159147\ttotal: 2m 2s\tremaining: 1m 54s\n",
      "517:\tlearn: 0.1158321\ttotal: 2m 2s\tremaining: 1m 54s\n",
      "518:\tlearn: 0.1157984\ttotal: 2m 2s\tremaining: 1m 53s\n",
      "519:\tlearn: 0.1156717\ttotal: 2m 3s\tremaining: 1m 53s\n",
      "520:\tlearn: 0.1156024\ttotal: 2m 3s\tremaining: 1m 53s\n",
      "521:\tlearn: 0.1155168\ttotal: 2m 3s\tremaining: 1m 52s\n",
      "522:\tlearn: 0.1154733\ttotal: 2m 3s\tremaining: 1m 52s\n",
      "523:\tlearn: 0.1154341\ttotal: 2m 3s\tremaining: 1m 52s\n",
      "524:\tlearn: 0.1153626\ttotal: 2m 3s\tremaining: 1m 51s\n",
      "525:\tlearn: 0.1153245\ttotal: 2m 4s\tremaining: 1m 51s\n",
      "526:\tlearn: 0.1153022\ttotal: 2m 4s\tremaining: 1m 51s\n",
      "527:\tlearn: 0.1152312\ttotal: 2m 4s\tremaining: 1m 51s\n",
      "528:\tlearn: 0.1151603\ttotal: 2m 4s\tremaining: 1m 51s\n",
      "529:\tlearn: 0.1150891\ttotal: 2m 4s\tremaining: 1m 50s\n",
      "530:\tlearn: 0.1149739\ttotal: 2m 5s\tremaining: 1m 50s\n",
      "531:\tlearn: 0.1149032\ttotal: 2m 5s\tremaining: 1m 50s\n",
      "532:\tlearn: 0.1148560\ttotal: 2m 5s\tremaining: 1m 49s\n",
      "533:\tlearn: 0.1147867\ttotal: 2m 5s\tremaining: 1m 49s\n",
      "534:\tlearn: 0.1147447\ttotal: 2m 5s\tremaining: 1m 49s\n",
      "535:\tlearn: 0.1146957\ttotal: 2m 5s\tremaining: 1m 49s\n",
      "536:\tlearn: 0.1146535\ttotal: 2m 6s\tremaining: 1m 48s\n",
      "537:\tlearn: 0.1146074\ttotal: 2m 6s\tremaining: 1m 48s\n",
      "538:\tlearn: 0.1145799\ttotal: 2m 6s\tremaining: 1m 48s\n",
      "539:\tlearn: 0.1145144\ttotal: 2m 6s\tremaining: 1m 47s\n",
      "540:\tlearn: 0.1144669\ttotal: 2m 6s\tremaining: 1m 47s\n",
      "541:\tlearn: 0.1144211\ttotal: 2m 7s\tremaining: 1m 47s\n",
      "542:\tlearn: 0.1143333\ttotal: 2m 7s\tremaining: 1m 47s\n",
      "543:\tlearn: 0.1142828\ttotal: 2m 7s\tremaining: 1m 46s\n",
      "544:\tlearn: 0.1142496\ttotal: 2m 7s\tremaining: 1m 46s\n",
      "545:\tlearn: 0.1142110\ttotal: 2m 7s\tremaining: 1m 46s\n",
      "546:\tlearn: 0.1141785\ttotal: 2m 8s\tremaining: 1m 46s\n",
      "547:\tlearn: 0.1141232\ttotal: 2m 8s\tremaining: 1m 45s\n",
      "548:\tlearn: 0.1140657\ttotal: 2m 8s\tremaining: 1m 45s\n",
      "549:\tlearn: 0.1139938\ttotal: 2m 8s\tremaining: 1m 45s\n",
      "550:\tlearn: 0.1139652\ttotal: 2m 8s\tremaining: 1m 44s\n",
      "551:\tlearn: 0.1138847\ttotal: 2m 8s\tremaining: 1m 44s\n",
      "552:\tlearn: 0.1138599\ttotal: 2m 9s\tremaining: 1m 44s\n",
      "553:\tlearn: 0.1137492\ttotal: 2m 9s\tremaining: 1m 44s\n",
      "554:\tlearn: 0.1136973\ttotal: 2m 9s\tremaining: 1m 43s\n",
      "555:\tlearn: 0.1136380\ttotal: 2m 9s\tremaining: 1m 43s\n",
      "556:\tlearn: 0.1136254\ttotal: 2m 9s\tremaining: 1m 43s\n",
      "557:\tlearn: 0.1135680\ttotal: 2m 9s\tremaining: 1m 42s\n",
      "558:\tlearn: 0.1134989\ttotal: 2m 9s\tremaining: 1m 42s\n",
      "559:\tlearn: 0.1134729\ttotal: 2m 10s\tremaining: 1m 42s\n",
      "560:\tlearn: 0.1134115\ttotal: 2m 10s\tremaining: 1m 41s\n",
      "561:\tlearn: 0.1133781\ttotal: 2m 10s\tremaining: 1m 41s\n",
      "562:\tlearn: 0.1133341\ttotal: 2m 10s\tremaining: 1m 41s\n",
      "563:\tlearn: 0.1133006\ttotal: 2m 10s\tremaining: 1m 41s\n",
      "564:\tlearn: 0.1132564\ttotal: 2m 11s\tremaining: 1m 41s\n",
      "565:\tlearn: 0.1132207\ttotal: 2m 11s\tremaining: 1m 40s\n",
      "566:\tlearn: 0.1131497\ttotal: 2m 11s\tremaining: 1m 40s\n",
      "567:\tlearn: 0.1131259\ttotal: 2m 11s\tremaining: 1m 40s\n",
      "568:\tlearn: 0.1130784\ttotal: 2m 11s\tremaining: 1m 39s\n",
      "569:\tlearn: 0.1130185\ttotal: 2m 11s\tremaining: 1m 39s\n",
      "570:\tlearn: 0.1129632\ttotal: 2m 12s\tremaining: 1m 39s\n",
      "571:\tlearn: 0.1128950\ttotal: 2m 12s\tremaining: 1m 39s\n",
      "572:\tlearn: 0.1128471\ttotal: 2m 12s\tremaining: 1m 38s\n",
      "573:\tlearn: 0.1127959\ttotal: 2m 12s\tremaining: 1m 38s\n",
      "574:\tlearn: 0.1127304\ttotal: 2m 13s\tremaining: 1m 38s\n",
      "575:\tlearn: 0.1126972\ttotal: 2m 13s\tremaining: 1m 38s\n",
      "576:\tlearn: 0.1126583\ttotal: 2m 13s\tremaining: 1m 37s\n",
      "577:\tlearn: 0.1125828\ttotal: 2m 13s\tremaining: 1m 37s\n",
      "578:\tlearn: 0.1124779\ttotal: 2m 13s\tremaining: 1m 37s\n",
      "579:\tlearn: 0.1124135\ttotal: 2m 13s\tremaining: 1m 36s\n",
      "580:\tlearn: 0.1123521\ttotal: 2m 14s\tremaining: 1m 36s\n",
      "581:\tlearn: 0.1122771\ttotal: 2m 14s\tremaining: 1m 36s\n",
      "582:\tlearn: 0.1121751\ttotal: 2m 14s\tremaining: 1m 36s\n",
      "583:\tlearn: 0.1121297\ttotal: 2m 14s\tremaining: 1m 35s\n",
      "584:\tlearn: 0.1120804\ttotal: 2m 14s\tremaining: 1m 35s\n",
      "585:\tlearn: 0.1120224\ttotal: 2m 14s\tremaining: 1m 35s\n",
      "586:\tlearn: 0.1119844\ttotal: 2m 15s\tremaining: 1m 35s\n",
      "587:\tlearn: 0.1119072\ttotal: 2m 15s\tremaining: 1m 34s\n",
      "588:\tlearn: 0.1118671\ttotal: 2m 15s\tremaining: 1m 34s\n",
      "589:\tlearn: 0.1118065\ttotal: 2m 15s\tremaining: 1m 34s\n",
      "590:\tlearn: 0.1117594\ttotal: 2m 15s\tremaining: 1m 33s\n",
      "591:\tlearn: 0.1117324\ttotal: 2m 15s\tremaining: 1m 33s\n",
      "592:\tlearn: 0.1116916\ttotal: 2m 15s\tremaining: 1m 33s\n",
      "593:\tlearn: 0.1116384\ttotal: 2m 16s\tremaining: 1m 33s\n",
      "594:\tlearn: 0.1115959\ttotal: 2m 16s\tremaining: 1m 32s\n",
      "595:\tlearn: 0.1115588\ttotal: 2m 16s\tremaining: 1m 32s\n",
      "596:\tlearn: 0.1115155\ttotal: 2m 16s\tremaining: 1m 32s\n",
      "597:\tlearn: 0.1114631\ttotal: 2m 16s\tremaining: 1m 32s\n",
      "598:\tlearn: 0.1114042\ttotal: 2m 17s\tremaining: 1m 31s\n",
      "599:\tlearn: 0.1113678\ttotal: 2m 17s\tremaining: 1m 31s\n",
      "600:\tlearn: 0.1113376\ttotal: 2m 17s\tremaining: 1m 31s\n",
      "601:\tlearn: 0.1112917\ttotal: 2m 17s\tremaining: 1m 30s\n",
      "602:\tlearn: 0.1112345\ttotal: 2m 17s\tremaining: 1m 30s\n",
      "603:\tlearn: 0.1111855\ttotal: 2m 17s\tremaining: 1m 30s\n",
      "604:\tlearn: 0.1111163\ttotal: 2m 17s\tremaining: 1m 30s\n",
      "605:\tlearn: 0.1110613\ttotal: 2m 18s\tremaining: 1m 29s\n",
      "606:\tlearn: 0.1110014\ttotal: 2m 18s\tremaining: 1m 29s\n",
      "607:\tlearn: 0.1109249\ttotal: 2m 18s\tremaining: 1m 29s\n",
      "608:\tlearn: 0.1108883\ttotal: 2m 18s\tremaining: 1m 28s\n",
      "609:\tlearn: 0.1108495\ttotal: 2m 18s\tremaining: 1m 28s\n",
      "610:\tlearn: 0.1108016\ttotal: 2m 18s\tremaining: 1m 28s\n",
      "611:\tlearn: 0.1107545\ttotal: 2m 19s\tremaining: 1m 28s\n",
      "612:\tlearn: 0.1107373\ttotal: 2m 19s\tremaining: 1m 27s\n",
      "613:\tlearn: 0.1106821\ttotal: 2m 19s\tremaining: 1m 27s\n",
      "614:\tlearn: 0.1106559\ttotal: 2m 20s\tremaining: 1m 27s\n",
      "615:\tlearn: 0.1106051\ttotal: 2m 20s\tremaining: 1m 27s\n",
      "616:\tlearn: 0.1105460\ttotal: 2m 20s\tremaining: 1m 27s\n",
      "617:\tlearn: 0.1105153\ttotal: 2m 20s\tremaining: 1m 26s\n",
      "618:\tlearn: 0.1104712\ttotal: 2m 20s\tremaining: 1m 26s\n",
      "619:\tlearn: 0.1104472\ttotal: 2m 20s\tremaining: 1m 26s\n",
      "620:\tlearn: 0.1104081\ttotal: 2m 21s\tremaining: 1m 26s\n",
      "621:\tlearn: 0.1103919\ttotal: 2m 21s\tremaining: 1m 25s\n",
      "622:\tlearn: 0.1103288\ttotal: 2m 21s\tremaining: 1m 25s\n",
      "623:\tlearn: 0.1103010\ttotal: 2m 21s\tremaining: 1m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624:\tlearn: 0.1102527\ttotal: 2m 21s\tremaining: 1m 25s\n",
      "625:\tlearn: 0.1102218\ttotal: 2m 21s\tremaining: 1m 24s\n",
      "626:\tlearn: 0.1101589\ttotal: 2m 22s\tremaining: 1m 24s\n",
      "627:\tlearn: 0.1101333\ttotal: 2m 22s\tremaining: 1m 24s\n",
      "628:\tlearn: 0.1100830\ttotal: 2m 22s\tremaining: 1m 24s\n",
      "629:\tlearn: 0.1100684\ttotal: 2m 22s\tremaining: 1m 23s\n",
      "630:\tlearn: 0.1100267\ttotal: 2m 22s\tremaining: 1m 23s\n",
      "631:\tlearn: 0.1099767\ttotal: 2m 22s\tremaining: 1m 23s\n",
      "632:\tlearn: 0.1099126\ttotal: 2m 23s\tremaining: 1m 23s\n",
      "633:\tlearn: 0.1098443\ttotal: 2m 23s\tremaining: 1m 22s\n",
      "634:\tlearn: 0.1097959\ttotal: 2m 23s\tremaining: 1m 22s\n",
      "635:\tlearn: 0.1097383\ttotal: 2m 23s\tremaining: 1m 22s\n",
      "636:\tlearn: 0.1096925\ttotal: 2m 23s\tremaining: 1m 21s\n",
      "637:\tlearn: 0.1096546\ttotal: 2m 23s\tremaining: 1m 21s\n",
      "638:\tlearn: 0.1095893\ttotal: 2m 24s\tremaining: 1m 21s\n",
      "639:\tlearn: 0.1095394\ttotal: 2m 24s\tremaining: 1m 21s\n",
      "640:\tlearn: 0.1095010\ttotal: 2m 24s\tremaining: 1m 21s\n",
      "641:\tlearn: 0.1094757\ttotal: 2m 24s\tremaining: 1m 20s\n",
      "642:\tlearn: 0.1094393\ttotal: 2m 24s\tremaining: 1m 20s\n",
      "643:\tlearn: 0.1094035\ttotal: 2m 25s\tremaining: 1m 20s\n",
      "644:\tlearn: 0.1093410\ttotal: 2m 25s\tremaining: 1m 19s\n",
      "645:\tlearn: 0.1093149\ttotal: 2m 25s\tremaining: 1m 19s\n",
      "646:\tlearn: 0.1092765\ttotal: 2m 25s\tremaining: 1m 19s\n",
      "647:\tlearn: 0.1092227\ttotal: 2m 25s\tremaining: 1m 19s\n",
      "648:\tlearn: 0.1091804\ttotal: 2m 25s\tremaining: 1m 18s\n",
      "649:\tlearn: 0.1090942\ttotal: 2m 25s\tremaining: 1m 18s\n",
      "650:\tlearn: 0.1090255\ttotal: 2m 26s\tremaining: 1m 18s\n",
      "651:\tlearn: 0.1089816\ttotal: 2m 26s\tremaining: 1m 18s\n",
      "652:\tlearn: 0.1089061\ttotal: 2m 26s\tremaining: 1m 17s\n",
      "653:\tlearn: 0.1088597\ttotal: 2m 26s\tremaining: 1m 17s\n",
      "654:\tlearn: 0.1087984\ttotal: 2m 26s\tremaining: 1m 17s\n",
      "655:\tlearn: 0.1087202\ttotal: 2m 26s\tremaining: 1m 16s\n",
      "656:\tlearn: 0.1086746\ttotal: 2m 26s\tremaining: 1m 16s\n",
      "657:\tlearn: 0.1086295\ttotal: 2m 27s\tremaining: 1m 16s\n",
      "658:\tlearn: 0.1085914\ttotal: 2m 27s\tremaining: 1m 16s\n",
      "659:\tlearn: 0.1085567\ttotal: 2m 27s\tremaining: 1m 15s\n",
      "660:\tlearn: 0.1085032\ttotal: 2m 27s\tremaining: 1m 15s\n",
      "661:\tlearn: 0.1084345\ttotal: 2m 27s\tremaining: 1m 15s\n",
      "662:\tlearn: 0.1083941\ttotal: 2m 27s\tremaining: 1m 15s\n",
      "663:\tlearn: 0.1083603\ttotal: 2m 27s\tremaining: 1m 14s\n",
      "664:\tlearn: 0.1083163\ttotal: 2m 28s\tremaining: 1m 14s\n",
      "665:\tlearn: 0.1082935\ttotal: 2m 28s\tremaining: 1m 14s\n",
      "666:\tlearn: 0.1082809\ttotal: 2m 28s\tremaining: 1m 14s\n",
      "667:\tlearn: 0.1082177\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "668:\tlearn: 0.1081886\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "669:\tlearn: 0.1081567\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "670:\tlearn: 0.1081085\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "671:\tlearn: 0.1080474\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "672:\tlearn: 0.1079826\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "673:\tlearn: 0.1079328\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "674:\tlearn: 0.1078875\ttotal: 2m 30s\tremaining: 1m 12s\n",
      "675:\tlearn: 0.1078319\ttotal: 2m 30s\tremaining: 1m 12s\n",
      "676:\tlearn: 0.1077938\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "677:\tlearn: 0.1077614\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "678:\tlearn: 0.1077229\ttotal: 2m 31s\tremaining: 1m 11s\n",
      "679:\tlearn: 0.1076653\ttotal: 2m 31s\tremaining: 1m 11s\n",
      "680:\tlearn: 0.1076527\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "681:\tlearn: 0.1076127\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "682:\tlearn: 0.1075497\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "683:\tlearn: 0.1074816\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "684:\tlearn: 0.1074388\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "685:\tlearn: 0.1074030\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "686:\tlearn: 0.1073527\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "687:\tlearn: 0.1073076\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "688:\tlearn: 0.1072571\ttotal: 2m 32s\tremaining: 1m 8s\n",
      "689:\tlearn: 0.1072312\ttotal: 2m 32s\tremaining: 1m 8s\n",
      "690:\tlearn: 0.1071625\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "691:\tlearn: 0.1070443\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "692:\tlearn: 0.1070053\ttotal: 2m 33s\tremaining: 1m 7s\n",
      "693:\tlearn: 0.1069709\ttotal: 2m 33s\tremaining: 1m 7s\n",
      "694:\tlearn: 0.1069371\ttotal: 2m 33s\tremaining: 1m 7s\n",
      "695:\tlearn: 0.1068856\ttotal: 2m 33s\tremaining: 1m 7s\n",
      "696:\tlearn: 0.1068363\ttotal: 2m 34s\tremaining: 1m 7s\n",
      "697:\tlearn: 0.1067935\ttotal: 2m 34s\tremaining: 1m 6s\n",
      "698:\tlearn: 0.1067815\ttotal: 2m 34s\tremaining: 1m 6s\n",
      "699:\tlearn: 0.1067356\ttotal: 2m 34s\tremaining: 1m 6s\n",
      "700:\tlearn: 0.1067045\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "701:\tlearn: 0.1066609\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "702:\tlearn: 0.1065679\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "703:\tlearn: 0.1065333\ttotal: 2m 35s\tremaining: 1m 5s\n",
      "704:\tlearn: 0.1065180\ttotal: 2m 35s\tremaining: 1m 5s\n",
      "705:\tlearn: 0.1064687\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "706:\tlearn: 0.1064312\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "707:\tlearn: 0.1063836\ttotal: 2m 36s\tremaining: 1m 4s\n",
      "708:\tlearn: 0.1063536\ttotal: 2m 36s\tremaining: 1m 4s\n",
      "709:\tlearn: 0.1063356\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "710:\tlearn: 0.1063065\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "711:\tlearn: 0.1062581\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "712:\tlearn: 0.1062120\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "713:\tlearn: 0.1061486\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "714:\tlearn: 0.1061178\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "715:\tlearn: 0.1060892\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "716:\tlearn: 0.1059912\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "717:\tlearn: 0.1059391\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "718:\tlearn: 0.1058622\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "719:\tlearn: 0.1058022\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "720:\tlearn: 0.1057772\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "721:\tlearn: 0.1057036\ttotal: 2m 38s\tremaining: 1m\n",
      "722:\tlearn: 0.1056704\ttotal: 2m 38s\tremaining: 1m\n",
      "723:\tlearn: 0.1056421\ttotal: 2m 38s\tremaining: 1m\n",
      "724:\tlearn: 0.1056205\ttotal: 2m 38s\tremaining: 1m\n",
      "725:\tlearn: 0.1055698\ttotal: 2m 38s\tremaining: 60s\n",
      "726:\tlearn: 0.1055269\ttotal: 2m 39s\tremaining: 59.7s\n",
      "727:\tlearn: 0.1054914\ttotal: 2m 39s\tremaining: 59.5s\n",
      "728:\tlearn: 0.1054357\ttotal: 2m 39s\tremaining: 59.2s\n",
      "729:\tlearn: 0.1054212\ttotal: 2m 39s\tremaining: 59s\n",
      "730:\tlearn: 0.1053669\ttotal: 2m 39s\tremaining: 58.7s\n",
      "731:\tlearn: 0.1053000\ttotal: 2m 39s\tremaining: 58.5s\n",
      "732:\tlearn: 0.1052479\ttotal: 2m 40s\tremaining: 58.4s\n",
      "733:\tlearn: 0.1052220\ttotal: 2m 40s\tremaining: 58.2s\n",
      "734:\tlearn: 0.1051957\ttotal: 2m 40s\tremaining: 57.9s\n",
      "735:\tlearn: 0.1051779\ttotal: 2m 40s\tremaining: 57.6s\n",
      "736:\tlearn: 0.1051426\ttotal: 2m 40s\tremaining: 57.4s\n",
      "737:\tlearn: 0.1051115\ttotal: 2m 41s\tremaining: 57.2s\n",
      "738:\tlearn: 0.1050976\ttotal: 2m 41s\tremaining: 56.9s\n",
      "739:\tlearn: 0.1050736\ttotal: 2m 41s\tremaining: 56.7s\n",
      "740:\tlearn: 0.1050054\ttotal: 2m 41s\tremaining: 56.4s\n",
      "741:\tlearn: 0.1049373\ttotal: 2m 41s\tremaining: 56.2s\n",
      "742:\tlearn: 0.1048936\ttotal: 2m 41s\tremaining: 55.9s\n",
      "743:\tlearn: 0.1048081\ttotal: 2m 41s\tremaining: 55.7s\n",
      "744:\tlearn: 0.1047368\ttotal: 2m 42s\tremaining: 55.5s\n",
      "745:\tlearn: 0.1046982\ttotal: 2m 42s\tremaining: 55.2s\n",
      "746:\tlearn: 0.1046701\ttotal: 2m 42s\tremaining: 55s\n",
      "747:\tlearn: 0.1046155\ttotal: 2m 42s\tremaining: 54.7s\n",
      "748:\tlearn: 0.1045665\ttotal: 2m 42s\tremaining: 54.5s\n",
      "749:\tlearn: 0.1045320\ttotal: 2m 42s\tremaining: 54.2s\n",
      "750:\tlearn: 0.1044980\ttotal: 2m 42s\tremaining: 54s\n",
      "751:\tlearn: 0.1044426\ttotal: 2m 43s\tremaining: 53.8s\n",
      "752:\tlearn: 0.1044192\ttotal: 2m 43s\tremaining: 53.5s\n",
      "753:\tlearn: 0.1043940\ttotal: 2m 43s\tremaining: 53.3s\n",
      "754:\tlearn: 0.1043784\ttotal: 2m 43s\tremaining: 53s\n",
      "755:\tlearn: 0.1043519\ttotal: 2m 43s\tremaining: 52.8s\n",
      "756:\tlearn: 0.1043103\ttotal: 2m 43s\tremaining: 52.5s\n",
      "757:\tlearn: 0.1042826\ttotal: 2m 43s\tremaining: 52.3s\n",
      "758:\tlearn: 0.1042307\ttotal: 2m 44s\tremaining: 52.1s\n",
      "759:\tlearn: 0.1041797\ttotal: 2m 44s\tremaining: 51.9s\n",
      "760:\tlearn: 0.1041267\ttotal: 2m 44s\tremaining: 51.6s\n",
      "761:\tlearn: 0.1041168\ttotal: 2m 44s\tremaining: 51.4s\n",
      "762:\tlearn: 0.1041110\ttotal: 2m 44s\tremaining: 51.1s\n",
      "763:\tlearn: 0.1041070\ttotal: 2m 44s\tremaining: 50.9s\n",
      "764:\tlearn: 0.1040774\ttotal: 2m 44s\tremaining: 50.6s\n",
      "765:\tlearn: 0.1040313\ttotal: 2m 45s\tremaining: 50.4s\n",
      "766:\tlearn: 0.1039888\ttotal: 2m 45s\tremaining: 50.2s\n",
      "767:\tlearn: 0.1039469\ttotal: 2m 45s\tremaining: 49.9s\n",
      "768:\tlearn: 0.1038833\ttotal: 2m 45s\tremaining: 49.7s\n",
      "769:\tlearn: 0.1038089\ttotal: 2m 45s\tremaining: 49.5s\n",
      "770:\tlearn: 0.1037664\ttotal: 2m 45s\tremaining: 49.2s\n",
      "771:\tlearn: 0.1037078\ttotal: 2m 45s\tremaining: 49s\n",
      "772:\tlearn: 0.1036623\ttotal: 2m 46s\tremaining: 48.8s\n",
      "773:\tlearn: 0.1036038\ttotal: 2m 46s\tremaining: 48.5s\n",
      "774:\tlearn: 0.1035602\ttotal: 2m 46s\tremaining: 48.3s\n",
      "775:\tlearn: 0.1035211\ttotal: 2m 46s\tremaining: 48s\n",
      "776:\tlearn: 0.1035079\ttotal: 2m 46s\tremaining: 47.8s\n",
      "777:\tlearn: 0.1034311\ttotal: 2m 46s\tremaining: 47.5s\n",
      "778:\tlearn: 0.1034094\ttotal: 2m 46s\tremaining: 47.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779:\tlearn: 0.1033875\ttotal: 2m 47s\tremaining: 47.1s\n",
      "780:\tlearn: 0.1033697\ttotal: 2m 47s\tremaining: 46.9s\n",
      "781:\tlearn: 0.1033510\ttotal: 2m 47s\tremaining: 46.7s\n",
      "782:\tlearn: 0.1033069\ttotal: 2m 47s\tremaining: 46.4s\n",
      "783:\tlearn: 0.1032576\ttotal: 2m 47s\tremaining: 46.2s\n",
      "784:\tlearn: 0.1032351\ttotal: 2m 47s\tremaining: 45.9s\n",
      "785:\tlearn: 0.1032120\ttotal: 2m 47s\tremaining: 45.7s\n",
      "786:\tlearn: 0.1031708\ttotal: 2m 48s\tremaining: 45.5s\n",
      "787:\tlearn: 0.1031340\ttotal: 2m 48s\tremaining: 45.3s\n",
      "788:\tlearn: 0.1030806\ttotal: 2m 48s\tremaining: 45s\n",
      "789:\tlearn: 0.1030572\ttotal: 2m 48s\tremaining: 44.8s\n",
      "790:\tlearn: 0.1030264\ttotal: 2m 48s\tremaining: 44.6s\n",
      "791:\tlearn: 0.1030063\ttotal: 2m 48s\tremaining: 44.3s\n",
      "792:\tlearn: 0.1029811\ttotal: 2m 48s\tremaining: 44.1s\n",
      "793:\tlearn: 0.1029742\ttotal: 2m 49s\tremaining: 43.9s\n",
      "794:\tlearn: 0.1029452\ttotal: 2m 49s\tremaining: 43.6s\n",
      "795:\tlearn: 0.1029172\ttotal: 2m 49s\tremaining: 43.4s\n",
      "796:\tlearn: 0.1028968\ttotal: 2m 49s\tremaining: 43.2s\n",
      "797:\tlearn: 0.1028909\ttotal: 2m 49s\tremaining: 42.9s\n",
      "798:\tlearn: 0.1028382\ttotal: 2m 49s\tremaining: 42.7s\n",
      "799:\tlearn: 0.1027810\ttotal: 2m 49s\tremaining: 42.4s\n",
      "800:\tlearn: 0.1027334\ttotal: 2m 49s\tremaining: 42.2s\n",
      "801:\tlearn: 0.1027216\ttotal: 2m 50s\tremaining: 42s\n",
      "802:\tlearn: 0.1026846\ttotal: 2m 50s\tremaining: 41.8s\n",
      "803:\tlearn: 0.1026596\ttotal: 2m 50s\tremaining: 41.5s\n",
      "804:\tlearn: 0.1025881\ttotal: 2m 50s\tremaining: 41.3s\n",
      "805:\tlearn: 0.1025540\ttotal: 2m 50s\tremaining: 41.1s\n",
      "806:\tlearn: 0.1025199\ttotal: 2m 50s\tremaining: 40.8s\n",
      "807:\tlearn: 0.1024823\ttotal: 2m 51s\tremaining: 40.6s\n",
      "808:\tlearn: 0.1024546\ttotal: 2m 51s\tremaining: 40.4s\n",
      "809:\tlearn: 0.1024201\ttotal: 2m 51s\tremaining: 40.2s\n",
      "810:\tlearn: 0.1023969\ttotal: 2m 51s\tremaining: 39.9s\n",
      "811:\tlearn: 0.1023901\ttotal: 2m 51s\tremaining: 39.7s\n",
      "812:\tlearn: 0.1023476\ttotal: 2m 51s\tremaining: 39.5s\n",
      "813:\tlearn: 0.1023338\ttotal: 2m 51s\tremaining: 39.2s\n",
      "814:\tlearn: 0.1023188\ttotal: 2m 51s\tremaining: 39s\n",
      "815:\tlearn: 0.1022905\ttotal: 2m 52s\tremaining: 38.8s\n",
      "816:\tlearn: 0.1022708\ttotal: 2m 52s\tremaining: 38.6s\n",
      "817:\tlearn: 0.1021917\ttotal: 2m 52s\tremaining: 38.4s\n",
      "818:\tlearn: 0.1021583\ttotal: 2m 52s\tremaining: 38.1s\n",
      "819:\tlearn: 0.1021347\ttotal: 2m 52s\tremaining: 37.9s\n",
      "820:\tlearn: 0.1021240\ttotal: 2m 52s\tremaining: 37.7s\n",
      "821:\tlearn: 0.1020788\ttotal: 2m 53s\tremaining: 37.5s\n",
      "822:\tlearn: 0.1020460\ttotal: 2m 53s\tremaining: 37.2s\n",
      "823:\tlearn: 0.1019953\ttotal: 2m 53s\tremaining: 37s\n",
      "824:\tlearn: 0.1019475\ttotal: 2m 53s\tremaining: 36.8s\n",
      "825:\tlearn: 0.1019188\ttotal: 2m 53s\tremaining: 36.6s\n",
      "826:\tlearn: 0.1019045\ttotal: 2m 53s\tremaining: 36.3s\n",
      "827:\tlearn: 0.1018666\ttotal: 2m 53s\tremaining: 36.1s\n",
      "828:\tlearn: 0.1018578\ttotal: 2m 53s\tremaining: 35.9s\n",
      "829:\tlearn: 0.1018300\ttotal: 2m 54s\tremaining: 35.7s\n",
      "830:\tlearn: 0.1017553\ttotal: 2m 54s\tremaining: 35.4s\n",
      "831:\tlearn: 0.1016965\ttotal: 2m 54s\tremaining: 35.2s\n",
      "832:\tlearn: 0.1016261\ttotal: 2m 54s\tremaining: 35s\n",
      "833:\tlearn: 0.1015692\ttotal: 2m 54s\tremaining: 34.8s\n",
      "834:\tlearn: 0.1015519\ttotal: 2m 54s\tremaining: 34.5s\n",
      "835:\tlearn: 0.1014828\ttotal: 2m 55s\tremaining: 34.3s\n",
      "836:\tlearn: 0.1014540\ttotal: 2m 55s\tremaining: 34.1s\n",
      "837:\tlearn: 0.1014516\ttotal: 2m 55s\tremaining: 33.9s\n",
      "838:\tlearn: 0.1014177\ttotal: 2m 55s\tremaining: 33.7s\n",
      "839:\tlearn: 0.1013802\ttotal: 2m 55s\tremaining: 33.4s\n",
      "840:\tlearn: 0.1013460\ttotal: 2m 55s\tremaining: 33.2s\n",
      "841:\tlearn: 0.1012999\ttotal: 2m 55s\tremaining: 33s\n",
      "842:\tlearn: 0.1012488\ttotal: 2m 56s\tremaining: 32.8s\n",
      "843:\tlearn: 0.1012290\ttotal: 2m 56s\tremaining: 32.6s\n",
      "844:\tlearn: 0.1011992\ttotal: 2m 56s\tremaining: 32.4s\n",
      "845:\tlearn: 0.1011749\ttotal: 2m 56s\tremaining: 32.1s\n",
      "846:\tlearn: 0.1011426\ttotal: 2m 56s\tremaining: 31.9s\n",
      "847:\tlearn: 0.1011244\ttotal: 2m 56s\tremaining: 31.7s\n",
      "848:\tlearn: 0.1010729\ttotal: 2m 56s\tremaining: 31.5s\n",
      "849:\tlearn: 0.1010517\ttotal: 2m 57s\tremaining: 31.3s\n",
      "850:\tlearn: 0.1010097\ttotal: 2m 57s\tremaining: 31s\n",
      "851:\tlearn: 0.1009962\ttotal: 2m 57s\tremaining: 30.8s\n",
      "852:\tlearn: 0.1009573\ttotal: 2m 57s\tremaining: 30.6s\n",
      "853:\tlearn: 0.1009203\ttotal: 2m 57s\tremaining: 30.4s\n",
      "854:\tlearn: 0.1008838\ttotal: 2m 57s\tremaining: 30.1s\n",
      "855:\tlearn: 0.1008405\ttotal: 2m 57s\tremaining: 29.9s\n",
      "856:\tlearn: 0.1008000\ttotal: 2m 58s\tremaining: 29.7s\n",
      "857:\tlearn: 0.1007914\ttotal: 2m 58s\tremaining: 29.5s\n",
      "858:\tlearn: 0.1007566\ttotal: 2m 58s\tremaining: 29.3s\n",
      "859:\tlearn: 0.1007161\ttotal: 2m 58s\tremaining: 29s\n",
      "860:\tlearn: 0.1007006\ttotal: 2m 58s\tremaining: 28.8s\n",
      "861:\tlearn: 0.1006656\ttotal: 2m 58s\tremaining: 28.6s\n",
      "862:\tlearn: 0.1006356\ttotal: 2m 58s\tremaining: 28.4s\n",
      "863:\tlearn: 0.1006001\ttotal: 2m 58s\tremaining: 28.2s\n",
      "864:\tlearn: 0.1005632\ttotal: 2m 59s\tremaining: 27.9s\n",
      "865:\tlearn: 0.1005117\ttotal: 2m 59s\tremaining: 27.7s\n",
      "866:\tlearn: 0.1004668\ttotal: 2m 59s\tremaining: 27.5s\n",
      "867:\tlearn: 0.1004413\ttotal: 2m 59s\tremaining: 27.3s\n",
      "868:\tlearn: 0.1004021\ttotal: 2m 59s\tremaining: 27.1s\n",
      "869:\tlearn: 0.1003689\ttotal: 2m 59s\tremaining: 26.8s\n",
      "870:\tlearn: 0.1003211\ttotal: 2m 59s\tremaining: 26.6s\n",
      "871:\tlearn: 0.1002897\ttotal: 2m 59s\tremaining: 26.4s\n",
      "872:\tlearn: 0.1002544\ttotal: 3m\tremaining: 26.2s\n",
      "873:\tlearn: 0.1002090\ttotal: 3m\tremaining: 26s\n",
      "874:\tlearn: 0.1001633\ttotal: 3m\tremaining: 25.8s\n",
      "875:\tlearn: 0.1001356\ttotal: 3m\tremaining: 25.6s\n",
      "876:\tlearn: 0.1001133\ttotal: 3m\tremaining: 25.3s\n",
      "877:\tlearn: 0.1000799\ttotal: 3m\tremaining: 25.1s\n",
      "878:\tlearn: 0.1000750\ttotal: 3m 1s\tremaining: 24.9s\n",
      "879:\tlearn: 0.1000157\ttotal: 3m 1s\tremaining: 24.7s\n",
      "880:\tlearn: 0.0999727\ttotal: 3m 1s\tremaining: 24.5s\n",
      "881:\tlearn: 0.0999620\ttotal: 3m 1s\tremaining: 24.3s\n",
      "882:\tlearn: 0.0999431\ttotal: 3m 1s\tremaining: 24.1s\n",
      "883:\tlearn: 0.0999111\ttotal: 3m 1s\tremaining: 23.8s\n",
      "884:\tlearn: 0.0998531\ttotal: 3m 1s\tremaining: 23.6s\n",
      "885:\tlearn: 0.0998163\ttotal: 3m 2s\tremaining: 23.4s\n",
      "886:\tlearn: 0.0997784\ttotal: 3m 2s\tremaining: 23.2s\n",
      "887:\tlearn: 0.0997315\ttotal: 3m 2s\tremaining: 23s\n",
      "888:\tlearn: 0.0996813\ttotal: 3m 2s\tremaining: 22.8s\n",
      "889:\tlearn: 0.0996661\ttotal: 3m 2s\tremaining: 22.6s\n",
      "890:\tlearn: 0.0996127\ttotal: 3m 2s\tremaining: 22.3s\n",
      "891:\tlearn: 0.0996026\ttotal: 3m 2s\tremaining: 22.1s\n",
      "892:\tlearn: 0.0995493\ttotal: 3m 2s\tremaining: 21.9s\n",
      "893:\tlearn: 0.0994962\ttotal: 3m 3s\tremaining: 21.7s\n",
      "894:\tlearn: 0.0994802\ttotal: 3m 3s\tremaining: 21.5s\n",
      "895:\tlearn: 0.0994426\ttotal: 3m 3s\tremaining: 21.3s\n",
      "896:\tlearn: 0.0994250\ttotal: 3m 3s\tremaining: 21.1s\n",
      "897:\tlearn: 0.0993929\ttotal: 3m 3s\tremaining: 20.8s\n",
      "898:\tlearn: 0.0993898\ttotal: 3m 3s\tremaining: 20.6s\n",
      "899:\tlearn: 0.0993779\ttotal: 3m 3s\tremaining: 20.4s\n",
      "900:\tlearn: 0.0993348\ttotal: 3m 4s\tremaining: 20.3s\n",
      "901:\tlearn: 0.0993171\ttotal: 3m 4s\tremaining: 20s\n",
      "902:\tlearn: 0.0993144\ttotal: 3m 4s\tremaining: 19.8s\n",
      "903:\tlearn: 0.0992874\ttotal: 3m 4s\tremaining: 19.6s\n",
      "904:\tlearn: 0.0992413\ttotal: 3m 5s\tremaining: 19.4s\n",
      "905:\tlearn: 0.0991716\ttotal: 3m 5s\tremaining: 19.2s\n",
      "906:\tlearn: 0.0991500\ttotal: 3m 5s\tremaining: 19s\n",
      "907:\tlearn: 0.0991220\ttotal: 3m 5s\tremaining: 18.8s\n",
      "908:\tlearn: 0.0990845\ttotal: 3m 5s\tremaining: 18.6s\n",
      "909:\tlearn: 0.0990459\ttotal: 3m 5s\tremaining: 18.4s\n",
      "910:\tlearn: 0.0990229\ttotal: 3m 5s\tremaining: 18.1s\n",
      "911:\tlearn: 0.0989889\ttotal: 3m 5s\tremaining: 17.9s\n",
      "912:\tlearn: 0.0989664\ttotal: 3m 6s\tremaining: 17.7s\n",
      "913:\tlearn: 0.0989376\ttotal: 3m 6s\tremaining: 17.5s\n",
      "914:\tlearn: 0.0989248\ttotal: 3m 6s\tremaining: 17.3s\n",
      "915:\tlearn: 0.0988623\ttotal: 3m 6s\tremaining: 17.1s\n",
      "916:\tlearn: 0.0988239\ttotal: 3m 6s\tremaining: 16.9s\n",
      "917:\tlearn: 0.0988078\ttotal: 3m 6s\tremaining: 16.7s\n",
      "918:\tlearn: 0.0987750\ttotal: 3m 6s\tremaining: 16.5s\n",
      "919:\tlearn: 0.0987294\ttotal: 3m 6s\tremaining: 16.3s\n",
      "920:\tlearn: 0.0986870\ttotal: 3m 7s\tremaining: 16.1s\n",
      "921:\tlearn: 0.0986582\ttotal: 3m 7s\tremaining: 15.8s\n",
      "922:\tlearn: 0.0986443\ttotal: 3m 7s\tremaining: 15.6s\n",
      "923:\tlearn: 0.0986221\ttotal: 3m 7s\tremaining: 15.4s\n",
      "924:\tlearn: 0.0986049\ttotal: 3m 7s\tremaining: 15.2s\n",
      "925:\tlearn: 0.0985779\ttotal: 3m 7s\tremaining: 15s\n",
      "926:\tlearn: 0.0985452\ttotal: 3m 7s\tremaining: 14.8s\n",
      "927:\tlearn: 0.0985155\ttotal: 3m 8s\tremaining: 14.6s\n",
      "928:\tlearn: 0.0984960\ttotal: 3m 8s\tremaining: 14.4s\n",
      "929:\tlearn: 0.0984472\ttotal: 3m 8s\tremaining: 14.2s\n",
      "930:\tlearn: 0.0984020\ttotal: 3m 8s\tremaining: 14s\n",
      "931:\tlearn: 0.0983660\ttotal: 3m 8s\tremaining: 13.8s\n",
      "932:\tlearn: 0.0983490\ttotal: 3m 8s\tremaining: 13.6s\n",
      "933:\tlearn: 0.0983156\ttotal: 3m 9s\tremaining: 13.4s\n",
      "934:\tlearn: 0.0982981\ttotal: 3m 9s\tremaining: 13.1s\n",
      "935:\tlearn: 0.0982777\ttotal: 3m 9s\tremaining: 12.9s\n",
      "936:\tlearn: 0.0982650\ttotal: 3m 9s\tremaining: 12.7s\n",
      "937:\tlearn: 0.0982334\ttotal: 3m 9s\tremaining: 12.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938:\tlearn: 0.0981958\ttotal: 3m 9s\tremaining: 12.3s\n",
      "939:\tlearn: 0.0981675\ttotal: 3m 9s\tremaining: 12.1s\n",
      "940:\tlearn: 0.0981340\ttotal: 3m 9s\tremaining: 11.9s\n",
      "941:\tlearn: 0.0981023\ttotal: 3m 10s\tremaining: 11.7s\n",
      "942:\tlearn: 0.0980809\ttotal: 3m 10s\tremaining: 11.5s\n",
      "943:\tlearn: 0.0980353\ttotal: 3m 10s\tremaining: 11.3s\n",
      "944:\tlearn: 0.0980154\ttotal: 3m 10s\tremaining: 11.1s\n",
      "945:\tlearn: 0.0979744\ttotal: 3m 10s\tremaining: 10.9s\n",
      "946:\tlearn: 0.0979542\ttotal: 3m 10s\tremaining: 10.7s\n",
      "947:\tlearn: 0.0979129\ttotal: 3m 10s\tremaining: 10.5s\n",
      "948:\tlearn: 0.0978612\ttotal: 3m 10s\tremaining: 10.3s\n",
      "949:\tlearn: 0.0978359\ttotal: 3m 11s\tremaining: 10.1s\n",
      "950:\tlearn: 0.0977834\ttotal: 3m 11s\tremaining: 9.85s\n",
      "951:\tlearn: 0.0977774\ttotal: 3m 11s\tremaining: 9.65s\n",
      "952:\tlearn: 0.0977687\ttotal: 3m 11s\tremaining: 9.44s\n",
      "953:\tlearn: 0.0977440\ttotal: 3m 11s\tremaining: 9.24s\n",
      "954:\tlearn: 0.0976980\ttotal: 3m 11s\tremaining: 9.04s\n",
      "955:\tlearn: 0.0976657\ttotal: 3m 12s\tremaining: 8.84s\n",
      "956:\tlearn: 0.0976171\ttotal: 3m 12s\tremaining: 8.64s\n",
      "957:\tlearn: 0.0975704\ttotal: 3m 12s\tremaining: 8.44s\n",
      "958:\tlearn: 0.0975372\ttotal: 3m 12s\tremaining: 8.23s\n",
      "959:\tlearn: 0.0974921\ttotal: 3m 12s\tremaining: 8.03s\n",
      "960:\tlearn: 0.0974827\ttotal: 3m 12s\tremaining: 7.83s\n",
      "961:\tlearn: 0.0974634\ttotal: 3m 13s\tremaining: 7.63s\n",
      "962:\tlearn: 0.0974297\ttotal: 3m 13s\tremaining: 7.42s\n",
      "963:\tlearn: 0.0974217\ttotal: 3m 13s\tremaining: 7.22s\n",
      "964:\tlearn: 0.0973709\ttotal: 3m 13s\tremaining: 7.02s\n",
      "965:\tlearn: 0.0973392\ttotal: 3m 13s\tremaining: 6.81s\n",
      "966:\tlearn: 0.0972728\ttotal: 3m 13s\tremaining: 6.61s\n",
      "967:\tlearn: 0.0972520\ttotal: 3m 13s\tremaining: 6.41s\n",
      "968:\tlearn: 0.0972058\ttotal: 3m 14s\tremaining: 6.21s\n",
      "969:\tlearn: 0.0971607\ttotal: 3m 14s\tremaining: 6s\n",
      "970:\tlearn: 0.0971326\ttotal: 3m 14s\tremaining: 5.8s\n",
      "971:\tlearn: 0.0971107\ttotal: 3m 14s\tremaining: 5.6s\n",
      "972:\tlearn: 0.0970753\ttotal: 3m 14s\tremaining: 5.4s\n",
      "973:\tlearn: 0.0970418\ttotal: 3m 14s\tremaining: 5.2s\n",
      "974:\tlearn: 0.0969967\ttotal: 3m 14s\tremaining: 4.99s\n",
      "975:\tlearn: 0.0969642\ttotal: 3m 14s\tremaining: 4.79s\n",
      "976:\tlearn: 0.0969147\ttotal: 3m 15s\tremaining: 4.59s\n",
      "977:\tlearn: 0.0968750\ttotal: 3m 15s\tremaining: 4.39s\n",
      "978:\tlearn: 0.0968682\ttotal: 3m 15s\tremaining: 4.19s\n",
      "979:\tlearn: 0.0968504\ttotal: 3m 15s\tremaining: 3.99s\n",
      "980:\tlearn: 0.0967994\ttotal: 3m 15s\tremaining: 3.79s\n",
      "981:\tlearn: 0.0967555\ttotal: 3m 15s\tremaining: 3.59s\n",
      "982:\tlearn: 0.0967019\ttotal: 3m 15s\tremaining: 3.39s\n",
      "983:\tlearn: 0.0966802\ttotal: 3m 16s\tremaining: 3.19s\n",
      "984:\tlearn: 0.0966369\ttotal: 3m 16s\tremaining: 2.99s\n",
      "985:\tlearn: 0.0966146\ttotal: 3m 16s\tremaining: 2.79s\n",
      "986:\tlearn: 0.0965953\ttotal: 3m 16s\tremaining: 2.59s\n",
      "987:\tlearn: 0.0965618\ttotal: 3m 16s\tremaining: 2.39s\n",
      "988:\tlearn: 0.0965355\ttotal: 3m 16s\tremaining: 2.19s\n",
      "989:\tlearn: 0.0965101\ttotal: 3m 16s\tremaining: 1.99s\n",
      "990:\tlearn: 0.0964833\ttotal: 3m 17s\tremaining: 1.79s\n",
      "991:\tlearn: 0.0964544\ttotal: 3m 17s\tremaining: 1.59s\n",
      "992:\tlearn: 0.0964166\ttotal: 3m 17s\tremaining: 1.39s\n",
      "993:\tlearn: 0.0964025\ttotal: 3m 17s\tremaining: 1.19s\n",
      "994:\tlearn: 0.0963752\ttotal: 3m 17s\tremaining: 993ms\n",
      "995:\tlearn: 0.0963522\ttotal: 3m 17s\tremaining: 794ms\n",
      "996:\tlearn: 0.0963118\ttotal: 3m 17s\tremaining: 595ms\n",
      "997:\tlearn: 0.0962967\ttotal: 3m 18s\tremaining: 397ms\n",
      "998:\tlearn: 0.0962696\ttotal: 3m 18s\tremaining: 198ms\n",
      "999:\tlearn: 0.0962374\ttotal: 3m 18s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[55989,   956],\n",
       "       [  500,  1609]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing catboost\n",
    "cbc = CatBoostClassifier(random_state=42)\n",
    "cbc.fit(mod.X_features, mod.y_target)\n",
    "y_pred_cbc = cbc.predict(mod.X_test)\n",
    "print(roc_auc_score(mod.y_test, y_pred_cbc))\n",
    "confusion_matrix(mod.y_test, y_pred_cbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3517638\ttotal: 974ms\tremaining: 1m 36s\n",
      "1:\tlearn: 0.2640057\ttotal: 1.8s\tremaining: 1m 28s\n",
      "2:\tlearn: 0.2282460\ttotal: 2.77s\tremaining: 1m 29s\n",
      "3:\tlearn: 0.2103504\ttotal: 3.37s\tremaining: 1m 20s\n",
      "4:\tlearn: 0.2006201\ttotal: 3.79s\tremaining: 1m 12s\n",
      "5:\tlearn: 0.1937147\ttotal: 4.52s\tremaining: 1m 10s\n",
      "6:\tlearn: 0.1865759\ttotal: 5.23s\tremaining: 1m 9s\n",
      "7:\tlearn: 0.1819507\ttotal: 5.84s\tremaining: 1m 7s\n",
      "8:\tlearn: 0.1774145\ttotal: 6.3s\tremaining: 1m 3s\n",
      "9:\tlearn: 0.1734796\ttotal: 6.89s\tremaining: 1m 2s\n",
      "10:\tlearn: 0.1677005\ttotal: 7.44s\tremaining: 1m\n",
      "11:\tlearn: 0.1646502\ttotal: 7.95s\tremaining: 58.3s\n",
      "12:\tlearn: 0.1618970\ttotal: 8.41s\tremaining: 56.3s\n",
      "13:\tlearn: 0.1597990\ttotal: 9.29s\tremaining: 57s\n",
      "14:\tlearn: 0.1577593\ttotal: 9.8s\tremaining: 55.5s\n",
      "15:\tlearn: 0.1560187\ttotal: 10.2s\tremaining: 53.7s\n",
      "16:\tlearn: 0.1532521\ttotal: 11.2s\tremaining: 54.9s\n",
      "17:\tlearn: 0.1512195\ttotal: 11.9s\tremaining: 54.1s\n",
      "18:\tlearn: 0.1498575\ttotal: 12.4s\tremaining: 52.8s\n",
      "19:\tlearn: 0.1481979\ttotal: 12.8s\tremaining: 51.2s\n",
      "20:\tlearn: 0.1467489\ttotal: 13.3s\tremaining: 49.9s\n",
      "21:\tlearn: 0.1436990\ttotal: 13.8s\tremaining: 48.9s\n",
      "22:\tlearn: 0.1423254\ttotal: 14.2s\tremaining: 47.7s\n",
      "23:\tlearn: 0.1414026\ttotal: 15.5s\tremaining: 49s\n",
      "24:\tlearn: 0.1394646\ttotal: 16.2s\tremaining: 48.6s\n",
      "25:\tlearn: 0.1387917\ttotal: 16.8s\tremaining: 47.9s\n",
      "26:\tlearn: 0.1365140\ttotal: 17.3s\tremaining: 46.8s\n",
      "27:\tlearn: 0.1353182\ttotal: 17.7s\tremaining: 45.6s\n",
      "28:\tlearn: 0.1338506\ttotal: 18.2s\tremaining: 44.5s\n",
      "29:\tlearn: 0.1319820\ttotal: 18.8s\tremaining: 43.8s\n",
      "30:\tlearn: 0.1305698\ttotal: 19.1s\tremaining: 42.4s\n",
      "31:\tlearn: 0.1287626\ttotal: 19.5s\tremaining: 41.4s\n",
      "32:\tlearn: 0.1272618\ttotal: 19.8s\tremaining: 40.3s\n",
      "33:\tlearn: 0.1260707\ttotal: 20.1s\tremaining: 39.1s\n",
      "34:\tlearn: 0.1254810\ttotal: 20.6s\tremaining: 38.2s\n",
      "35:\tlearn: 0.1248150\ttotal: 20.9s\tremaining: 37.1s\n",
      "36:\tlearn: 0.1238381\ttotal: 21.2s\tremaining: 36.1s\n",
      "37:\tlearn: 0.1230888\ttotal: 22.2s\tremaining: 36.2s\n",
      "38:\tlearn: 0.1221578\ttotal: 22.9s\tremaining: 35.8s\n",
      "39:\tlearn: 0.1215840\ttotal: 23.3s\tremaining: 34.9s\n",
      "40:\tlearn: 0.1211732\ttotal: 23.9s\tremaining: 34.4s\n",
      "41:\tlearn: 0.1203388\ttotal: 24.3s\tremaining: 33.5s\n",
      "42:\tlearn: 0.1189232\ttotal: 24.7s\tremaining: 32.7s\n",
      "43:\tlearn: 0.1179171\ttotal: 25s\tremaining: 31.8s\n",
      "44:\tlearn: 0.1168888\ttotal: 25.9s\tremaining: 31.7s\n",
      "45:\tlearn: 0.1157321\ttotal: 26.6s\tremaining: 31.3s\n",
      "46:\tlearn: 0.1154099\ttotal: 27.2s\tremaining: 30.7s\n",
      "47:\tlearn: 0.1146528\ttotal: 27.8s\tremaining: 30.1s\n",
      "48:\tlearn: 0.1138644\ttotal: 28.1s\tremaining: 29.3s\n",
      "49:\tlearn: 0.1131578\ttotal: 28.5s\tremaining: 28.5s\n",
      "50:\tlearn: 0.1124830\ttotal: 28.8s\tremaining: 27.7s\n",
      "51:\tlearn: 0.1119420\ttotal: 29.1s\tremaining: 26.8s\n",
      "52:\tlearn: 0.1112085\ttotal: 29.9s\tremaining: 26.5s\n",
      "53:\tlearn: 0.1103830\ttotal: 30.3s\tremaining: 25.8s\n",
      "54:\tlearn: 0.1095889\ttotal: 31.1s\tremaining: 25.5s\n",
      "55:\tlearn: 0.1088978\ttotal: 31.7s\tremaining: 24.9s\n",
      "56:\tlearn: 0.1082461\ttotal: 32.1s\tremaining: 24.2s\n",
      "57:\tlearn: 0.1076822\ttotal: 33s\tremaining: 23.9s\n",
      "58:\tlearn: 0.1073614\ttotal: 34.2s\tremaining: 23.8s\n",
      "59:\tlearn: 0.1064896\ttotal: 34.8s\tremaining: 23.2s\n",
      "60:\tlearn: 0.1057074\ttotal: 35.2s\tremaining: 22.5s\n",
      "61:\tlearn: 0.1054331\ttotal: 36.2s\tremaining: 22.2s\n",
      "62:\tlearn: 0.1046959\ttotal: 37.5s\tremaining: 22s\n",
      "63:\tlearn: 0.1041658\ttotal: 39.2s\tremaining: 22.1s\n",
      "64:\tlearn: 0.1034065\ttotal: 40.2s\tremaining: 21.7s\n",
      "65:\tlearn: 0.1030178\ttotal: 40.8s\tremaining: 21s\n",
      "66:\tlearn: 0.1024537\ttotal: 41.2s\tremaining: 20.3s\n",
      "67:\tlearn: 0.1012625\ttotal: 42.1s\tremaining: 19.8s\n",
      "68:\tlearn: 0.1008987\ttotal: 43.1s\tremaining: 19.3s\n",
      "69:\tlearn: 0.0999777\ttotal: 44.1s\tremaining: 18.9s\n",
      "70:\tlearn: 0.0990045\ttotal: 44.9s\tremaining: 18.3s\n",
      "71:\tlearn: 0.0985313\ttotal: 45.3s\tremaining: 17.6s\n",
      "72:\tlearn: 0.0979798\ttotal: 46.1s\tremaining: 17.1s\n",
      "73:\tlearn: 0.0973870\ttotal: 1m\tremaining: 21.3s\n",
      "74:\tlearn: 0.0968237\ttotal: 1m 12s\tremaining: 24s\n",
      "75:\tlearn: 0.0957872\ttotal: 1m 22s\tremaining: 26.1s\n",
      "76:\tlearn: 0.0949447\ttotal: 1m 25s\tremaining: 25.5s\n",
      "77:\tlearn: 0.0947110\ttotal: 1m 26s\tremaining: 24.4s\n",
      "78:\tlearn: 0.0943319\ttotal: 1m 27s\tremaining: 23.2s\n",
      "79:\tlearn: 0.0936805\ttotal: 1m 28s\tremaining: 22.2s\n",
      "80:\tlearn: 0.0927363\ttotal: 1m 31s\tremaining: 21.4s\n",
      "81:\tlearn: 0.0924144\ttotal: 1m 33s\tremaining: 20.4s\n",
      "82:\tlearn: 0.0918065\ttotal: 1m 35s\tremaining: 19.5s\n",
      "83:\tlearn: 0.0913752\ttotal: 1m 35s\tremaining: 18.3s\n",
      "84:\tlearn: 0.0911620\ttotal: 1m 36s\tremaining: 17s\n",
      "85:\tlearn: 0.0907690\ttotal: 1m 36s\tremaining: 15.8s\n",
      "86:\tlearn: 0.0904588\ttotal: 1m 37s\tremaining: 14.5s\n",
      "87:\tlearn: 0.0899142\ttotal: 1m 39s\tremaining: 13.5s\n",
      "88:\tlearn: 0.0897486\ttotal: 1m 39s\tremaining: 12.3s\n",
      "89:\tlearn: 0.0893485\ttotal: 1m 40s\tremaining: 11.1s\n",
      "90:\tlearn: 0.0887666\ttotal: 1m 41s\tremaining: 9.99s\n",
      "91:\tlearn: 0.0882787\ttotal: 1m 41s\tremaining: 8.86s\n",
      "92:\tlearn: 0.0881243\ttotal: 1m 42s\tremaining: 7.7s\n",
      "93:\tlearn: 0.0873299\ttotal: 1m 43s\tremaining: 6.58s\n",
      "94:\tlearn: 0.0871956\ttotal: 1m 43s\tremaining: 5.46s\n",
      "95:\tlearn: 0.0867169\ttotal: 1m 44s\tremaining: 4.34s\n",
      "96:\tlearn: 0.0865507\ttotal: 1m 45s\tremaining: 3.25s\n",
      "97:\tlearn: 0.0860068\ttotal: 1m 45s\tremaining: 2.16s\n",
      "98:\tlearn: 0.0856561\ttotal: 1m 46s\tremaining: 1.07s\n",
      "99:\tlearn: 0.0853949\ttotal: 1m 46s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.989, total= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3678666\ttotal: 1.06s\tremaining: 1m 45s\n",
      "1:\tlearn: 0.2734234\ttotal: 2.43s\tremaining: 1m 59s\n",
      "2:\tlearn: 0.2340610\ttotal: 3.3s\tremaining: 1m 46s\n",
      "3:\tlearn: 0.2113590\ttotal: 3.65s\tremaining: 1m 27s\n",
      "4:\tlearn: 0.2023309\ttotal: 4.4s\tremaining: 1m 23s\n",
      "5:\tlearn: 0.1927832\ttotal: 4.77s\tremaining: 1m 14s\n",
      "6:\tlearn: 0.1863297\ttotal: 8.16s\tremaining: 1m 48s\n",
      "7:\tlearn: 0.1798634\ttotal: 9.22s\tremaining: 1m 46s\n",
      "8:\tlearn: 0.1758478\ttotal: 9.65s\tremaining: 1m 37s\n",
      "9:\tlearn: 0.1692001\ttotal: 10.4s\tremaining: 1m 33s\n",
      "10:\tlearn: 0.1656409\ttotal: 10.8s\tremaining: 1m 27s\n",
      "11:\tlearn: 0.1626856\ttotal: 11.1s\tremaining: 1m 21s\n",
      "12:\tlearn: 0.1592466\ttotal: 11.8s\tremaining: 1m 19s\n",
      "13:\tlearn: 0.1576300\ttotal: 12.2s\tremaining: 1m 14s\n",
      "14:\tlearn: 0.1554937\ttotal: 12.7s\tremaining: 1m 11s\n",
      "15:\tlearn: 0.1532581\ttotal: 13.1s\tremaining: 1m 8s\n",
      "16:\tlearn: 0.1516407\ttotal: 13.5s\tremaining: 1m 6s\n",
      "17:\tlearn: 0.1493231\ttotal: 14s\tremaining: 1m 3s\n",
      "18:\tlearn: 0.1472880\ttotal: 14.3s\tremaining: 1m\n",
      "19:\tlearn: 0.1453697\ttotal: 14.7s\tremaining: 58.7s\n",
      "20:\tlearn: 0.1418944\ttotal: 15.1s\tremaining: 56.7s\n",
      "21:\tlearn: 0.1406834\ttotal: 15.6s\tremaining: 55.2s\n",
      "22:\tlearn: 0.1395664\ttotal: 16.1s\tremaining: 53.8s\n",
      "23:\tlearn: 0.1375267\ttotal: 16.7s\tremaining: 52.9s\n",
      "24:\tlearn: 0.1362963\ttotal: 17.5s\tremaining: 52.5s\n",
      "25:\tlearn: 0.1354221\ttotal: 18.1s\tremaining: 51.5s\n",
      "26:\tlearn: 0.1339730\ttotal: 19s\tremaining: 51.3s\n",
      "27:\tlearn: 0.1326901\ttotal: 19.6s\tremaining: 50.3s\n",
      "28:\tlearn: 0.1314536\ttotal: 20.4s\tremaining: 49.9s\n",
      "29:\tlearn: 0.1305618\ttotal: 21.1s\tremaining: 49.3s\n",
      "30:\tlearn: 0.1297760\ttotal: 21.5s\tremaining: 47.9s\n",
      "31:\tlearn: 0.1288366\ttotal: 22.3s\tremaining: 47.5s\n",
      "32:\tlearn: 0.1275979\ttotal: 23.2s\tremaining: 47s\n",
      "33:\tlearn: 0.1265199\ttotal: 23.6s\tremaining: 45.7s\n",
      "34:\tlearn: 0.1260104\ttotal: 24.1s\tremaining: 44.7s\n",
      "35:\tlearn: 0.1252095\ttotal: 24.9s\tremaining: 44.2s\n",
      "36:\tlearn: 0.1243345\ttotal: 25.6s\tremaining: 43.5s\n",
      "37:\tlearn: 0.1234153\ttotal: 26.3s\tremaining: 43s\n",
      "38:\tlearn: 0.1226348\ttotal: 26.8s\tremaining: 41.8s\n",
      "39:\tlearn: 0.1218187\ttotal: 27.1s\tremaining: 40.6s\n",
      "40:\tlearn: 0.1208467\ttotal: 28s\tremaining: 40.3s\n",
      "41:\tlearn: 0.1196773\ttotal: 28.4s\tremaining: 39.2s\n",
      "42:\tlearn: 0.1187768\ttotal: 28.9s\tremaining: 38.3s\n",
      "43:\tlearn: 0.1179380\ttotal: 29.4s\tremaining: 37.4s\n",
      "44:\tlearn: 0.1169910\ttotal: 29.8s\tremaining: 36.4s\n",
      "45:\tlearn: 0.1164096\ttotal: 30.2s\tremaining: 35.4s\n",
      "46:\tlearn: 0.1159050\ttotal: 30.5s\tremaining: 34.4s\n",
      "47:\tlearn: 0.1141509\ttotal: 31.3s\tremaining: 33.9s\n",
      "48:\tlearn: 0.1132978\ttotal: 31.6s\tremaining: 32.9s\n",
      "49:\tlearn: 0.1125509\ttotal: 32s\tremaining: 32s\n",
      "50:\tlearn: 0.1116212\ttotal: 32.4s\tremaining: 31.1s\n",
      "51:\tlearn: 0.1109444\ttotal: 32.9s\tremaining: 30.4s\n",
      "52:\tlearn: 0.1100152\ttotal: 33.3s\tremaining: 29.5s\n",
      "53:\tlearn: 0.1089185\ttotal: 33.6s\tremaining: 28.7s\n",
      "54:\tlearn: 0.1082288\ttotal: 34.2s\tremaining: 28s\n",
      "55:\tlearn: 0.1073326\ttotal: 34.9s\tremaining: 27.4s\n",
      "56:\tlearn: 0.1063678\ttotal: 35.3s\tremaining: 26.6s\n",
      "57:\tlearn: 0.1053540\ttotal: 35.8s\tremaining: 25.9s\n",
      "58:\tlearn: 0.1047034\ttotal: 36.1s\tremaining: 25.1s\n",
      "59:\tlearn: 0.1044268\ttotal: 36.5s\tremaining: 24.4s\n",
      "60:\tlearn: 0.1036545\ttotal: 36.9s\tremaining: 23.6s\n",
      "61:\tlearn: 0.1032286\ttotal: 37.5s\tremaining: 23s\n",
      "62:\tlearn: 0.1020703\ttotal: 38s\tremaining: 22.3s\n",
      "63:\tlearn: 0.1014462\ttotal: 38.4s\tremaining: 21.6s\n",
      "64:\tlearn: 0.1010082\ttotal: 38.7s\tremaining: 20.9s\n",
      "65:\tlearn: 0.1003405\ttotal: 39.4s\tremaining: 20.3s\n",
      "66:\tlearn: 0.1000239\ttotal: 39.7s\tremaining: 19.6s\n",
      "67:\tlearn: 0.0996667\ttotal: 40.1s\tremaining: 18.9s\n",
      "68:\tlearn: 0.0994849\ttotal: 40.5s\tremaining: 18.2s\n",
      "69:\tlearn: 0.0987795\ttotal: 41.3s\tremaining: 17.7s\n",
      "70:\tlearn: 0.0985214\ttotal: 42.3s\tremaining: 17.3s\n",
      "71:\tlearn: 0.0978074\ttotal: 43.2s\tremaining: 16.8s\n",
      "72:\tlearn: 0.0972953\ttotal: 44.2s\tremaining: 16.3s\n",
      "73:\tlearn: 0.0965860\ttotal: 44.8s\tremaining: 15.7s\n",
      "74:\tlearn: 0.0960330\ttotal: 45.3s\tremaining: 15.1s\n",
      "75:\tlearn: 0.0953348\ttotal: 46.4s\tremaining: 14.6s\n",
      "76:\tlearn: 0.0949341\ttotal: 46.9s\tremaining: 14s\n",
      "77:\tlearn: 0.0942655\ttotal: 47.7s\tremaining: 13.4s\n",
      "78:\tlearn: 0.0938169\ttotal: 48.1s\tremaining: 12.8s\n",
      "79:\tlearn: 0.0934024\ttotal: 48.7s\tremaining: 12.2s\n",
      "80:\tlearn: 0.0928037\ttotal: 49.5s\tremaining: 11.6s\n",
      "81:\tlearn: 0.0920037\ttotal: 50.5s\tremaining: 11.1s\n",
      "82:\tlearn: 0.0917394\ttotal: 51.1s\tremaining: 10.5s\n",
      "83:\tlearn: 0.0911866\ttotal: 51.8s\tremaining: 9.87s\n",
      "84:\tlearn: 0.0906686\ttotal: 52.3s\tremaining: 9.23s\n",
      "85:\tlearn: 0.0903005\ttotal: 52.6s\tremaining: 8.57s\n",
      "86:\tlearn: 0.0898691\ttotal: 53.1s\tremaining: 7.93s\n",
      "87:\tlearn: 0.0896628\ttotal: 53.6s\tremaining: 7.31s\n",
      "88:\tlearn: 0.0892481\ttotal: 54.4s\tremaining: 6.72s\n",
      "89:\tlearn: 0.0886539\ttotal: 54.9s\tremaining: 6.1s\n",
      "90:\tlearn: 0.0882671\ttotal: 55.2s\tremaining: 5.46s\n",
      "91:\tlearn: 0.0874264\ttotal: 55.5s\tremaining: 4.83s\n",
      "92:\tlearn: 0.0871121\ttotal: 55.9s\tremaining: 4.21s\n",
      "93:\tlearn: 0.0868538\ttotal: 56.2s\tremaining: 3.59s\n",
      "94:\tlearn: 0.0866874\ttotal: 56.5s\tremaining: 2.97s\n",
      "95:\tlearn: 0.0863487\ttotal: 56.9s\tremaining: 2.37s\n",
      "96:\tlearn: 0.0860511\ttotal: 57.2s\tremaining: 1.77s\n",
      "97:\tlearn: 0.0856826\ttotal: 57.5s\tremaining: 1.17s\n",
      "98:\tlearn: 0.0854267\ttotal: 58.3s\tremaining: 589ms\n",
      "99:\tlearn: 0.0851186\ttotal: 58.7s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.988, total= 1.0min\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3573777\ttotal: 615ms\tremaining: 1m\n",
      "1:\tlearn: 0.2708593\ttotal: 947ms\tremaining: 46.4s\n",
      "2:\tlearn: 0.2303787\ttotal: 1.5s\tremaining: 48.4s\n",
      "3:\tlearn: 0.2104509\ttotal: 1.85s\tremaining: 44.3s\n",
      "4:\tlearn: 0.1993448\ttotal: 2.45s\tremaining: 46.5s\n",
      "5:\tlearn: 0.1925785\ttotal: 2.86s\tremaining: 44.8s\n",
      "6:\tlearn: 0.1829506\ttotal: 3.57s\tremaining: 47.5s\n",
      "7:\tlearn: 0.1783673\ttotal: 3.99s\tremaining: 45.9s\n",
      "8:\tlearn: 0.1744869\ttotal: 4.63s\tremaining: 46.9s\n",
      "9:\tlearn: 0.1705928\ttotal: 5.04s\tremaining: 45.4s\n",
      "10:\tlearn: 0.1664252\ttotal: 5.79s\tremaining: 46.9s\n",
      "11:\tlearn: 0.1639460\ttotal: 6.08s\tremaining: 44.6s\n",
      "12:\tlearn: 0.1616665\ttotal: 6.72s\tremaining: 45s\n",
      "13:\tlearn: 0.1594031\ttotal: 7.12s\tremaining: 43.7s\n",
      "14:\tlearn: 0.1569990\ttotal: 7.62s\tremaining: 43.2s\n",
      "15:\tlearn: 0.1555530\ttotal: 8.01s\tremaining: 42s\n",
      "16:\tlearn: 0.1544626\ttotal: 8.78s\tremaining: 42.9s\n",
      "17:\tlearn: 0.1519710\ttotal: 9.23s\tremaining: 42s\n",
      "18:\tlearn: 0.1497922\ttotal: 9.67s\tremaining: 41.2s\n",
      "19:\tlearn: 0.1479423\ttotal: 9.98s\tremaining: 39.9s\n",
      "20:\tlearn: 0.1447771\ttotal: 10.8s\tremaining: 40.6s\n",
      "21:\tlearn: 0.1430546\ttotal: 11.2s\tremaining: 39.5s\n",
      "22:\tlearn: 0.1418148\ttotal: 11.9s\tremaining: 39.8s\n",
      "23:\tlearn: 0.1400853\ttotal: 12.5s\tremaining: 39.6s\n",
      "24:\tlearn: 0.1383288\ttotal: 12.9s\tremaining: 38.6s\n",
      "25:\tlearn: 0.1355173\ttotal: 13.4s\tremaining: 38s\n",
      "26:\tlearn: 0.1341003\ttotal: 13.7s\tremaining: 37.2s\n",
      "27:\tlearn: 0.1327845\ttotal: 14s\tremaining: 36.1s\n",
      "28:\tlearn: 0.1318690\ttotal: 14.6s\tremaining: 35.7s\n",
      "29:\tlearn: 0.1312795\ttotal: 14.9s\tremaining: 34.7s\n",
      "30:\tlearn: 0.1298171\ttotal: 15.3s\tremaining: 34s\n",
      "31:\tlearn: 0.1286573\ttotal: 15.8s\tremaining: 33.5s\n",
      "32:\tlearn: 0.1275391\ttotal: 16.1s\tremaining: 32.6s\n",
      "33:\tlearn: 0.1258303\ttotal: 16.7s\tremaining: 32.3s\n",
      "34:\tlearn: 0.1248604\ttotal: 16.9s\tremaining: 31.4s\n",
      "35:\tlearn: 0.1241523\ttotal: 17.2s\tremaining: 30.6s\n",
      "36:\tlearn: 0.1229690\ttotal: 17.8s\tremaining: 30.4s\n",
      "37:\tlearn: 0.1223288\ttotal: 18.1s\tremaining: 29.5s\n",
      "38:\tlearn: 0.1212592\ttotal: 18.6s\tremaining: 29s\n",
      "39:\tlearn: 0.1204124\ttotal: 18.9s\tremaining: 28.4s\n",
      "40:\tlearn: 0.1192652\ttotal: 19.3s\tremaining: 27.8s\n",
      "41:\tlearn: 0.1186605\ttotal: 19.7s\tremaining: 27.1s\n",
      "42:\tlearn: 0.1181513\ttotal: 19.9s\tremaining: 26.4s\n",
      "43:\tlearn: 0.1172541\ttotal: 20.3s\tremaining: 25.9s\n",
      "44:\tlearn: 0.1159670\ttotal: 20.9s\tremaining: 25.6s\n",
      "45:\tlearn: 0.1152468\ttotal: 21.5s\tremaining: 25.2s\n",
      "46:\tlearn: 0.1142439\ttotal: 21.8s\tremaining: 24.6s\n",
      "47:\tlearn: 0.1128926\ttotal: 22.3s\tremaining: 24.1s\n",
      "48:\tlearn: 0.1109423\ttotal: 23.7s\tremaining: 24.6s\n",
      "49:\tlearn: 0.1101852\ttotal: 24.7s\tremaining: 24.7s\n",
      "50:\tlearn: 0.1094464\ttotal: 25.6s\tremaining: 24.6s\n",
      "51:\tlearn: 0.1083719\ttotal: 26.2s\tremaining: 24.2s\n",
      "52:\tlearn: 0.1078623\ttotal: 27s\tremaining: 23.9s\n",
      "53:\tlearn: 0.1073020\ttotal: 27.8s\tremaining: 23.7s\n",
      "54:\tlearn: 0.1066868\ttotal: 28.1s\tremaining: 23s\n",
      "55:\tlearn: 0.1057713\ttotal: 28.5s\tremaining: 22.4s\n",
      "56:\tlearn: 0.1053275\ttotal: 28.9s\tremaining: 21.8s\n",
      "57:\tlearn: 0.1049216\ttotal: 29.2s\tremaining: 21.2s\n",
      "58:\tlearn: 0.1042878\ttotal: 30s\tremaining: 20.8s\n",
      "59:\tlearn: 0.1037909\ttotal: 30.6s\tremaining: 20.4s\n",
      "60:\tlearn: 0.1031945\ttotal: 31s\tremaining: 19.8s\n",
      "61:\tlearn: 0.1023197\ttotal: 31.4s\tremaining: 19.2s\n",
      "62:\tlearn: 0.1019325\ttotal: 32.1s\tremaining: 18.9s\n",
      "63:\tlearn: 0.1013014\ttotal: 32.9s\tremaining: 18.5s\n",
      "64:\tlearn: 0.1007104\ttotal: 33.3s\tremaining: 17.9s\n",
      "65:\tlearn: 0.0999118\ttotal: 34s\tremaining: 17.5s\n",
      "66:\tlearn: 0.0992851\ttotal: 34.8s\tremaining: 17.2s\n",
      "67:\tlearn: 0.0986165\ttotal: 35.9s\tremaining: 16.9s\n",
      "68:\tlearn: 0.0981634\ttotal: 37s\tremaining: 16.6s\n",
      "69:\tlearn: 0.0976513\ttotal: 37.8s\tremaining: 16.2s\n",
      "70:\tlearn: 0.0968186\ttotal: 38.4s\tremaining: 15.7s\n",
      "71:\tlearn: 0.0964347\ttotal: 40.1s\tremaining: 15.6s\n",
      "72:\tlearn: 0.0960587\ttotal: 41s\tremaining: 15.2s\n",
      "73:\tlearn: 0.0957170\ttotal: 41.3s\tremaining: 14.5s\n",
      "74:\tlearn: 0.0951517\ttotal: 41.9s\tremaining: 14s\n",
      "75:\tlearn: 0.0946141\ttotal: 42.4s\tremaining: 13.4s\n",
      "76:\tlearn: 0.0939281\ttotal: 42.9s\tremaining: 12.8s\n",
      "77:\tlearn: 0.0935864\ttotal: 43.4s\tremaining: 12.3s\n",
      "78:\tlearn: 0.0933271\ttotal: 43.9s\tremaining: 11.7s\n",
      "79:\tlearn: 0.0929943\ttotal: 44.8s\tremaining: 11.2s\n",
      "80:\tlearn: 0.0923078\ttotal: 45.2s\tremaining: 10.6s\n",
      "81:\tlearn: 0.0921159\ttotal: 45.7s\tremaining: 10s\n",
      "82:\tlearn: 0.0917084\ttotal: 46.1s\tremaining: 9.44s\n",
      "83:\tlearn: 0.0914722\ttotal: 46.7s\tremaining: 8.89s\n",
      "84:\tlearn: 0.0907380\ttotal: 47.4s\tremaining: 8.36s\n",
      "85:\tlearn: 0.0900055\ttotal: 47.9s\tremaining: 7.8s\n",
      "86:\tlearn: 0.0894592\ttotal: 48.4s\tremaining: 7.22s\n",
      "87:\tlearn: 0.0891714\ttotal: 48.8s\tremaining: 6.65s\n",
      "88:\tlearn: 0.0887746\ttotal: 49.2s\tremaining: 6.08s\n",
      "89:\tlearn: 0.0885926\ttotal: 49.7s\tremaining: 5.52s\n",
      "90:\tlearn: 0.0881691\ttotal: 50.2s\tremaining: 4.96s\n",
      "91:\tlearn: 0.0877324\ttotal: 50.7s\tremaining: 4.41s\n",
      "92:\tlearn: 0.0876023\ttotal: 51.1s\tremaining: 3.84s\n",
      "93:\tlearn: 0.0870829\ttotal: 51.6s\tremaining: 3.29s\n",
      "94:\tlearn: 0.0865990\ttotal: 52.2s\tremaining: 2.75s\n",
      "95:\tlearn: 0.0863757\ttotal: 52.6s\tremaining: 2.19s\n",
      "96:\tlearn: 0.0863106\ttotal: 53.3s\tremaining: 1.65s\n",
      "97:\tlearn: 0.0861944\ttotal: 53.8s\tremaining: 1.1s\n",
      "98:\tlearn: 0.0859657\ttotal: 54.4s\tremaining: 549ms\n",
      "99:\tlearn: 0.0854417\ttotal: 55s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.990, total=  56.9s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3505344\ttotal: 749ms\tremaining: 1m 14s\n",
      "1:\tlearn: 0.2704401\ttotal: 1.28s\tremaining: 1m 2s\n",
      "2:\tlearn: 0.2281601\ttotal: 1.68s\tremaining: 54.4s\n",
      "3:\tlearn: 0.2123861\ttotal: 2.05s\tremaining: 49.2s\n",
      "4:\tlearn: 0.2025944\ttotal: 2.42s\tremaining: 46s\n",
      "5:\tlearn: 0.1960754\ttotal: 3.11s\tremaining: 48.7s\n",
      "6:\tlearn: 0.1856251\ttotal: 3.86s\tremaining: 51.3s\n",
      "7:\tlearn: 0.1804190\ttotal: 4.31s\tremaining: 49.6s\n",
      "8:\tlearn: 0.1754210\ttotal: 4.81s\tremaining: 48.6s\n",
      "9:\tlearn: 0.1723487\ttotal: 5.84s\tremaining: 52.6s\n",
      "10:\tlearn: 0.1691242\ttotal: 6.45s\tremaining: 52.2s\n",
      "11:\tlearn: 0.1658809\ttotal: 6.87s\tremaining: 50.4s\n",
      "12:\tlearn: 0.1617025\ttotal: 7.4s\tremaining: 49.5s\n",
      "13:\tlearn: 0.1576541\ttotal: 7.88s\tremaining: 48.4s\n",
      "14:\tlearn: 0.1556424\ttotal: 8.26s\tremaining: 46.8s\n",
      "15:\tlearn: 0.1542335\ttotal: 8.76s\tremaining: 46s\n",
      "16:\tlearn: 0.1521521\ttotal: 9.31s\tremaining: 45.5s\n",
      "17:\tlearn: 0.1505297\ttotal: 9.94s\tremaining: 45.3s\n",
      "18:\tlearn: 0.1494514\ttotal: 10.3s\tremaining: 43.9s\n",
      "19:\tlearn: 0.1465273\ttotal: 10.8s\tremaining: 43.1s\n",
      "20:\tlearn: 0.1451661\ttotal: 11.3s\tremaining: 42.6s\n",
      "21:\tlearn: 0.1438894\ttotal: 12.1s\tremaining: 43s\n",
      "22:\tlearn: 0.1429118\ttotal: 12.4s\tremaining: 41.6s\n",
      "23:\tlearn: 0.1407603\ttotal: 12.9s\tremaining: 40.9s\n",
      "24:\tlearn: 0.1394090\ttotal: 13.2s\tremaining: 39.7s\n",
      "25:\tlearn: 0.1372001\ttotal: 13.7s\tremaining: 39.1s\n",
      "26:\tlearn: 0.1351056\ttotal: 14.4s\tremaining: 38.9s\n",
      "27:\tlearn: 0.1335896\ttotal: 15s\tremaining: 38.5s\n",
      "28:\tlearn: 0.1322351\ttotal: 15.5s\tremaining: 38s\n",
      "29:\tlearn: 0.1310237\ttotal: 15.9s\tremaining: 37.1s\n",
      "30:\tlearn: 0.1299105\ttotal: 16.7s\tremaining: 37.1s\n",
      "31:\tlearn: 0.1291079\ttotal: 17.2s\tremaining: 36.6s\n",
      "32:\tlearn: 0.1272917\ttotal: 17.8s\tremaining: 36.1s\n",
      "33:\tlearn: 0.1256743\ttotal: 18.2s\tremaining: 35.3s\n",
      "34:\tlearn: 0.1247947\ttotal: 18.6s\tremaining: 34.5s\n",
      "35:\tlearn: 0.1241081\ttotal: 18.9s\tremaining: 33.5s\n",
      "36:\tlearn: 0.1230755\ttotal: 19.2s\tremaining: 32.8s\n",
      "37:\tlearn: 0.1222265\ttotal: 19.7s\tremaining: 32.1s\n",
      "38:\tlearn: 0.1215098\ttotal: 20.4s\tremaining: 32s\n",
      "39:\tlearn: 0.1198157\ttotal: 20.9s\tremaining: 31.3s\n",
      "40:\tlearn: 0.1189834\ttotal: 21.3s\tremaining: 30.6s\n",
      "41:\tlearn: 0.1185186\ttotal: 21.7s\tremaining: 29.9s\n",
      "42:\tlearn: 0.1180244\ttotal: 22s\tremaining: 29.1s\n",
      "43:\tlearn: 0.1174033\ttotal: 22.5s\tremaining: 28.6s\n",
      "44:\tlearn: 0.1159008\ttotal: 22.9s\tremaining: 28s\n",
      "45:\tlearn: 0.1152302\ttotal: 23.4s\tremaining: 27.5s\n",
      "46:\tlearn: 0.1147009\ttotal: 23.8s\tremaining: 26.8s\n",
      "47:\tlearn: 0.1141271\ttotal: 24.2s\tremaining: 26.3s\n",
      "48:\tlearn: 0.1129631\ttotal: 25s\tremaining: 26s\n",
      "49:\tlearn: 0.1122556\ttotal: 25.4s\tremaining: 25.4s\n",
      "50:\tlearn: 0.1112759\ttotal: 25.9s\tremaining: 24.9s\n",
      "51:\tlearn: 0.1104382\ttotal: 26.3s\tremaining: 24.2s\n",
      "52:\tlearn: 0.1097823\ttotal: 26.8s\tremaining: 23.8s\n",
      "53:\tlearn: 0.1090771\ttotal: 27.3s\tremaining: 23.3s\n",
      "54:\tlearn: 0.1082877\ttotal: 27.7s\tremaining: 22.7s\n",
      "55:\tlearn: 0.1076721\ttotal: 28.2s\tremaining: 22.1s\n",
      "56:\tlearn: 0.1070848\ttotal: 28.9s\tremaining: 21.8s\n",
      "57:\tlearn: 0.1064978\ttotal: 29.4s\tremaining: 21.3s\n",
      "58:\tlearn: 0.1052577\ttotal: 29.9s\tremaining: 20.8s\n",
      "59:\tlearn: 0.1046107\ttotal: 30.3s\tremaining: 20.2s\n",
      "60:\tlearn: 0.1036823\ttotal: 30.7s\tremaining: 19.6s\n",
      "61:\tlearn: 0.1030385\ttotal: 31.2s\tremaining: 19.1s\n",
      "62:\tlearn: 0.1023652\ttotal: 31.6s\tremaining: 18.6s\n",
      "63:\tlearn: 0.1007263\ttotal: 32.1s\tremaining: 18s\n",
      "64:\tlearn: 0.1001143\ttotal: 32.6s\tremaining: 17.6s\n",
      "65:\tlearn: 0.0996156\ttotal: 33s\tremaining: 17s\n",
      "66:\tlearn: 0.0990732\ttotal: 33.4s\tremaining: 16.5s\n",
      "67:\tlearn: 0.0986825\ttotal: 33.8s\tremaining: 15.9s\n",
      "68:\tlearn: 0.0982844\ttotal: 34.2s\tremaining: 15.4s\n",
      "69:\tlearn: 0.0976241\ttotal: 34.7s\tremaining: 14.9s\n",
      "70:\tlearn: 0.0967266\ttotal: 35.1s\tremaining: 14.3s\n",
      "71:\tlearn: 0.0961753\ttotal: 35.6s\tremaining: 13.9s\n",
      "72:\tlearn: 0.0957484\ttotal: 36.3s\tremaining: 13.4s\n",
      "73:\tlearn: 0.0951311\ttotal: 36.8s\tremaining: 12.9s\n",
      "74:\tlearn: 0.0948464\ttotal: 37.2s\tremaining: 12.4s\n",
      "75:\tlearn: 0.0945382\ttotal: 38s\tremaining: 12s\n",
      "76:\tlearn: 0.0942224\ttotal: 38.9s\tremaining: 11.6s\n",
      "77:\tlearn: 0.0938162\ttotal: 39.3s\tremaining: 11.1s\n",
      "78:\tlearn: 0.0928353\ttotal: 39.9s\tremaining: 10.6s\n",
      "79:\tlearn: 0.0920607\ttotal: 40.7s\tremaining: 10.2s\n",
      "80:\tlearn: 0.0916316\ttotal: 41.6s\tremaining: 9.76s\n",
      "81:\tlearn: 0.0911064\ttotal: 42.4s\tremaining: 9.31s\n",
      "82:\tlearn: 0.0910175\ttotal: 43.1s\tremaining: 8.82s\n",
      "83:\tlearn: 0.0906804\ttotal: 43.6s\tremaining: 8.3s\n",
      "84:\tlearn: 0.0898991\ttotal: 43.9s\tremaining: 7.75s\n",
      "85:\tlearn: 0.0893958\ttotal: 44.5s\tremaining: 7.25s\n",
      "86:\tlearn: 0.0891989\ttotal: 45.2s\tremaining: 6.75s\n",
      "87:\tlearn: 0.0889999\ttotal: 45.6s\tremaining: 6.22s\n",
      "88:\tlearn: 0.0887329\ttotal: 46.1s\tremaining: 5.7s\n",
      "89:\tlearn: 0.0885187\ttotal: 46.8s\tremaining: 5.2s\n",
      "90:\tlearn: 0.0881246\ttotal: 47.2s\tremaining: 4.67s\n",
      "91:\tlearn: 0.0876395\ttotal: 47.8s\tremaining: 4.15s\n",
      "92:\tlearn: 0.0875927\ttotal: 48.3s\tremaining: 3.63s\n",
      "93:\tlearn: 0.0867636\ttotal: 48.6s\tremaining: 3.1s\n",
      "94:\tlearn: 0.0861449\ttotal: 49.1s\tremaining: 2.58s\n",
      "95:\tlearn: 0.0857940\ttotal: 49.4s\tremaining: 2.06s\n",
      "96:\tlearn: 0.0854146\ttotal: 50s\tremaining: 1.55s\n",
      "97:\tlearn: 0.0848928\ttotal: 50.4s\tremaining: 1.03s\n",
      "98:\tlearn: 0.0846853\ttotal: 50.8s\tremaining: 513ms\n",
      "99:\tlearn: 0.0842714\ttotal: 51.2s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.988, total=  52.8s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  4.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3551216\ttotal: 739ms\tremaining: 1m 13s\n",
      "1:\tlearn: 0.2753895\ttotal: 1.33s\tremaining: 1m 5s\n",
      "2:\tlearn: 0.2280371\ttotal: 2.1s\tremaining: 1m 8s\n",
      "3:\tlearn: 0.2135853\ttotal: 2.77s\tremaining: 1m 6s\n",
      "4:\tlearn: 0.2000451\ttotal: 3.37s\tremaining: 1m 3s\n",
      "5:\tlearn: 0.1923724\ttotal: 4.15s\tremaining: 1m 5s\n",
      "6:\tlearn: 0.1847403\ttotal: 4.53s\tremaining: 1m\n",
      "7:\tlearn: 0.1794842\ttotal: 4.95s\tremaining: 56.9s\n",
      "8:\tlearn: 0.1751991\ttotal: 5.45s\tremaining: 55.1s\n",
      "9:\tlearn: 0.1720694\ttotal: 5.75s\tremaining: 51.7s\n",
      "10:\tlearn: 0.1689517\ttotal: 6.3s\tremaining: 51s\n",
      "11:\tlearn: 0.1659353\ttotal: 6.84s\tremaining: 50.1s\n",
      "12:\tlearn: 0.1627789\ttotal: 7.39s\tremaining: 49.5s\n",
      "13:\tlearn: 0.1601283\ttotal: 7.79s\tremaining: 47.9s\n",
      "14:\tlearn: 0.1583137\ttotal: 8.19s\tremaining: 46.4s\n",
      "15:\tlearn: 0.1555892\ttotal: 8.63s\tremaining: 45.3s\n",
      "16:\tlearn: 0.1515482\ttotal: 9.07s\tremaining: 44.3s\n",
      "17:\tlearn: 0.1491752\ttotal: 9.48s\tremaining: 43.2s\n",
      "18:\tlearn: 0.1483287\ttotal: 9.91s\tremaining: 42.2s\n",
      "19:\tlearn: 0.1471773\ttotal: 10.2s\tremaining: 40.7s\n",
      "20:\tlearn: 0.1457894\ttotal: 10.6s\tremaining: 39.9s\n",
      "21:\tlearn: 0.1443208\ttotal: 10.9s\tremaining: 38.6s\n",
      "22:\tlearn: 0.1421007\ttotal: 11.2s\tremaining: 37.6s\n",
      "23:\tlearn: 0.1406013\ttotal: 11.5s\tremaining: 36.6s\n",
      "24:\tlearn: 0.1389331\ttotal: 12s\tremaining: 36s\n",
      "25:\tlearn: 0.1378655\ttotal: 12.3s\tremaining: 35s\n",
      "26:\tlearn: 0.1365874\ttotal: 12.7s\tremaining: 34.3s\n",
      "27:\tlearn: 0.1344749\ttotal: 13.4s\tremaining: 34.3s\n",
      "28:\tlearn: 0.1329099\ttotal: 13.7s\tremaining: 33.5s\n",
      "29:\tlearn: 0.1316944\ttotal: 14s\tremaining: 32.7s\n",
      "30:\tlearn: 0.1307076\ttotal: 14.4s\tremaining: 32s\n",
      "31:\tlearn: 0.1285651\ttotal: 14.7s\tremaining: 31.3s\n",
      "32:\tlearn: 0.1267300\ttotal: 15.1s\tremaining: 30.6s\n",
      "33:\tlearn: 0.1257088\ttotal: 15.5s\tremaining: 30s\n",
      "34:\tlearn: 0.1238748\ttotal: 15.9s\tremaining: 29.6s\n",
      "35:\tlearn: 0.1228219\ttotal: 16.2s\tremaining: 28.9s\n",
      "36:\tlearn: 0.1215978\ttotal: 16.7s\tremaining: 28.4s\n",
      "37:\tlearn: 0.1207798\ttotal: 17.1s\tremaining: 27.8s\n",
      "38:\tlearn: 0.1196987\ttotal: 17.4s\tremaining: 27.2s\n",
      "39:\tlearn: 0.1189896\ttotal: 17.7s\tremaining: 26.5s\n",
      "40:\tlearn: 0.1179919\ttotal: 18s\tremaining: 25.9s\n",
      "41:\tlearn: 0.1171380\ttotal: 18.4s\tremaining: 25.4s\n",
      "42:\tlearn: 0.1164257\ttotal: 18.7s\tremaining: 24.8s\n",
      "43:\tlearn: 0.1158260\ttotal: 19.1s\tremaining: 24.3s\n",
      "44:\tlearn: 0.1146793\ttotal: 19.4s\tremaining: 23.7s\n",
      "45:\tlearn: 0.1137875\ttotal: 19.7s\tremaining: 23.2s\n",
      "46:\tlearn: 0.1127347\ttotal: 20.1s\tremaining: 22.7s\n",
      "47:\tlearn: 0.1117577\ttotal: 20.5s\tremaining: 22.2s\n",
      "48:\tlearn: 0.1107566\ttotal: 20.8s\tremaining: 21.7s\n",
      "49:\tlearn: 0.1101740\ttotal: 21.1s\tremaining: 21.1s\n",
      "50:\tlearn: 0.1095377\ttotal: 21.6s\tremaining: 20.7s\n",
      "51:\tlearn: 0.1085198\ttotal: 22.1s\tremaining: 20.4s\n",
      "52:\tlearn: 0.1078824\ttotal: 22.4s\tremaining: 19.9s\n",
      "53:\tlearn: 0.1073425\ttotal: 22.7s\tremaining: 19.3s\n",
      "54:\tlearn: 0.1070201\ttotal: 23s\tremaining: 18.8s\n",
      "55:\tlearn: 0.1065570\ttotal: 23.5s\tremaining: 18.5s\n",
      "56:\tlearn: 0.1060138\ttotal: 24s\tremaining: 18.1s\n",
      "57:\tlearn: 0.1055343\ttotal: 24.4s\tremaining: 17.6s\n",
      "58:\tlearn: 0.1050828\ttotal: 24.7s\tremaining: 17.1s\n",
      "59:\tlearn: 0.1046462\ttotal: 25s\tremaining: 16.7s\n",
      "60:\tlearn: 0.1036576\ttotal: 25.4s\tremaining: 16.2s\n",
      "61:\tlearn: 0.1028191\ttotal: 25.7s\tremaining: 15.7s\n",
      "62:\tlearn: 0.1017897\ttotal: 26.1s\tremaining: 15.3s\n",
      "63:\tlearn: 0.1011749\ttotal: 26.4s\tremaining: 14.9s\n",
      "64:\tlearn: 0.1008222\ttotal: 26.7s\tremaining: 14.4s\n",
      "65:\tlearn: 0.1002851\ttotal: 27s\tremaining: 13.9s\n",
      "66:\tlearn: 0.0990557\ttotal: 27.4s\tremaining: 13.5s\n",
      "67:\tlearn: 0.0985247\ttotal: 28s\tremaining: 13.2s\n",
      "68:\tlearn: 0.0976647\ttotal: 28.5s\tremaining: 12.8s\n",
      "69:\tlearn: 0.0974176\ttotal: 29.4s\tremaining: 12.6s\n",
      "70:\tlearn: 0.0968043\ttotal: 30.2s\tremaining: 12.3s\n",
      "71:\tlearn: 0.0961182\ttotal: 31.1s\tremaining: 12.1s\n",
      "72:\tlearn: 0.0959411\ttotal: 31.6s\tremaining: 11.7s\n",
      "73:\tlearn: 0.0952152\ttotal: 32.9s\tremaining: 11.6s\n",
      "74:\tlearn: 0.0948900\ttotal: 33.5s\tremaining: 11.2s\n",
      "75:\tlearn: 0.0945213\ttotal: 34.1s\tremaining: 10.8s\n",
      "76:\tlearn: 0.0939705\ttotal: 34.5s\tremaining: 10.3s\n",
      "77:\tlearn: 0.0936027\ttotal: 35.1s\tremaining: 9.89s\n",
      "78:\tlearn: 0.0932528\ttotal: 35.8s\tremaining: 9.52s\n",
      "79:\tlearn: 0.0927065\ttotal: 36.2s\tremaining: 9.05s\n",
      "80:\tlearn: 0.0919478\ttotal: 37.1s\tremaining: 8.71s\n",
      "81:\tlearn: 0.0916468\ttotal: 37.6s\tremaining: 8.26s\n",
      "82:\tlearn: 0.0912483\ttotal: 38.2s\tremaining: 7.82s\n",
      "83:\tlearn: 0.0909108\ttotal: 38.7s\tremaining: 7.38s\n",
      "84:\tlearn: 0.0905325\ttotal: 39.1s\tremaining: 6.9s\n",
      "85:\tlearn: 0.0901676\ttotal: 40.1s\tremaining: 6.53s\n",
      "86:\tlearn: 0.0898738\ttotal: 40.9s\tremaining: 6.11s\n",
      "87:\tlearn: 0.0895577\ttotal: 41.4s\tremaining: 5.64s\n",
      "88:\tlearn: 0.0893336\ttotal: 41.8s\tremaining: 5.17s\n",
      "89:\tlearn: 0.0892160\ttotal: 42.3s\tremaining: 4.7s\n",
      "90:\tlearn: 0.0889073\ttotal: 42.9s\tremaining: 4.24s\n",
      "91:\tlearn: 0.0883935\ttotal: 43.2s\tremaining: 3.75s\n",
      "92:\tlearn: 0.0879689\ttotal: 44s\tremaining: 3.31s\n",
      "93:\tlearn: 0.0873093\ttotal: 44.4s\tremaining: 2.83s\n",
      "94:\tlearn: 0.0869628\ttotal: 44.9s\tremaining: 2.36s\n",
      "95:\tlearn: 0.0864189\ttotal: 45.4s\tremaining: 1.89s\n",
      "96:\tlearn: 0.0857891\ttotal: 45.8s\tremaining: 1.42s\n",
      "97:\tlearn: 0.0855082\ttotal: 46.1s\tremaining: 941ms\n",
      "98:\tlearn: 0.0849643\ttotal: 46.9s\tremaining: 473ms\n",
      "99:\tlearn: 0.0845316\ttotal: 47.2s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.989, total=  49.2s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3621251\ttotal: 925ms\tremaining: 1m 31s\n",
      "1:\tlearn: 0.2830097\ttotal: 1.36s\tremaining: 1m 6s\n",
      "2:\tlearn: 0.2290945\ttotal: 1.94s\tremaining: 1m 2s\n",
      "3:\tlearn: 0.2090335\ttotal: 2.4s\tremaining: 57.5s\n",
      "4:\tlearn: 0.1991118\ttotal: 2.92s\tremaining: 55.6s\n",
      "5:\tlearn: 0.1921329\ttotal: 3.51s\tremaining: 55s\n",
      "6:\tlearn: 0.1868779\ttotal: 3.93s\tremaining: 52.2s\n",
      "7:\tlearn: 0.1811223\ttotal: 4.38s\tremaining: 50.4s\n",
      "8:\tlearn: 0.1751952\ttotal: 4.82s\tremaining: 48.7s\n",
      "9:\tlearn: 0.1719664\ttotal: 5.3s\tremaining: 47.8s\n",
      "10:\tlearn: 0.1686002\ttotal: 6.2s\tremaining: 50.2s\n",
      "11:\tlearn: 0.1657147\ttotal: 7.29s\tremaining: 53.5s\n",
      "12:\tlearn: 0.1618047\ttotal: 8.56s\tremaining: 57.3s\n",
      "13:\tlearn: 0.1601897\ttotal: 9.43s\tremaining: 57.9s\n",
      "14:\tlearn: 0.1572360\ttotal: 10.2s\tremaining: 57.8s\n",
      "15:\tlearn: 0.1544518\ttotal: 10.6s\tremaining: 55.9s\n",
      "16:\tlearn: 0.1527989\ttotal: 11.3s\tremaining: 55.1s\n",
      "17:\tlearn: 0.1500211\ttotal: 12.1s\tremaining: 55.3s\n",
      "18:\tlearn: 0.1480632\ttotal: 12.5s\tremaining: 53.3s\n",
      "19:\tlearn: 0.1459592\ttotal: 13.2s\tremaining: 52.9s\n",
      "20:\tlearn: 0.1443766\ttotal: 13.5s\tremaining: 50.9s\n",
      "21:\tlearn: 0.1426860\ttotal: 13.9s\tremaining: 49.1s\n",
      "22:\tlearn: 0.1414172\ttotal: 14.3s\tremaining: 48s\n",
      "23:\tlearn: 0.1396926\ttotal: 15.2s\tremaining: 48.1s\n",
      "24:\tlearn: 0.1388974\ttotal: 15.8s\tremaining: 47.4s\n",
      "25:\tlearn: 0.1376529\ttotal: 16.8s\tremaining: 47.9s\n",
      "26:\tlearn: 0.1362966\ttotal: 17.3s\tremaining: 46.8s\n",
      "27:\tlearn: 0.1351792\ttotal: 17.9s\tremaining: 46s\n",
      "28:\tlearn: 0.1339541\ttotal: 18.4s\tremaining: 45.1s\n",
      "29:\tlearn: 0.1330802\ttotal: 19.2s\tremaining: 44.9s\n",
      "30:\tlearn: 0.1314182\ttotal: 20.1s\tremaining: 44.7s\n",
      "31:\tlearn: 0.1304190\ttotal: 20.8s\tremaining: 44.1s\n",
      "32:\tlearn: 0.1290572\ttotal: 21.5s\tremaining: 43.6s\n",
      "33:\tlearn: 0.1283766\ttotal: 21.9s\tremaining: 42.5s\n",
      "34:\tlearn: 0.1267523\ttotal: 22.6s\tremaining: 42s\n",
      "35:\tlearn: 0.1257622\ttotal: 22.9s\tremaining: 40.7s\n",
      "36:\tlearn: 0.1242987\ttotal: 23.4s\tremaining: 39.8s\n",
      "37:\tlearn: 0.1227398\ttotal: 23.7s\tremaining: 38.7s\n",
      "38:\tlearn: 0.1216105\ttotal: 24.1s\tremaining: 37.6s\n",
      "39:\tlearn: 0.1202915\ttotal: 24.4s\tremaining: 36.6s\n",
      "40:\tlearn: 0.1184477\ttotal: 24.8s\tremaining: 35.6s\n",
      "41:\tlearn: 0.1171836\ttotal: 25.2s\tremaining: 34.7s\n",
      "42:\tlearn: 0.1160328\ttotal: 25.5s\tremaining: 33.8s\n",
      "43:\tlearn: 0.1150726\ttotal: 25.8s\tremaining: 32.9s\n",
      "44:\tlearn: 0.1141477\ttotal: 26.4s\tremaining: 32.3s\n",
      "45:\tlearn: 0.1135710\ttotal: 26.8s\tremaining: 31.4s\n",
      "46:\tlearn: 0.1125988\ttotal: 27.1s\tremaining: 30.6s\n",
      "47:\tlearn: 0.1116239\ttotal: 27.5s\tremaining: 29.8s\n",
      "48:\tlearn: 0.1110373\ttotal: 27.8s\tremaining: 29s\n",
      "49:\tlearn: 0.1095297\ttotal: 28.2s\tremaining: 28.2s\n",
      "50:\tlearn: 0.1086239\ttotal: 28.5s\tremaining: 27.4s\n",
      "51:\tlearn: 0.1081973\ttotal: 28.8s\tremaining: 26.6s\n",
      "52:\tlearn: 0.1074650\ttotal: 29.2s\tremaining: 25.9s\n",
      "53:\tlearn: 0.1071553\ttotal: 29.5s\tremaining: 25.1s\n",
      "54:\tlearn: 0.1064170\ttotal: 29.9s\tremaining: 24.4s\n",
      "55:\tlearn: 0.1058096\ttotal: 30.2s\tremaining: 23.7s\n",
      "56:\tlearn: 0.1050308\ttotal: 30.5s\tremaining: 23s\n",
      "57:\tlearn: 0.1042582\ttotal: 31.1s\tremaining: 22.5s\n",
      "58:\tlearn: 0.1036808\ttotal: 31.4s\tremaining: 21.8s\n",
      "59:\tlearn: 0.1035290\ttotal: 31.7s\tremaining: 21.2s\n",
      "60:\tlearn: 0.1029175\ttotal: 32.1s\tremaining: 20.5s\n",
      "61:\tlearn: 0.1024869\ttotal: 32.4s\tremaining: 19.8s\n",
      "62:\tlearn: 0.1020164\ttotal: 32.7s\tremaining: 19.2s\n",
      "63:\tlearn: 0.1017458\ttotal: 33s\tremaining: 18.5s\n",
      "64:\tlearn: 0.1010882\ttotal: 33.4s\tremaining: 18s\n",
      "65:\tlearn: 0.1007718\ttotal: 33.7s\tremaining: 17.4s\n",
      "66:\tlearn: 0.0998878\ttotal: 34s\tremaining: 16.8s\n",
      "67:\tlearn: 0.0990658\ttotal: 34.4s\tremaining: 16.2s\n",
      "68:\tlearn: 0.0985692\ttotal: 34.7s\tremaining: 15.6s\n",
      "69:\tlearn: 0.0978161\ttotal: 35s\tremaining: 15s\n",
      "70:\tlearn: 0.0973203\ttotal: 35.3s\tremaining: 14.4s\n",
      "71:\tlearn: 0.0969122\ttotal: 36.1s\tremaining: 14s\n",
      "72:\tlearn: 0.0963464\ttotal: 36.4s\tremaining: 13.5s\n",
      "73:\tlearn: 0.0959250\ttotal: 36.8s\tremaining: 12.9s\n",
      "74:\tlearn: 0.0954040\ttotal: 37.1s\tremaining: 12.4s\n",
      "75:\tlearn: 0.0948654\ttotal: 37.4s\tremaining: 11.8s\n",
      "76:\tlearn: 0.0944758\ttotal: 37.9s\tremaining: 11.3s\n",
      "77:\tlearn: 0.0940588\ttotal: 38.3s\tremaining: 10.8s\n",
      "78:\tlearn: 0.0938577\ttotal: 38.5s\tremaining: 10.2s\n",
      "79:\tlearn: 0.0932696\ttotal: 38.8s\tremaining: 9.71s\n",
      "80:\tlearn: 0.0930046\ttotal: 39.1s\tremaining: 9.18s\n",
      "81:\tlearn: 0.0923620\ttotal: 39.5s\tremaining: 8.66s\n",
      "82:\tlearn: 0.0919764\ttotal: 39.8s\tremaining: 8.16s\n",
      "83:\tlearn: 0.0915240\ttotal: 40.1s\tremaining: 7.65s\n",
      "84:\tlearn: 0.0909461\ttotal: 40.5s\tremaining: 7.14s\n",
      "85:\tlearn: 0.0905661\ttotal: 41s\tremaining: 6.67s\n",
      "86:\tlearn: 0.0899540\ttotal: 41.3s\tremaining: 6.17s\n",
      "87:\tlearn: 0.0896305\ttotal: 41.6s\tremaining: 5.67s\n",
      "88:\tlearn: 0.0892278\ttotal: 41.9s\tremaining: 5.18s\n",
      "89:\tlearn: 0.0889134\ttotal: 42.3s\tremaining: 4.7s\n",
      "90:\tlearn: 0.0882127\ttotal: 42.6s\tremaining: 4.22s\n",
      "91:\tlearn: 0.0880113\ttotal: 42.9s\tremaining: 3.73s\n",
      "92:\tlearn: 0.0875163\ttotal: 43.2s\tremaining: 3.25s\n",
      "93:\tlearn: 0.0870504\ttotal: 43.5s\tremaining: 2.78s\n",
      "94:\tlearn: 0.0865866\ttotal: 43.9s\tremaining: 2.31s\n",
      "95:\tlearn: 0.0862639\ttotal: 44.2s\tremaining: 1.84s\n",
      "96:\tlearn: 0.0859821\ttotal: 44.6s\tremaining: 1.38s\n",
      "97:\tlearn: 0.0855764\ttotal: 44.9s\tremaining: 916ms\n",
      "98:\tlearn: 0.0853019\ttotal: 45.2s\tremaining: 457ms\n",
      "99:\tlearn: 0.0848032\ttotal: 45.5s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.989, total=  47.5s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  6.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3529298\ttotal: 539ms\tremaining: 53.4s\n",
      "1:\tlearn: 0.2780151\ttotal: 880ms\tremaining: 43.1s\n",
      "2:\tlearn: 0.2375566\ttotal: 1.26s\tremaining: 40.7s\n",
      "3:\tlearn: 0.2195839\ttotal: 1.61s\tremaining: 38.8s\n",
      "4:\tlearn: 0.2079817\ttotal: 2.03s\tremaining: 38.7s\n",
      "5:\tlearn: 0.1937233\ttotal: 2.39s\tremaining: 37.4s\n",
      "6:\tlearn: 0.1871217\ttotal: 2.73s\tremaining: 36.2s\n",
      "7:\tlearn: 0.1806085\ttotal: 3.07s\tremaining: 35.4s\n",
      "8:\tlearn: 0.1765390\ttotal: 3.37s\tremaining: 34.1s\n",
      "9:\tlearn: 0.1732171\ttotal: 3.97s\tremaining: 35.7s\n",
      "10:\tlearn: 0.1706462\ttotal: 4.36s\tremaining: 35.3s\n",
      "11:\tlearn: 0.1684389\ttotal: 4.72s\tremaining: 34.6s\n",
      "12:\tlearn: 0.1655716\ttotal: 5.04s\tremaining: 33.7s\n",
      "13:\tlearn: 0.1622482\ttotal: 5.33s\tremaining: 32.7s\n",
      "14:\tlearn: 0.1597387\ttotal: 5.67s\tremaining: 32.2s\n",
      "15:\tlearn: 0.1570448\ttotal: 6s\tremaining: 31.5s\n",
      "16:\tlearn: 0.1543857\ttotal: 6.3s\tremaining: 30.7s\n",
      "17:\tlearn: 0.1530646\ttotal: 6.62s\tremaining: 30.2s\n",
      "18:\tlearn: 0.1513738\ttotal: 6.94s\tremaining: 29.6s\n",
      "19:\tlearn: 0.1498336\ttotal: 7.26s\tremaining: 29.1s\n",
      "20:\tlearn: 0.1470594\ttotal: 7.6s\tremaining: 28.6s\n",
      "21:\tlearn: 0.1460457\ttotal: 7.95s\tremaining: 28.2s\n",
      "22:\tlearn: 0.1449190\ttotal: 8.27s\tremaining: 27.7s\n",
      "23:\tlearn: 0.1427920\ttotal: 8.81s\tremaining: 27.9s\n",
      "24:\tlearn: 0.1414669\ttotal: 9.16s\tremaining: 27.5s\n",
      "25:\tlearn: 0.1402347\ttotal: 9.46s\tremaining: 26.9s\n",
      "26:\tlearn: 0.1387583\ttotal: 9.77s\tremaining: 26.4s\n",
      "27:\tlearn: 0.1367178\ttotal: 10.1s\tremaining: 26s\n",
      "28:\tlearn: 0.1346056\ttotal: 10.5s\tremaining: 25.6s\n",
      "29:\tlearn: 0.1337512\ttotal: 10.7s\tremaining: 25s\n",
      "30:\tlearn: 0.1323252\ttotal: 11s\tremaining: 24.6s\n",
      "31:\tlearn: 0.1307456\ttotal: 11.4s\tremaining: 24.3s\n",
      "32:\tlearn: 0.1293370\ttotal: 11.8s\tremaining: 23.9s\n",
      "33:\tlearn: 0.1286362\ttotal: 12.2s\tremaining: 23.7s\n",
      "34:\tlearn: 0.1260128\ttotal: 12.6s\tremaining: 23.4s\n",
      "35:\tlearn: 0.1252947\ttotal: 13s\tremaining: 23.1s\n",
      "36:\tlearn: 0.1245239\ttotal: 13.6s\tremaining: 23.2s\n",
      "37:\tlearn: 0.1235939\ttotal: 14.1s\tremaining: 22.9s\n",
      "38:\tlearn: 0.1227150\ttotal: 14.4s\tremaining: 22.5s\n",
      "39:\tlearn: 0.1215925\ttotal: 14.8s\tremaining: 22.1s\n",
      "40:\tlearn: 0.1201457\ttotal: 15.1s\tremaining: 21.8s\n",
      "41:\tlearn: 0.1188319\ttotal: 15.5s\tremaining: 21.5s\n",
      "42:\tlearn: 0.1179105\ttotal: 15.9s\tremaining: 21.1s\n",
      "43:\tlearn: 0.1172938\ttotal: 16.2s\tremaining: 20.6s\n",
      "44:\tlearn: 0.1159925\ttotal: 16.6s\tremaining: 20.2s\n",
      "45:\tlearn: 0.1150910\ttotal: 16.9s\tremaining: 19.8s\n",
      "46:\tlearn: 0.1141455\ttotal: 17.2s\tremaining: 19.4s\n",
      "47:\tlearn: 0.1135994\ttotal: 17.5s\tremaining: 19s\n",
      "48:\tlearn: 0.1130798\ttotal: 17.9s\tremaining: 18.6s\n",
      "49:\tlearn: 0.1125988\ttotal: 18.2s\tremaining: 18.2s\n",
      "50:\tlearn: 0.1118112\ttotal: 18.7s\tremaining: 18s\n",
      "51:\tlearn: 0.1111221\ttotal: 19.3s\tremaining: 17.8s\n",
      "52:\tlearn: 0.1102948\ttotal: 19.7s\tremaining: 17.5s\n",
      "53:\tlearn: 0.1093405\ttotal: 20s\tremaining: 17.1s\n",
      "54:\tlearn: 0.1087554\ttotal: 20.4s\tremaining: 16.6s\n",
      "55:\tlearn: 0.1081897\ttotal: 20.7s\tremaining: 16.3s\n",
      "56:\tlearn: 0.1074171\ttotal: 21s\tremaining: 15.8s\n",
      "57:\tlearn: 0.1070095\ttotal: 21.3s\tremaining: 15.4s\n",
      "58:\tlearn: 0.1062738\ttotal: 21.7s\tremaining: 15.1s\n",
      "59:\tlearn: 0.1052891\ttotal: 22s\tremaining: 14.7s\n",
      "60:\tlearn: 0.1048314\ttotal: 22.3s\tremaining: 14.3s\n",
      "61:\tlearn: 0.1043976\ttotal: 22.7s\tremaining: 13.9s\n",
      "62:\tlearn: 0.1031365\ttotal: 23s\tremaining: 13.5s\n",
      "63:\tlearn: 0.1023630\ttotal: 23.4s\tremaining: 13.1s\n",
      "64:\tlearn: 0.1016245\ttotal: 24s\tremaining: 12.9s\n",
      "65:\tlearn: 0.1012806\ttotal: 24.3s\tremaining: 12.5s\n",
      "66:\tlearn: 0.1004519\ttotal: 24.7s\tremaining: 12.1s\n",
      "67:\tlearn: 0.0993827\ttotal: 25s\tremaining: 11.8s\n",
      "68:\tlearn: 0.0987866\ttotal: 25.3s\tremaining: 11.4s\n",
      "69:\tlearn: 0.0984452\ttotal: 25.7s\tremaining: 11s\n",
      "70:\tlearn: 0.0979516\ttotal: 26s\tremaining: 10.6s\n",
      "71:\tlearn: 0.0975262\ttotal: 26.3s\tremaining: 10.2s\n",
      "72:\tlearn: 0.0969029\ttotal: 26.7s\tremaining: 9.86s\n",
      "73:\tlearn: 0.0964413\ttotal: 27s\tremaining: 9.48s\n",
      "74:\tlearn: 0.0955154\ttotal: 27.3s\tremaining: 9.11s\n",
      "75:\tlearn: 0.0953274\ttotal: 27.6s\tremaining: 8.73s\n",
      "76:\tlearn: 0.0952272\ttotal: 27.9s\tremaining: 8.35s\n",
      "77:\tlearn: 0.0947658\ttotal: 28.3s\tremaining: 7.97s\n",
      "78:\tlearn: 0.0939915\ttotal: 28.9s\tremaining: 7.67s\n",
      "79:\tlearn: 0.0934557\ttotal: 29.2s\tremaining: 7.3s\n",
      "80:\tlearn: 0.0931390\ttotal: 29.5s\tremaining: 6.93s\n",
      "81:\tlearn: 0.0928498\ttotal: 29.8s\tremaining: 6.54s\n",
      "82:\tlearn: 0.0923416\ttotal: 30.2s\tremaining: 6.19s\n",
      "83:\tlearn: 0.0918080\ttotal: 30.6s\tremaining: 5.83s\n",
      "84:\tlearn: 0.0917163\ttotal: 30.9s\tremaining: 5.46s\n",
      "85:\tlearn: 0.0913259\ttotal: 31.2s\tremaining: 5.08s\n",
      "86:\tlearn: 0.0907130\ttotal: 31.6s\tremaining: 4.72s\n",
      "87:\tlearn: 0.0901975\ttotal: 31.9s\tremaining: 4.35s\n",
      "88:\tlearn: 0.0894145\ttotal: 32.3s\tremaining: 3.99s\n",
      "89:\tlearn: 0.0890013\ttotal: 32.6s\tremaining: 3.62s\n",
      "90:\tlearn: 0.0887762\ttotal: 32.9s\tremaining: 3.25s\n",
      "91:\tlearn: 0.0884054\ttotal: 33.2s\tremaining: 2.89s\n",
      "92:\tlearn: 0.0880302\ttotal: 33.8s\tremaining: 2.54s\n",
      "93:\tlearn: 0.0875839\ttotal: 34.1s\tremaining: 2.18s\n",
      "94:\tlearn: 0.0871696\ttotal: 34.5s\tremaining: 1.81s\n",
      "95:\tlearn: 0.0864556\ttotal: 34.8s\tremaining: 1.45s\n",
      "96:\tlearn: 0.0861195\ttotal: 35.2s\tremaining: 1.09s\n",
      "97:\tlearn: 0.0859912\ttotal: 35.5s\tremaining: 725ms\n",
      "98:\tlearn: 0.0856954\ttotal: 35.8s\tremaining: 362ms\n",
      "99:\tlearn: 0.0853498\ttotal: 36.2s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.989, total=  37.8s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  6.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3457547\ttotal: 509ms\tremaining: 50.3s\n",
      "1:\tlearn: 0.2693729\ttotal: 822ms\tremaining: 40.3s\n",
      "2:\tlearn: 0.2342542\ttotal: 1.29s\tremaining: 41.7s\n",
      "3:\tlearn: 0.2182791\ttotal: 1.68s\tremaining: 40.4s\n",
      "4:\tlearn: 0.2050912\ttotal: 2.09s\tremaining: 39.7s\n",
      "5:\tlearn: 0.1942998\ttotal: 2.56s\tremaining: 40.1s\n",
      "6:\tlearn: 0.1877297\ttotal: 2.91s\tremaining: 38.6s\n",
      "7:\tlearn: 0.1828917\ttotal: 3.24s\tremaining: 37.2s\n",
      "8:\tlearn: 0.1764570\ttotal: 3.72s\tremaining: 37.6s\n",
      "9:\tlearn: 0.1721857\ttotal: 4.11s\tremaining: 37s\n",
      "10:\tlearn: 0.1655971\ttotal: 4.5s\tremaining: 36.4s\n",
      "11:\tlearn: 0.1619036\ttotal: 4.81s\tremaining: 35.3s\n",
      "12:\tlearn: 0.1601400\ttotal: 5.11s\tremaining: 34.2s\n",
      "13:\tlearn: 0.1581722\ttotal: 5.4s\tremaining: 33.2s\n",
      "14:\tlearn: 0.1565986\ttotal: 5.76s\tremaining: 32.7s\n",
      "15:\tlearn: 0.1542977\ttotal: 6.15s\tremaining: 32.3s\n",
      "16:\tlearn: 0.1527964\ttotal: 6.55s\tremaining: 32s\n",
      "17:\tlearn: 0.1502785\ttotal: 6.8s\tremaining: 31s\n",
      "18:\tlearn: 0.1481050\ttotal: 7.13s\tremaining: 30.4s\n",
      "19:\tlearn: 0.1464941\ttotal: 7.72s\tremaining: 30.9s\n",
      "20:\tlearn: 0.1447314\ttotal: 8.21s\tremaining: 30.9s\n",
      "21:\tlearn: 0.1430733\ttotal: 9.11s\tremaining: 32.3s\n",
      "22:\tlearn: 0.1415409\ttotal: 9.53s\tremaining: 31.9s\n",
      "23:\tlearn: 0.1406588\ttotal: 9.83s\tremaining: 31.1s\n",
      "24:\tlearn: 0.1380415\ttotal: 10.2s\tremaining: 30.7s\n",
      "25:\tlearn: 0.1368177\ttotal: 10.6s\tremaining: 30.2s\n",
      "26:\tlearn: 0.1353648\ttotal: 11s\tremaining: 29.8s\n",
      "27:\tlearn: 0.1341348\ttotal: 11.4s\tremaining: 29.3s\n",
      "28:\tlearn: 0.1330133\ttotal: 11.7s\tremaining: 28.8s\n",
      "29:\tlearn: 0.1319263\ttotal: 12.1s\tremaining: 28.2s\n",
      "30:\tlearn: 0.1305109\ttotal: 12.6s\tremaining: 28.1s\n",
      "31:\tlearn: 0.1294069\ttotal: 13.1s\tremaining: 27.7s\n",
      "32:\tlearn: 0.1284469\ttotal: 13.5s\tremaining: 27.4s\n",
      "33:\tlearn: 0.1272426\ttotal: 14.2s\tremaining: 27.5s\n",
      "34:\tlearn: 0.1262539\ttotal: 14.6s\tremaining: 27.2s\n",
      "35:\tlearn: 0.1247680\ttotal: 15.3s\tremaining: 27.2s\n",
      "36:\tlearn: 0.1242718\ttotal: 15.7s\tremaining: 26.7s\n",
      "37:\tlearn: 0.1231508\ttotal: 16.1s\tremaining: 26.3s\n",
      "38:\tlearn: 0.1225204\ttotal: 16.6s\tremaining: 25.9s\n",
      "39:\tlearn: 0.1215656\ttotal: 17s\tremaining: 25.5s\n",
      "40:\tlearn: 0.1206643\ttotal: 17.3s\tremaining: 24.9s\n",
      "41:\tlearn: 0.1189873\ttotal: 17.8s\tremaining: 24.5s\n",
      "42:\tlearn: 0.1182461\ttotal: 18.5s\tremaining: 24.6s\n",
      "43:\tlearn: 0.1176563\ttotal: 18.8s\tremaining: 24s\n",
      "44:\tlearn: 0.1165939\ttotal: 19.3s\tremaining: 23.6s\n",
      "45:\tlearn: 0.1160731\ttotal: 19.7s\tremaining: 23.1s\n",
      "46:\tlearn: 0.1152177\ttotal: 20.1s\tremaining: 22.6s\n",
      "47:\tlearn: 0.1141480\ttotal: 20.5s\tremaining: 22.2s\n",
      "48:\tlearn: 0.1130610\ttotal: 20.8s\tremaining: 21.7s\n",
      "49:\tlearn: 0.1121214\ttotal: 21.3s\tremaining: 21.3s\n",
      "50:\tlearn: 0.1110565\ttotal: 21.6s\tremaining: 20.8s\n",
      "51:\tlearn: 0.1106760\ttotal: 22s\tremaining: 20.3s\n",
      "52:\tlearn: 0.1100659\ttotal: 22.4s\tremaining: 19.8s\n",
      "53:\tlearn: 0.1087705\ttotal: 23.1s\tremaining: 19.6s\n",
      "54:\tlearn: 0.1075194\ttotal: 23.5s\tremaining: 19.2s\n",
      "55:\tlearn: 0.1069228\ttotal: 23.8s\tremaining: 18.7s\n",
      "56:\tlearn: 0.1063647\ttotal: 24.3s\tremaining: 18.3s\n",
      "57:\tlearn: 0.1056133\ttotal: 24.6s\tremaining: 17.8s\n",
      "58:\tlearn: 0.1051596\ttotal: 25s\tremaining: 17.4s\n",
      "59:\tlearn: 0.1045072\ttotal: 25.5s\tremaining: 17s\n",
      "60:\tlearn: 0.1036731\ttotal: 25.8s\tremaining: 16.5s\n",
      "61:\tlearn: 0.1032136\ttotal: 26.2s\tremaining: 16s\n",
      "62:\tlearn: 0.1023631\ttotal: 26.5s\tremaining: 15.6s\n",
      "63:\tlearn: 0.1018036\ttotal: 27s\tremaining: 15.2s\n",
      "64:\tlearn: 0.1008210\ttotal: 27.7s\tremaining: 14.9s\n",
      "65:\tlearn: 0.1006133\ttotal: 28.1s\tremaining: 14.5s\n",
      "66:\tlearn: 0.0997819\ttotal: 28.5s\tremaining: 14s\n",
      "67:\tlearn: 0.0993925\ttotal: 28.8s\tremaining: 13.5s\n",
      "68:\tlearn: 0.0989781\ttotal: 29.1s\tremaining: 13.1s\n",
      "69:\tlearn: 0.0983289\ttotal: 29.5s\tremaining: 12.7s\n",
      "70:\tlearn: 0.0979500\ttotal: 29.8s\tremaining: 12.2s\n",
      "71:\tlearn: 0.0971088\ttotal: 30.2s\tremaining: 11.7s\n",
      "72:\tlearn: 0.0968359\ttotal: 30.7s\tremaining: 11.3s\n",
      "73:\tlearn: 0.0964637\ttotal: 31.1s\tremaining: 10.9s\n",
      "74:\tlearn: 0.0958168\ttotal: 31.4s\tremaining: 10.5s\n",
      "75:\tlearn: 0.0956970\ttotal: 31.7s\tremaining: 10s\n",
      "76:\tlearn: 0.0951192\ttotal: 32.1s\tremaining: 9.6s\n",
      "77:\tlearn: 0.0946608\ttotal: 32.7s\tremaining: 9.23s\n",
      "78:\tlearn: 0.0940421\ttotal: 33.3s\tremaining: 8.85s\n",
      "79:\tlearn: 0.0936678\ttotal: 33.7s\tremaining: 8.43s\n",
      "80:\tlearn: 0.0932195\ttotal: 34.1s\tremaining: 8s\n",
      "81:\tlearn: 0.0926671\ttotal: 34.4s\tremaining: 7.56s\n",
      "82:\tlearn: 0.0923816\ttotal: 34.8s\tremaining: 7.12s\n",
      "83:\tlearn: 0.0915819\ttotal: 35.1s\tremaining: 6.69s\n",
      "84:\tlearn: 0.0908075\ttotal: 35.5s\tremaining: 6.27s\n",
      "85:\tlearn: 0.0903942\ttotal: 35.9s\tremaining: 5.84s\n",
      "86:\tlearn: 0.0898351\ttotal: 36.3s\tremaining: 5.42s\n",
      "87:\tlearn: 0.0897354\ttotal: 36.7s\tremaining: 5s\n",
      "88:\tlearn: 0.0891760\ttotal: 37.1s\tremaining: 4.58s\n",
      "89:\tlearn: 0.0889675\ttotal: 37.4s\tremaining: 4.15s\n",
      "90:\tlearn: 0.0884297\ttotal: 38.1s\tremaining: 3.76s\n",
      "91:\tlearn: 0.0881978\ttotal: 38.4s\tremaining: 3.34s\n",
      "92:\tlearn: 0.0877352\ttotal: 38.7s\tremaining: 2.91s\n",
      "93:\tlearn: 0.0873131\ttotal: 39.1s\tremaining: 2.5s\n",
      "94:\tlearn: 0.0866194\ttotal: 39.6s\tremaining: 2.08s\n",
      "95:\tlearn: 0.0862889\ttotal: 39.9s\tremaining: 1.66s\n",
      "96:\tlearn: 0.0857838\ttotal: 40.3s\tremaining: 1.25s\n",
      "97:\tlearn: 0.0854781\ttotal: 40.6s\tremaining: 829ms\n",
      "98:\tlearn: 0.0853570\ttotal: 41s\tremaining: 414ms\n",
      "99:\tlearn: 0.0849308\ttotal: 41.3s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.990, total=  45.0s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  7.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3540666\ttotal: 518ms\tremaining: 51.3s\n",
      "1:\tlearn: 0.2688633\ttotal: 960ms\tremaining: 47s\n",
      "2:\tlearn: 0.2368169\ttotal: 1.35s\tremaining: 43.6s\n",
      "3:\tlearn: 0.2191351\ttotal: 1.65s\tremaining: 39.7s\n",
      "4:\tlearn: 0.2093813\ttotal: 2.03s\tremaining: 38.6s\n",
      "5:\tlearn: 0.2010497\ttotal: 2.41s\tremaining: 37.7s\n",
      "6:\tlearn: 0.1931983\ttotal: 2.79s\tremaining: 37.1s\n",
      "7:\tlearn: 0.1877752\ttotal: 3.12s\tremaining: 35.9s\n",
      "8:\tlearn: 0.1836112\ttotal: 3.46s\tremaining: 35s\n",
      "9:\tlearn: 0.1756548\ttotal: 3.87s\tremaining: 34.9s\n",
      "10:\tlearn: 0.1707218\ttotal: 4.2s\tremaining: 34s\n",
      "11:\tlearn: 0.1671451\ttotal: 4.81s\tremaining: 35.3s\n",
      "12:\tlearn: 0.1608846\ttotal: 5.21s\tremaining: 34.9s\n",
      "13:\tlearn: 0.1587348\ttotal: 5.55s\tremaining: 34.1s\n",
      "14:\tlearn: 0.1568652\ttotal: 5.91s\tremaining: 33.5s\n",
      "15:\tlearn: 0.1553256\ttotal: 6.42s\tremaining: 33.7s\n",
      "16:\tlearn: 0.1529397\ttotal: 6.73s\tremaining: 32.9s\n",
      "17:\tlearn: 0.1502055\ttotal: 7.08s\tremaining: 32.3s\n",
      "18:\tlearn: 0.1489004\ttotal: 7.43s\tremaining: 31.7s\n",
      "19:\tlearn: 0.1473301\ttotal: 7.75s\tremaining: 31s\n",
      "20:\tlearn: 0.1458705\ttotal: 8.1s\tremaining: 30.5s\n",
      "21:\tlearn: 0.1451462\ttotal: 8.46s\tremaining: 30s\n",
      "22:\tlearn: 0.1437719\ttotal: 8.76s\tremaining: 29.3s\n",
      "23:\tlearn: 0.1425290\ttotal: 9.1s\tremaining: 28.8s\n",
      "24:\tlearn: 0.1410738\ttotal: 9.6s\tremaining: 28.8s\n",
      "25:\tlearn: 0.1396543\ttotal: 9.97s\tremaining: 28.4s\n",
      "26:\tlearn: 0.1382384\ttotal: 10.5s\tremaining: 28.3s\n",
      "27:\tlearn: 0.1365184\ttotal: 11s\tremaining: 28.3s\n",
      "28:\tlearn: 0.1351253\ttotal: 11.4s\tremaining: 27.8s\n",
      "29:\tlearn: 0.1339781\ttotal: 11.6s\tremaining: 27.2s\n",
      "30:\tlearn: 0.1327038\ttotal: 12.1s\tremaining: 26.8s\n",
      "31:\tlearn: 0.1310229\ttotal: 12.4s\tremaining: 26.4s\n",
      "32:\tlearn: 0.1298958\ttotal: 12.8s\tremaining: 25.9s\n",
      "33:\tlearn: 0.1277542\ttotal: 13.2s\tremaining: 25.7s\n",
      "34:\tlearn: 0.1269149\ttotal: 13.5s\tremaining: 25.1s\n",
      "35:\tlearn: 0.1259460\ttotal: 13.9s\tremaining: 24.7s\n",
      "36:\tlearn: 0.1250419\ttotal: 14.2s\tremaining: 24.3s\n",
      "37:\tlearn: 0.1240925\ttotal: 14.9s\tremaining: 24.2s\n",
      "38:\tlearn: 0.1230500\ttotal: 15.2s\tremaining: 23.7s\n",
      "39:\tlearn: 0.1223271\ttotal: 15.5s\tremaining: 23.3s\n",
      "40:\tlearn: 0.1207537\ttotal: 15.9s\tremaining: 22.8s\n",
      "41:\tlearn: 0.1195334\ttotal: 16.3s\tremaining: 22.5s\n",
      "42:\tlearn: 0.1186622\ttotal: 16.6s\tremaining: 22s\n",
      "43:\tlearn: 0.1177217\ttotal: 17s\tremaining: 21.7s\n",
      "44:\tlearn: 0.1160782\ttotal: 17.4s\tremaining: 21.3s\n",
      "45:\tlearn: 0.1152780\ttotal: 17.7s\tremaining: 20.8s\n",
      "46:\tlearn: 0.1145483\ttotal: 18.1s\tremaining: 20.4s\n",
      "47:\tlearn: 0.1135596\ttotal: 18.4s\tremaining: 20s\n",
      "48:\tlearn: 0.1125511\ttotal: 18.8s\tremaining: 19.6s\n",
      "49:\tlearn: 0.1116213\ttotal: 19.1s\tremaining: 19.1s\n",
      "50:\tlearn: 0.1109148\ttotal: 19.5s\tremaining: 18.7s\n",
      "51:\tlearn: 0.1103863\ttotal: 19.8s\tremaining: 18.3s\n",
      "52:\tlearn: 0.1096704\ttotal: 20.2s\tremaining: 17.9s\n",
      "53:\tlearn: 0.1086014\ttotal: 20.5s\tremaining: 17.5s\n",
      "54:\tlearn: 0.1081667\ttotal: 20.8s\tremaining: 17s\n",
      "55:\tlearn: 0.1077321\ttotal: 21.2s\tremaining: 16.6s\n",
      "56:\tlearn: 0.1071686\ttotal: 21.5s\tremaining: 16.2s\n",
      "57:\tlearn: 0.1068976\ttotal: 22s\tremaining: 15.9s\n",
      "58:\tlearn: 0.1063658\ttotal: 22.4s\tremaining: 15.6s\n",
      "59:\tlearn: 0.1055225\ttotal: 22.8s\tremaining: 15.2s\n",
      "60:\tlearn: 0.1051062\ttotal: 23.1s\tremaining: 14.8s\n",
      "61:\tlearn: 0.1041290\ttotal: 23.5s\tremaining: 14.4s\n",
      "62:\tlearn: 0.1035745\ttotal: 23.8s\tremaining: 14s\n",
      "63:\tlearn: 0.1030609\ttotal: 24.1s\tremaining: 13.6s\n",
      "64:\tlearn: 0.1018041\ttotal: 24.5s\tremaining: 13.2s\n",
      "65:\tlearn: 0.1010177\ttotal: 24.9s\tremaining: 12.8s\n",
      "66:\tlearn: 0.1003708\ttotal: 25.2s\tremaining: 12.4s\n",
      "67:\tlearn: 0.0991719\ttotal: 25.6s\tremaining: 12s\n",
      "68:\tlearn: 0.0985870\ttotal: 25.9s\tremaining: 11.7s\n",
      "69:\tlearn: 0.0981743\ttotal: 26.6s\tremaining: 11.4s\n",
      "70:\tlearn: 0.0977346\ttotal: 27s\tremaining: 11s\n",
      "71:\tlearn: 0.0973477\ttotal: 27.4s\tremaining: 10.6s\n",
      "72:\tlearn: 0.0970057\ttotal: 27.6s\tremaining: 10.2s\n",
      "73:\tlearn: 0.0966640\ttotal: 28.1s\tremaining: 9.86s\n",
      "74:\tlearn: 0.0961166\ttotal: 28.4s\tremaining: 9.47s\n",
      "75:\tlearn: 0.0954119\ttotal: 28.8s\tremaining: 9.08s\n",
      "76:\tlearn: 0.0951230\ttotal: 29.1s\tremaining: 8.69s\n",
      "77:\tlearn: 0.0949281\ttotal: 29.4s\tremaining: 8.3s\n",
      "78:\tlearn: 0.0945083\ttotal: 29.8s\tremaining: 7.92s\n",
      "79:\tlearn: 0.0941763\ttotal: 30.3s\tremaining: 7.57s\n",
      "80:\tlearn: 0.0937860\ttotal: 30.6s\tremaining: 7.17s\n",
      "81:\tlearn: 0.0934460\ttotal: 30.9s\tremaining: 6.79s\n",
      "82:\tlearn: 0.0929079\ttotal: 31.3s\tremaining: 6.42s\n",
      "83:\tlearn: 0.0922685\ttotal: 32s\tremaining: 6.1s\n",
      "84:\tlearn: 0.0914666\ttotal: 32.4s\tremaining: 5.72s\n",
      "85:\tlearn: 0.0908681\ttotal: 32.8s\tremaining: 5.34s\n",
      "86:\tlearn: 0.0905841\ttotal: 33.1s\tremaining: 4.94s\n",
      "87:\tlearn: 0.0898232\ttotal: 33.5s\tremaining: 4.56s\n",
      "88:\tlearn: 0.0894986\ttotal: 33.9s\tremaining: 4.18s\n",
      "89:\tlearn: 0.0890381\ttotal: 34.2s\tremaining: 3.8s\n",
      "90:\tlearn: 0.0886412\ttotal: 34.5s\tremaining: 3.42s\n",
      "91:\tlearn: 0.0882548\ttotal: 34.9s\tremaining: 3.03s\n",
      "92:\tlearn: 0.0874219\ttotal: 35.2s\tremaining: 2.65s\n",
      "93:\tlearn: 0.0870826\ttotal: 35.6s\tremaining: 2.27s\n",
      "94:\tlearn: 0.0864437\ttotal: 36s\tremaining: 1.89s\n",
      "95:\tlearn: 0.0858567\ttotal: 36.6s\tremaining: 1.52s\n",
      "96:\tlearn: 0.0856528\ttotal: 36.9s\tremaining: 1.14s\n",
      "97:\tlearn: 0.0853123\ttotal: 37.4s\tremaining: 763ms\n",
      "98:\tlearn: 0.0848186\ttotal: 37.7s\tremaining: 381ms\n",
      "99:\tlearn: 0.0846945\ttotal: 38s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.989, total=  39.9s\n",
      "[CV] subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  8.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3445064\ttotal: 506ms\tremaining: 50.1s\n",
      "1:\tlearn: 0.2722273\ttotal: 891ms\tremaining: 43.7s\n",
      "2:\tlearn: 0.2282333\ttotal: 1.28s\tremaining: 41.5s\n",
      "3:\tlearn: 0.2118496\ttotal: 1.76s\tremaining: 42.3s\n",
      "4:\tlearn: 0.2004711\ttotal: 2.27s\tremaining: 43.2s\n",
      "5:\tlearn: 0.1937070\ttotal: 2.6s\tremaining: 40.7s\n",
      "6:\tlearn: 0.1813295\ttotal: 2.97s\tremaining: 39.5s\n",
      "7:\tlearn: 0.1762894\ttotal: 3.31s\tremaining: 38.1s\n",
      "8:\tlearn: 0.1724754\ttotal: 3.65s\tremaining: 36.9s\n",
      "9:\tlearn: 0.1687139\ttotal: 3.96s\tremaining: 35.7s\n",
      "10:\tlearn: 0.1657851\ttotal: 4.3s\tremaining: 34.8s\n",
      "11:\tlearn: 0.1633152\ttotal: 4.64s\tremaining: 34s\n",
      "12:\tlearn: 0.1597993\ttotal: 4.96s\tremaining: 33.2s\n",
      "13:\tlearn: 0.1571714\ttotal: 5.51s\tremaining: 33.8s\n",
      "14:\tlearn: 0.1555592\ttotal: 5.82s\tremaining: 33s\n",
      "15:\tlearn: 0.1527024\ttotal: 6.17s\tremaining: 32.4s\n",
      "16:\tlearn: 0.1513730\ttotal: 6.49s\tremaining: 31.7s\n",
      "17:\tlearn: 0.1502105\ttotal: 7.01s\tremaining: 32s\n",
      "18:\tlearn: 0.1484275\ttotal: 7.38s\tremaining: 31.5s\n",
      "19:\tlearn: 0.1468319\ttotal: 7.75s\tremaining: 31s\n",
      "20:\tlearn: 0.1448580\ttotal: 8.07s\tremaining: 30.3s\n",
      "21:\tlearn: 0.1432133\ttotal: 8.42s\tremaining: 29.9s\n",
      "22:\tlearn: 0.1416357\ttotal: 8.81s\tremaining: 29.5s\n",
      "23:\tlearn: 0.1403606\ttotal: 9.19s\tremaining: 29.1s\n",
      "24:\tlearn: 0.1391654\ttotal: 9.52s\tremaining: 28.6s\n",
      "25:\tlearn: 0.1378040\ttotal: 9.94s\tremaining: 28.3s\n",
      "26:\tlearn: 0.1366657\ttotal: 10.4s\tremaining: 28s\n",
      "27:\tlearn: 0.1347232\ttotal: 10.7s\tremaining: 27.6s\n",
      "28:\tlearn: 0.1333615\ttotal: 11s\tremaining: 27s\n",
      "29:\tlearn: 0.1321345\ttotal: 11.4s\tremaining: 26.6s\n",
      "30:\tlearn: 0.1306695\ttotal: 11.9s\tremaining: 26.6s\n",
      "31:\tlearn: 0.1291138\ttotal: 12.4s\tremaining: 26.3s\n",
      "32:\tlearn: 0.1276249\ttotal: 12.8s\tremaining: 26s\n",
      "33:\tlearn: 0.1264534\ttotal: 13.1s\tremaining: 25.5s\n",
      "34:\tlearn: 0.1242302\ttotal: 13.5s\tremaining: 25.1s\n",
      "35:\tlearn: 0.1228042\ttotal: 13.9s\tremaining: 24.7s\n",
      "36:\tlearn: 0.1220793\ttotal: 14.2s\tremaining: 24.2s\n",
      "37:\tlearn: 0.1209499\ttotal: 14.6s\tremaining: 23.7s\n",
      "38:\tlearn: 0.1202920\ttotal: 14.9s\tremaining: 23.3s\n",
      "39:\tlearn: 0.1192487\ttotal: 15.2s\tremaining: 22.9s\n",
      "40:\tlearn: 0.1185403\ttotal: 15.7s\tremaining: 22.6s\n",
      "41:\tlearn: 0.1170747\ttotal: 16.2s\tremaining: 22.3s\n",
      "42:\tlearn: 0.1163970\ttotal: 16.7s\tremaining: 22.1s\n",
      "43:\tlearn: 0.1159474\ttotal: 17.2s\tremaining: 21.9s\n",
      "44:\tlearn: 0.1151550\ttotal: 17.6s\tremaining: 21.5s\n",
      "45:\tlearn: 0.1142109\ttotal: 17.9s\tremaining: 21s\n",
      "46:\tlearn: 0.1137153\ttotal: 18.3s\tremaining: 20.6s\n",
      "47:\tlearn: 0.1125991\ttotal: 18.6s\tremaining: 20.2s\n",
      "48:\tlearn: 0.1120807\ttotal: 18.9s\tremaining: 19.7s\n",
      "49:\tlearn: 0.1112303\ttotal: 19.3s\tremaining: 19.3s\n",
      "50:\tlearn: 0.1105523\ttotal: 19.7s\tremaining: 19s\n",
      "51:\tlearn: 0.1101907\ttotal: 20s\tremaining: 18.5s\n",
      "52:\tlearn: 0.1095821\ttotal: 20.4s\tremaining: 18.1s\n",
      "53:\tlearn: 0.1087287\ttotal: 20.8s\tremaining: 17.7s\n",
      "54:\tlearn: 0.1078692\ttotal: 21.3s\tremaining: 17.4s\n",
      "55:\tlearn: 0.1074287\ttotal: 21.6s\tremaining: 17s\n",
      "56:\tlearn: 0.1068255\ttotal: 22.2s\tremaining: 16.8s\n",
      "57:\tlearn: 0.1059631\ttotal: 22.7s\tremaining: 16.4s\n",
      "58:\tlearn: 0.1050983\ttotal: 23.1s\tremaining: 16s\n",
      "59:\tlearn: 0.1047158\ttotal: 23.4s\tremaining: 15.6s\n",
      "60:\tlearn: 0.1043344\ttotal: 23.8s\tremaining: 15.2s\n",
      "61:\tlearn: 0.1034920\ttotal: 24.2s\tremaining: 14.8s\n",
      "62:\tlearn: 0.1030913\ttotal: 24.5s\tremaining: 14.4s\n",
      "63:\tlearn: 0.1024761\ttotal: 24.8s\tremaining: 14s\n",
      "64:\tlearn: 0.1019950\ttotal: 25.2s\tremaining: 13.6s\n",
      "65:\tlearn: 0.1018125\ttotal: 25.5s\tremaining: 13.1s\n",
      "66:\tlearn: 0.1010283\ttotal: 25.8s\tremaining: 12.7s\n",
      "67:\tlearn: 0.1004868\ttotal: 26.2s\tremaining: 12.3s\n",
      "68:\tlearn: 0.0996228\ttotal: 26.6s\tremaining: 11.9s\n",
      "69:\tlearn: 0.0992608\ttotal: 27.2s\tremaining: 11.7s\n",
      "70:\tlearn: 0.0985939\ttotal: 27.5s\tremaining: 11.2s\n",
      "71:\tlearn: 0.0982796\ttotal: 27.9s\tremaining: 10.8s\n",
      "72:\tlearn: 0.0979481\ttotal: 28.2s\tremaining: 10.4s\n",
      "73:\tlearn: 0.0976493\ttotal: 28.5s\tremaining: 10s\n",
      "74:\tlearn: 0.0971285\ttotal: 28.9s\tremaining: 9.63s\n",
      "75:\tlearn: 0.0967169\ttotal: 29.2s\tremaining: 9.23s\n",
      "76:\tlearn: 0.0959393\ttotal: 29.6s\tremaining: 8.84s\n",
      "77:\tlearn: 0.0954927\ttotal: 29.9s\tremaining: 8.44s\n",
      "78:\tlearn: 0.0952822\ttotal: 30.3s\tremaining: 8.04s\n",
      "79:\tlearn: 0.0947779\ttotal: 30.6s\tremaining: 7.64s\n",
      "80:\tlearn: 0.0941179\ttotal: 30.9s\tremaining: 7.25s\n",
      "81:\tlearn: 0.0935451\ttotal: 31.3s\tremaining: 6.87s\n",
      "82:\tlearn: 0.0930112\ttotal: 31.9s\tremaining: 6.54s\n",
      "83:\tlearn: 0.0924846\ttotal: 32.4s\tremaining: 6.17s\n",
      "84:\tlearn: 0.0918291\ttotal: 32.7s\tremaining: 5.78s\n",
      "85:\tlearn: 0.0915435\ttotal: 33.1s\tremaining: 5.38s\n",
      "86:\tlearn: 0.0909326\ttotal: 33.4s\tremaining: 5s\n",
      "87:\tlearn: 0.0898545\ttotal: 33.8s\tremaining: 4.61s\n",
      "88:\tlearn: 0.0897150\ttotal: 34.1s\tremaining: 4.22s\n",
      "89:\tlearn: 0.0890454\ttotal: 34.5s\tremaining: 3.83s\n",
      "90:\tlearn: 0.0884985\ttotal: 34.8s\tremaining: 3.44s\n",
      "91:\tlearn: 0.0881844\ttotal: 35.2s\tremaining: 3.06s\n",
      "92:\tlearn: 0.0874988\ttotal: 35.5s\tremaining: 2.67s\n",
      "93:\tlearn: 0.0872243\ttotal: 35.8s\tremaining: 2.29s\n",
      "94:\tlearn: 0.0869465\ttotal: 36.2s\tremaining: 1.9s\n",
      "95:\tlearn: 0.0867862\ttotal: 36.5s\tremaining: 1.52s\n",
      "96:\tlearn: 0.0858981\ttotal: 37.1s\tremaining: 1.15s\n",
      "97:\tlearn: 0.0850773\ttotal: 37.5s\tremaining: 765ms\n",
      "98:\tlearn: 0.0847077\ttotal: 37.9s\tremaining: 383ms\n",
      "99:\tlearn: 0.0840877\ttotal: 38.5s\tremaining: 0us\n",
      "[CV]  subsample=1, n_estimators=100, max_depth=9, grow_policy=SymmetricTree, score=0.990, total=  40.1s\n",
      "[CV] subsample=5, n_estimators=150, max_depth=13, grow_policy=SymmetricTree \n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/options/bootstrap_options.cpp:5: Taken fraction should in in (0,1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0f198d8fe01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# best hyper parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3842\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   3843\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3844\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0msave_snapshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0m_check_param_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_params_type_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0m_check_train_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0meval_set_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/private/libs/options/bootstrap_options.cpp:5: Taken fraction should in in (0,1]"
     ]
    }
   ],
   "source": [
    "### Tuning CatBoost READY ###\n",
    "# Tune learning rate manually.\n",
    "cbc = CatBoostClassifier(random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "learning_rate = [.1,.3,.5,.7,.9]\n",
    "# bagging_temperature = []\n",
    "subsample = [1,3,5,7]\n",
    "n_estimators = [50,75,100,150]\n",
    "depth = [2,4,6,8,10]\n",
    "grow_policy = ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "#                        learning_rate=learning_rate, \n",
    "                       n_estimators=n_estimators,\n",
    "                       subsample=subsample,\n",
    "#                        depth=depth,\n",
    "                       grow_policy=grow_policy\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(cbc, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n",
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbc.fit(x)\n",
    "mod.X_train, mod.X_test, mod.y_train, mod.y_test = train_test_split(mod.X_features, \n",
    "                                                                    mod.y_target, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbc.fit(mod.X_train, mod.y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc = cbc.predict(mod.X_test)\n",
    "confusion_matrix(y_test, pred_cbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n",
      "       isFraud\n",
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "...        ...\n",
      "59049        0\n",
      "59050        0\n",
      "59051        0\n",
      "59052        0\n",
      "59053        0\n",
      "\n",
      "[59054 rows x 1 columns]\n",
      "[0 0 0 ... 0 0 0]\n",
      "roc auc score: 0.8850176446948032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     56945\n",
      "           1       0.34      0.83      0.48      2109\n",
      "\n",
      "    accuracy                           0.94     59054\n",
      "   macro avg       0.67      0.89      0.72     59054\n",
      "weighted avg       0.97      0.94      0.95     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn      fp  precision    recall  time_elapsed (min)  \\\n",
      "384          NaN  876.0  4959.0   0.199128  0.584637            0.129198   \n",
      "385          NaN  362.0  4608.0   0.274902  0.828355            3.408594   \n",
      "386  model score  357.0  4560.0   0.277567  0.830725            4.032026   \n",
      "387          NaN  359.0  3402.0   0.339674  0.829777            7.586321   \n",
      "0            NaN  359.0  3402.0   0.339674  0.829777            7.044470   \n",
      "\n",
      "         tn       tp  \n",
      "384  1233.0  51986.0  \n",
      "385  1747.0  52337.0  \n",
      "386  1752.0  52385.0  \n",
      "387  1750.0  53543.0  \n",
      "0    1750.0  53543.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col  feat_rank\n",
      "0          V67   0.127891\n",
      "1  ProductCD_R   0.053158\n",
      "2          V88   0.053002\n",
      "3         V301   0.022626\n",
      "4         V300   0.021941\n",
      "5     card5_fe   0.020173\n",
      "6     card6_fe   0.018008\n",
      "7          V47   0.016993\n",
      "8           V9   0.016852\n",
      "9         V302   0.014930\n",
      "Fitting model:\n",
      " XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=0.3,\n",
      "              colsample_bynode=0.5, colsample_bytree=0.7, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=13,\n",
      "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=3, reg_lambda=5, scale_pos_weight=5, seed=None,\n",
      "              silent=None, subsample=0.5, verbosity=1)\n",
      "       isFraud\n",
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "...        ...\n",
      "59049        0\n",
      "59050        0\n",
      "59051        0\n",
      "59052        0\n",
      "59053        0\n",
      "\n",
      "[59054 rows x 1 columns]\n",
      "[0 0 0 ... 0 0 0]\n",
      "roc auc score: 0.8850176446948032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     56945\n",
      "           1       0.34      0.83      0.48      2109\n",
      "\n",
      "    accuracy                           0.94     59054\n",
      "   macro avg       0.67      0.89      0.72     59054\n",
      "weighted avg       0.97      0.94      0.95     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn      fp  precision    recall  time_elapsed (min)  \\\n",
      "385          NaN  362.0  4608.0   0.274902  0.828355            3.408594   \n",
      "386  model score  357.0  4560.0   0.277567  0.830725            4.032026   \n",
      "387          NaN  359.0  3402.0   0.339674  0.829777            7.586321   \n",
      "388          NaN  359.0  3402.0   0.339674  0.829777            7.044470   \n",
      "0            NaN  359.0  3402.0   0.339674  0.829777            6.538398   \n",
      "\n",
      "         tn       tp  \n",
      "385  1747.0  52337.0  \n",
      "386  1752.0  52385.0  \n",
      "387  1750.0  53543.0  \n",
      "388  1750.0  53543.0  \n",
      "0    1750.0  53543.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col  feat_rank\n",
      "0          V67   0.127891\n",
      "1  ProductCD_R   0.053158\n",
      "2          V88   0.053002\n",
      "3         V301   0.022626\n",
      "4         V300   0.021941\n",
      "5     card5_fe   0.020173\n",
      "6     card6_fe   0.018008\n",
      "7          V47   0.016993\n",
      "8           V9   0.016852\n",
      "9         V302   0.014930\n"
     ]
    }
   ],
   "source": [
    "bool_predict_proba = False\n",
    "# tuned model\n",
    "current_model = RandomForestClassifier(\n",
    "                                       max_depth=3, max_features='log2',\n",
    "                                       min_impurity_decrease=0.0, \n",
    "                                       min_samples_leaf=1, min_samples_split=7,\n",
    "                                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                                       n_jobs=-1, oob_score=False, random_state=42,\n",
    "                                       verbose=0, warm_start=False)\n",
    "mod.create_df_score_model(model_current)\n",
    "\n",
    "# base model\n",
    "current_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_read = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "df_temp_read[len(df_temp_read)-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it reads the csv, creates a dataframe, then appends the results, then saves over the old version and keeps\n",
    "# # a record of all columns in the tested dataframe.\n",
    "\n",
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "\n",
    "# # we want to test a feature for feature engineering... we must \n",
    "\n",
    "# for col_original, col_new in zip(list_col, fe.list_new_feat):\n",
    "#     print(col_original, col_new)\n",
    "#     X[col_new] = fe.df_feat[col_new]\n",
    "#     X = X.drop(col_original, axis=1)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "#     X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "#     model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1) # logistic regression\n",
    "#     model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "    \n",
    "#     y_pred = model_lr_pca_sm.predict(X_test)     # predict\n",
    "    \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     print('\\nTime elapsed:', elapsed_time / 60)\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "# #     X = X.drop(col, axis=1)\n",
    "#     print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat['time_delta'] = df_temp['time_delta']\n",
    "# fe.df_feat['time_delta_week'] = df_temp['time_delta']/7\n",
    "# fe.df_feat['time_delta_month'] = df_temp['time_delta']/30\n",
    "# fe.df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### implement into feature engineering class. days lapsed\n",
    "df_temp = fe.df_feat[['TransactionDT']]\n",
    "df_temp['time_delta'] = 0\n",
    "len_df_temp = df_temp.shape[0]\n",
    "for i in range(1,len(df_temp['TransactionDT'][0:len_df_temp])):\n",
    "    val_time_1 = df_temp.loc[i - 1, 'TransactionDT']\n",
    "    val_time_2 = df_temp.loc[i, 'TransactionDT']\n",
    "    val_time_delta = val_time_2 - val_time_1\n",
    "    df_temp.loc[i, 'time_delta'] = val_time_delta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes, do we need to visualize our results?? We need to look at our confusion matrix\n",
    "# and decide how to improve our results from here... Test and see what happens if we \n",
    "# increase our principal components... test removing columns, adding columns, imputing\n",
    "# certain columns all together. \n",
    "\n",
    "# test imputing 500 or less for ohe. Then test dropping card1 due to its number\n",
    "# of unique values that would make our data very high dimensional. We can try only ohe for\n",
    "# the entire dataset to see how our model performs over all... Though.. it will likely run \n",
    "# out of memory and crash. \n",
    "\n",
    "# ALSO test probabilities on logisticregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.rcParams['font.size'] = 14\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "# plt.hist(y_pred_prob[:,1], bins=8)\n",
    "# plt.xlim(0,1)\n",
    "# plt.title(\"Histogram of Probability of Fraud\")\n",
    "# plt.xlabel(\"Predicted probability of Fraud\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "e\n",
    "\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:,1])\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.title(\"ROC curve for fraud detection classifier\")\n",
    "# plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "# plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_threshold(threshold):\n",
    "#     print(\"Sensitivity:\", tpr[thresholds > threshold][-1])\n",
    "#     print(\"Specificity:\", 1 - fpr[thresholds > threshold][-1],'\\n')\n",
    "    \n",
    "# evaluate_threshold(.5)\n",
    "# evaluate_threshold(.2)\n",
    "# evaluate_threshold(.1)\n",
    "\n",
    "# print(roc_auc_score(y, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_test2) # actual prediction test data set\n",
    "# print('y_pred_actual on test set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_actual))\n",
    "# print(classification_report(y_test2, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_test2)\n",
    "# y_pred_class = binarize(y_pred_proba, 0.5)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_class))\n",
    "# print(classification_report(y_test2, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_pca) # actual full data set\n",
    "# print('Logistic Regression')\n",
    "# print('y_pred_actual full data set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y, y_pred_actual))\n",
    "# print(classification_report(y, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba full data set\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_pca)#[:,1]#[0:10]\n",
    "# y_pred_class = binarize(y_pred_proba, 0.2)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(x='TransactionDT', y='TransactionDT', hue='isFraud', data=df_features)\n",
    "# plt.title(\"Transaction Date Versus Fraud\")\n",
    "# plt.show()\n",
    "\n",
    "# we want to figure out how to create more features from TransactionDT.. \n",
    "# are certain transactions more likely. in general, to be fraudualant around a certain day? \n",
    "# we have transactionID yet we dont have the specific card of who it belongs to.. \n",
    "# if card1 is the unique identifier... and we did a groupby on fraud.. \n",
    "# we have average spent per day as an option... \n",
    "# create a feature that is average spent per day for non fraud versus average spent per day for fraud.. \n",
    "# what would the describe method reveal for us? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
