{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_train = train_transaction.merge(train_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.info();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.info();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_transaction_sum = train_transaction.duplicated().sum()\n",
    "# train_identity_sum = train_identity.duplicated().sum()\n",
    "# columns = train_transaction.columns\n",
    "# print('Train transaction duplicates: {}\\nTrain identity duplicates: {} \\n'.format(train_transaction_sum, train_identity_sum))\n",
    "# # print('Train feature columns:\\n', list(columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "# D = time elapsed between each transaction, card = card information, C = counting, ie how many addresses \n",
    "# associated with card, M=True/False, V created features on ranking, counting, etc. \n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = ['isFraud']\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# boolean columns. convert via dummy variable. We dont know if true/false is better than one or the other. \n",
    "# col_bool = col_m\n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i + col_target)))\n",
    "\n",
    "# col_all = col_cat + col_num + col_date + col_bool + col_id + col_target\n",
    "# columns removed dist1, dist2, R_emaildomain, DeviceInfo, DeviceType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 195 columns with null values.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "                \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 10:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()\n",
    "# df_null_info\n",
    "\n",
    "# determine what to do with columns that have too many unique values... obviously.. types of solutions\n",
    "# would be to put \"MISSING\" for those that dont have an email address... but you will need to evaluate \n",
    "# and make instead a counter of unique values, then append that and look at the CSV via google sheets. use \n",
    "# something like the code below \n",
    "\n",
    "# Planning - our preprocessing method must automatically drop missing values, but we can't do that because\n",
    "# we need to see about filling them in first, then decide if we need to drop them. Right now, we need to\n",
    "# create a dataframe that shows unique values for each column with missing values. \n",
    "\n",
    "# we need to look at each variable and see if it's unique or categorical. We need to use possibly PCA...? How do\n",
    "# we handle so many variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded: addr1 unique 333\n",
      "label encoded: addr2 unique 75\n",
      "dummies encoded: ProductCD unique 5\n",
      "label encoded: P_emaildomain unique 59\n",
      "label encoded: card1 unique 13553\n",
      "label encoded: card2 unique 501\n",
      "label encoded: card3 unique 115\n",
      "dummies encoded: card4 unique 4\n",
      "label encoded: card5 unique 120\n",
      "dummies encoded: card6 unique 4\n",
      "dummies encoded: M1 unique 2\n",
      "dummies encoded: M2 unique 2\n",
      "dummies encoded: M3 unique 2\n",
      "dummies encoded: M4 unique 3\n",
      "dummies encoded: M6 unique 2\n",
      "new dataframe shape:(590540, 228)\n"
     ]
    }
   ],
   "source": [
    "pp.df_train = pp.impute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # c is num, ex, how many addresses associated with card\n",
    "# col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# # d is num, time/days between transactions\n",
    "# col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# # m is date of transaction\n",
    "# col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# # v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "# col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# # i is identity information like network and digital signature associated with transaction\n",
    "# col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# # ca is cat, card information such as card type, etc. \n",
    "# col_card = [ca for ca in df_train.columns if ca.startswith('card')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # c is num, ex, how many addresses associated with card\n",
    "# col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# # d is num, time/days between transactions\n",
    "# col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# # m is date of transaction\n",
    "# col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# # v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "# col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# # i is identity information like network and digital signature associated with transaction\n",
    "# col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# # ca is cat, card information such as card type, etc. \n",
    "# col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "# # D = time elapsed between each transaction, card = card information, C = counting, ie how many addresses \n",
    "# # associated with card, M=True/False, V created features on ranking, counting, etc. \n",
    "\n",
    "# # column id and target\n",
    "# col_id = ['TransactionID']\n",
    "# col_target = ['isFraud']\n",
    "\n",
    "# # converting categorical columns with numerical values to string types.\n",
    "# col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "# for val in col_cat_to_obj:\n",
    "#     df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# # categorical columns\n",
    "# col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# # C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "# col_num = ['TransactionAmt'] + col_c + col_d + col_v\n",
    "\n",
    "# # figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "# col_date = ['TransactionDT'] \n",
    "\n",
    "# # boolean columns. convert via dummy variable. We dont know if true/false is better than one or the other. \n",
    "# # col_bool = col_m\n",
    "\n",
    "# # confirming all columns are accounted for\n",
    "# print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i + col_target)))\n",
    "\n",
    "# # col_all = col_cat + col_num + col_date + col_bool + col_id + col_target\n",
    "# # columns removed dist1, dist2, R_emaildomain, DeviceInfo, DeviceType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X.card1, y)\n",
    "# sns.regplot(x='card5_237.0', y='isFraud', data=pp.df_train, logistic=True, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features, target and split the dataframe\n",
    "X = pp.df_train.drop(col_target, axis=1)\n",
    "X = X.drop(col_id, axis=1)\n",
    "y = pp.df_train[col_target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dropping Features On Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# label encoded: addr1 unique 333\n",
    "# dummies encoded: addr2 unique 75\n",
    "# dummies encoded: ProductCD unique 5\n",
    "# dummies encoded: P_emaildomain unique 59\n",
    "# label encoded: card1 unique 13553\n",
    "# label encoded: card2 unique 501\n",
    "# label encoded: card3 unique 115\n",
    "# dummies encoded: card4 unique 4\n",
    "# label encoded: card5 unique 120\n",
    "# dummies encoded: card6 unique 4\n",
    "# dummies encoded: M1 unique 2\n",
    "# dummies encoded: M2 unique 2\n",
    "# dummies encoded: M3 unique 2\n",
    "# dummies encoded: M4 unique 3\n",
    "# dummies encoded: M6 unique 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[960  26]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       986\n",
      "           1       0.37      1.00      0.54        15\n",
      "\n",
      "    accuracy                           0.97      1001\n",
      "   macro avg       0.68      0.99      0.76      1001\n",
      "weighted avg       0.99      0.97      0.98      1001\n",
      "\n",
      "AUC:  0.9961460446247464\n",
      "[[964  22]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       986\n",
      "           1       0.41      1.00      0.58        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.70      0.99      0.78      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.9963488843813387\n",
      "[[963  23]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       986\n",
      "           1       0.39      1.00      0.57        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.70      0.99      0.78      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.996078431372549\n",
      "[[963  23]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       986\n",
      "           1       0.39      1.00      0.57        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.70      0.99      0.78      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.9961460446247464\n",
      "[[963  23]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       986\n",
      "           1       0.39      1.00      0.57        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.70      0.99      0.78      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.9966869506423259\n",
      "[[964  22]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       986\n",
      "           1       0.41      1.00      0.58        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.70      0.99      0.78      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.9963488843813387\n",
      "[[961  25]\n",
      " [  0  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       986\n",
      "           1       0.38      1.00      0.55        15\n",
      "\n",
      "    accuracy                           0.98      1001\n",
      "   macro avg       0.69      0.99      0.77      1001\n",
      "weighted avg       0.99      0.98      0.98      1001\n",
      "\n",
      "AUC:  0.9967545638945233\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.996146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.996349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.996146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.996687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.996349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auc_score  recall  precision\n",
       "0   0.996146     1.0   0.365854\n",
       "1   0.996349     1.0   0.405405\n",
       "2   0.996078     1.0   0.394737\n",
       "3   0.996146     1.0   0.394737\n",
       "4   0.996687     1.0   0.394737\n",
       "5   0.996349     1.0   0.405405\n",
       "6   0.996755     1.0   0.375000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing dropping columns\n",
    "model_lr_results = pd.DataFrame()\n",
    "model_recall = []\n",
    "model_precision = []\n",
    "model_auc_score = []\n",
    "\n",
    "for col in ['addr1', 'addr2', 'P_emaildomain', 'card1', 'card2', 'card3', 'card5']:\n",
    "    X_drop = X.drop(col, axis=1)\n",
    "    X_drop = X_drop.loc[:1000,:]\n",
    "    y_drop = y[:1001]\n",
    "    \n",
    "    scaled_X = StandardScaler().fit_transform(X_drop)\n",
    "    # pca\n",
    "    pca = PCA()\n",
    "    pcomponents = pca.fit_transform(scaled_X)\n",
    "    X_pca = pd.DataFrame(data=pcomponents)\n",
    "    # split\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y_drop, test_size=0.1, random_state=42)\n",
    "    # smote\n",
    "    sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "    # model fit\n",
    "    model_lr_pca = LogisticRegression(random_state=42)\n",
    "    model_lr_pca.fit(X_train_res, y_train_res)\n",
    "    # predict\n",
    "    y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "    y_pred_class = binarize(y_pred_prob, 0.5)[:,1]\n",
    "    # scoring\n",
    "    model_recall.append(recall_score(y_drop, y_pred_class))\n",
    "    model_precision.append(precision_score(y_drop, y_pred_class))\n",
    "    model_auc_score.append(roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "    print(confusion_matrix(y_drop, y_pred_class))\n",
    "    print(classification_report(y_drop, y_pred_class))\n",
    "    print('AUC: ', roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "\n",
    "model_recall = pd.Series(model_recall, name='recall')\n",
    "model_precision = pd.Series(model_precision, name='precision')\n",
    "model_auc_score = pd.Series(model_auc_score, name='auc_score')\n",
    "model_results_final = pd.concat([model_auc_score, model_recall, model_precision],axis=1)\n",
    "model_results_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA (2 components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing our data, which is required for PCA.\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "pd.DataFrame(scaled_X, columns=X.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA instantiate and fit \n",
    "pca = PCA(n_components=2)\n",
    "pcomponents = pca.fit_transform(scaled_X)\n",
    "X_pca = pd.DataFrame(data = pcomponents, columns=['PC1','PC2'])\n",
    "print(X_pca.shape)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two principal components scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca['PC1'], X_pca['PC2'], c=y['isFraud'])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "\n",
    "# explaining vaariance\n",
    "print('Variance ratio:')\n",
    "print(pca.explained_variance_ratio_)\n",
    "# interpreting principal components\n",
    "print('\\nPrincipal components explained:')\n",
    "pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA to all features (all components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA().fit(scaled_X)\n",
    "# pca2.explained_variance_ratio_\n",
    "# np.cumsum(pca2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca2.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.title('Credit Card Fraud Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model w/ SMOTE only - base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=42)\n",
    "model_lr.fit(X_train_res, y_train_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using only SMOTE (and w/o PCA)\\n\")\n",
    "y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_lr.score(X_test1, y_test1))\n",
    "print(recall_score(y_test1, y_pred_test1))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test1, y_pred_test1))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test1, y_pred_test1))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred = model_lr.predict(X)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_lr.score(X, y))\n",
    "print(recall_score(y, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/PCA  w/SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = X.drop('card1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying PCA\n",
    "scaled_X = StandardScaler().fit_transform(X_drop)\n",
    "pca = PCA(n_components=250)\n",
    "pcomponents = pca.fit_transform(scaled_X)\n",
    "X_pca = pd.DataFrame(data=pcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting logistic regression\n",
    "model_lr_pca = LogisticRegression(random_state=42)\n",
    "model_lr_pca.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes, do we need to visualize our results?? We need to look at our confusion matrix\n",
    "# and decide how to improve our results from here... Test and see what happens if we \n",
    "# increase our principal components... test removing columns, adding columns, imputing\n",
    "# certain columns all together. \n",
    "\n",
    "# test imputing 500 or less for ohe. Then test dropping card1 due to its number\n",
    "# of unique values that would make our data very high dimensional. We can try only ohe for\n",
    "# the entire dataset to see how our model performs over all... Though.. it will likely run \n",
    "# out of memory and crash. \n",
    "\n",
    "# ALSO test probabilities on logisticregression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "plt.hist(y_pred_prob[:,1], bins=8)\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Histogram of Probability of Fraud\")\n",
    "plt.xlabel(\"Predicted probability of Fraud\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "y_pred_class = binarize(y_pred_prob, 0.5)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y, y_pred_class))\n",
    "print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title(\"ROC curve for fraud detection classifier\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold):\n",
    "    print(\"Sensitivity:\", tpr[thresholds > threshold][-1])\n",
    "    print(\"Specificity:\", 1 - fpr[thresholds > threshold][-1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(.5)\n",
    "evaluate_threshold(.2)\n",
    "evaluate_threshold(.1)\n",
    "\n",
    "print(roc_auc_score(y, y_pred_prob[:,1]))\n",
    "# NEXT CONT, from here on youtube vid 49:00. once we finish the video\n",
    "# go back to improving the model using feature selection, eda, \n",
    "# and decision tree eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_actual = model_lr_pca.predict(X_test2) # actual prediction test data set\n",
    "print('y_pred_actual on test set\\n')\n",
    "print(y_pred_actual[0:10])\n",
    "print(confusion_matrix(y_test2, y_pred_actual))\n",
    "print(classification_report(y_test2, y_pred_actual))\n",
    "\n",
    "print('y_pred_proba\\n')\n",
    "y_pred_proba = model_lr_pca.predict_proba(X_test2)\n",
    "y_pred_class = binarize(y_pred_proba, 0.5)[:,1]\n",
    "print(y_pred_class[0:10])\n",
    "print(confusion_matrix(y_test2, y_pred_class))\n",
    "print(classification_report(y_test2, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_actual = model_lr_pca.predict(X_pca) # actual full data set\n",
    "print('Logistic Regression')\n",
    "print('y_pred_actual full data set\\n')\n",
    "print(y_pred_actual[0:10])\n",
    "print(confusion_matrix(y, y_pred_actual))\n",
    "print(classification_report(y, y_pred_actual))\n",
    "\n",
    "print('y_pred_proba full data set\\n')\n",
    "y_pred_proba = model_lr_pca.predict_proba(X_pca)#[:,1]#[0:10]\n",
    "y_pred_class = binarize(y_pred_proba, 0.2)[:,1]\n",
    "print(y_pred_class[0:10])\n",
    "print(confusion_matrix(y, y_pred_class))\n",
    "print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using Logistic Regression, PCA, SMOTE\\n\")\n",
    "y_pred_pca_sm = model_lr_pca.predict(X_test2)\n",
    "# y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_lr_pca.score(X_test2, y_test2))\n",
    "print(recall_score(y_test2, y_pred_pca_sm))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test2, y_pred_pca_sm))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test2, y_pred_pca_sm))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred_pca_sm_whole = model_lr_pca.predict(X_pca)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_lr_pca.score(X_pca, y))\n",
    "print(recall_score(y, y_pred_pca_sm_whole))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred_pca_sm_whole))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred_pca_sm_whole))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Decision Tree - w/PCA w/SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "model_dt_pca_smote.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set w/o PCA\n",
    "print(\"Predicting using PCA\\n\")\n",
    "y_pred_pca = model_dt_pca_smote.predict(X_test2)\n",
    "# y_pred_test1 = model_lr.predict(X_test1)\n",
    "print(\"Test set:\")\n",
    "print(\"Validation results\")\n",
    "print(model_dt_pca_smote.score(X_test2, y_test2))\n",
    "print(recall_score(y_test2, y_pred_pca))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test2, y_pred_pca))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test2, y_pred_pca))\n",
    "\n",
    "# predicting on original dataset\n",
    "print(\"Whole dataset:\")\n",
    "y_pred_pca = model_dt_pca_smote.predict(X_pca)\n",
    "print(\"\\nTest Results\")\n",
    "print(model_dt_pca_smote.score(X_pca, y))\n",
    "print(recall_score(y, y_pred_pca))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred_pca))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"\\nPrincipal components explained:\")\n",
    "pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explaining variance\n",
    "# print('Variance ratio:')\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # interpreting principal components\n",
    "# print('\\nPrincipal components explained:')\n",
    "# pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))\n",
    "\n",
    "# # # predicting on original dataset\n",
    "# # y_pred = clf_lr.predict(X)\n",
    "# # print(\"\\nTest Results\")\n",
    "# # print(clf_lr.score(X, y))\n",
    "# # print(recall_score(y, y_pred))\n",
    "# # print(\"\\nConfusion Matrix\")\n",
    "# # print(confusion_matrix(y, y_pred))\n",
    "# # print('\\nClassification Report:\\n')\n",
    "# # print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X2 = StandardScaler().fit_transform(X)\n",
    "# pca3 = PCA(n_components=275)\n",
    "# pcomponents = pca3.fit_transform(scaled_X2)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying SMOTE to train set to correct class imbalance\n",
    "sm = SMOTE(random_state=42, ratio = 1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to residuals created by SMOTE\n",
    "clf_lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "clf_lr.fit(X_train_res, y_train_res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set\n",
    "y_test_pred = clf_lr.predict(X_test)\n",
    "print(\"Validation results\")\n",
    "print(clf_lr.score(X_test, y_test))\n",
    "print(recall_score(y_test, y_test_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on original dataset\n",
    "y_pred = clf_lr.predict(X)\n",
    "print(\"\\nTest Results\")\n",
    "print(clf_lr.score(X, y))\n",
    "print(recall_score(y, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# clf = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\n",
    "# clf.predict(X_test)\n",
    "# clf.score(X_test, y_test)\n",
    "# 0.7870216903822372 \n",
    "# 0.8013998429794899 < 60\n",
    "# 0.5085411973583608 < 1000\n",
    "# 0.63556873752431 < 60 dropped col_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confusion matrix\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read up on class im balance and correct it. \n",
    "# perhaps we can one by one run our model through a decision tree and do one hot encoding for one big \n",
    "# categorical column at a time lets say 13,000 unique values, then we can see of the new 13,000 columns we\n",
    "# have if any actually have predictive value for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# # define iv\n",
    "# iv = X.columns\n",
    "\n",
    "# # fit the logistic regression function\n",
    "# logReg = sm.Logit(y_train, X_train)\n",
    "# answer = logReg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in pp.df_train.columns:\n",
    "#     if (pp.df_train[pp.df_train[val]=='nan'].shape[0]) > 0:\n",
    "#         print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pp.df_train\n",
    "# # lets find out which columns are object... \n",
    "# list_col_object = []\n",
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[val].dtype=='O':\n",
    "#         list_col_object.append(val)\n",
    "        \n",
    "# pp.df_train[list_col_object]\n",
    "# # for card2, nan was the most commonly seen value... so it imputed that...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[pp.df_train[val]=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_col_object = []\n",
    "# for val in pp.df_train.columns:\n",
    "#     if pp.df_train[val]:\n",
    "#         list_col_object.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(pp.df_train[list_col_object].isnull())\n",
    "# pp.df_train['card2'].unique()\n",
    "# pp.df_train[df_train['card2']=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(np.sum(pp.df_train.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to clean the data before thinking about applying PCA. \n",
    "# 1. Determine which columns are continuous, which are ranking.\n",
    "# 2. Determine which columns are bool (easy)\n",
    "# 3. Determine which columns are categorical, then impute with pandas (we dont know which columns means what\n",
    "#    so we cant assume True or better than False, etc.)\n",
    "# 4. After \n",
    "\n",
    "# 1. impute all objects columns with one hot encoding\n",
    "# 1.1 What's a categorical? \n",
    "# We know the V's are ranking.. we need to discern meaning, \n",
    "\n",
    "# 2. figure out if we should do pca next. we should do that next..\n",
    "# 3. then stand up the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(df_train['V14'])\n",
    "# sns.barplot(df_train['V196'])\n",
    "# we need to imput the mode here.. \n",
    "# df_train['V14'].mode()\n",
    "# df_train['V22'].unique()\n",
    "# for val in col_v:\n",
    "#     print(val)\n",
    "#     print(df_train[val].unique())\n",
    "# we ned to descern what is a 0 1 outcome then impute.\n",
    "\n",
    "# col = 'V290'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mode()[0])\n",
    "# plt.hist(series_temp);\n",
    "# missing_val = np.sum(df_train[col].isnull())\n",
    "# print('Missing values: ' + str(missing_val))\n",
    "# print(\"REAL VALUE COUNTS: \")\n",
    "# df_train[col].value_counts().head()\n",
    "\n",
    "# col = 'card4'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mode()[0])\n",
    "# plt.hist(series_temp);\n",
    "# df_train[col].value_counts()\n",
    "\n",
    "# col = 'D1'\n",
    "# series_temp = df_train[col].fillna(df_train[col].mean())\n",
    "# plt.hist(series_temp);\n",
    "# df_train['D1'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
