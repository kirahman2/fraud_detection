{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_raw = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "df_train = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.head() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.head() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.info(); # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity.info(); # KEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_null = df_train.isnull().any()\n",
    "# df_null = pd.DataFrame(list_null).reset_index()\n",
    "# df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.loc[:,df_train.isnull().any()]['id_34'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# train_transaction_sum = train_transaction.duplicated().sum()\n",
    "# train_identity_sum = train_identity.duplicated().sum()\n",
    "# columns = train_transaction.columns\n",
    "# print('Train transaction duplicates: {}\\nTrain identity duplicates: {} \\n'.format(train_transaction_sum, train_identity_sum))\n",
    "# # print('Train feature columns:\\n', list(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.isnull().any() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.info() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.dtypes # KEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many transactions are in the dataset?\n",
    "# df_train.shape # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the fraud rate of the data set?\n",
    "# fraud_rate = df_train.isFraud.value_counts() / 590540\n",
    "# fraud_rate  # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the statistical overview of the data set?\n",
    "# df_train.describe() # KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud_summary = df_train.groupby('isFraud')\n",
    "# fraud_summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud_summary.mean().to_csv('/Users/krahman/work/fraud_detection/saved_files/fraud_groupby.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "# D = time elapsed between each transaction, card = card information, C = counting, ie how many addresses \n",
    "# associated with card, M=True/False, V created features on ranking, counting, etc. \n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = 'isFraud'\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v \n",
    "col_num.append(col_target)\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# boolean columns. convert via dummy variable. We dont know if true/false is better than one or the other. \n",
    "# col_bool = col_m\n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i)))\n",
    "\n",
    "# columns removed dist1, dist2, R_emaildomain, DeviceInfo, DeviceType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 195 columns with null values.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train.copy()\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "#                 self.list_mode_value.append('MISSING')\n",
    "\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "#                 print(\"fillna,\" + str(val))\n",
    "\n",
    "#                 self.df_train[val] = self.df_train[val].fillna('MISSING')\n",
    "    \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 60:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()\n",
    "# df_null_info\n",
    "\n",
    "# determine what to do with columns that have too many unique values... obviously.. types of solutions\n",
    "# would be to put \"MISSING\" for those that dont have an email address... but you will need to evaluate \n",
    "# and make instead a counter of unique values, then append that and look at the CSV via google sheets. use \n",
    "# something like the code below \n",
    "\n",
    "# TEST: test imputing with missing instead of mode to see if we have improvements in model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoded: addr1 unique 333\n",
      "label encoded: addr2 unique 75\n",
      "dummies encoded: ProductCD unique 5\n",
      "dummies encoded: P_emaildomain unique 59\n",
      "label encoded: card1 unique 13553\n",
      "label encoded: card2 unique 501\n",
      "label encoded: card3 unique 115\n",
      "dummies encoded: card4 unique 4\n",
      "label encoded: card5 unique 120\n",
      "dummies encoded: card6 unique 4\n",
      "dummies encoded: M1 unique 2\n",
      "dummies encoded: M2 unique 2\n",
      "dummies encoded: M3 unique 2\n",
      "dummies encoded: M4 unique 3\n",
      "dummies encoded: M6 unique 2\n",
      "new dataframe shape:(590540, 285)\n"
     ]
    }
   ],
   "source": [
    "pp.df_train = pp.impute_features()\n",
    "pp.df_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['addr1', 'addr2', 'ProductCD', 'P_emaildomain', 'card1', 'card2', 'card3', 'card4', \n",
    "#  'card5', 'card6', 'M1', 'M2', 'M3', 'M4', 'M6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')\n",
    "df_features = df_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>V40</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V51</th>\n",
       "      <th>V52</th>\n",
       "      <th>V86</th>\n",
       "      <th>V87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>isFraud</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174672</td>\n",
       "      <td>0.217870</td>\n",
       "      <td>0.235436</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>0.195492</td>\n",
       "      <td>0.222343</td>\n",
       "      <td>0.221568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V40</td>\n",
       "      <td>0.174672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.225232</td>\n",
       "      <td>0.271469</td>\n",
       "      <td>0.744831</td>\n",
       "      <td>0.745758</td>\n",
       "      <td>0.217055</td>\n",
       "      <td>0.213533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V44</td>\n",
       "      <td>0.217870</td>\n",
       "      <td>0.225232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905537</td>\n",
       "      <td>0.257145</td>\n",
       "      <td>0.251881</td>\n",
       "      <td>0.604776</td>\n",
       "      <td>0.515480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V45</td>\n",
       "      <td>0.235436</td>\n",
       "      <td>0.271469</td>\n",
       "      <td>0.905537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.296102</td>\n",
       "      <td>0.585396</td>\n",
       "      <td>0.608788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V51</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>0.744831</td>\n",
       "      <td>0.257145</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954315</td>\n",
       "      <td>0.212453</td>\n",
       "      <td>0.196567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V52</td>\n",
       "      <td>0.195492</td>\n",
       "      <td>0.745758</td>\n",
       "      <td>0.251881</td>\n",
       "      <td>0.296102</td>\n",
       "      <td>0.954315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215183</td>\n",
       "      <td>0.207535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V86</td>\n",
       "      <td>0.222343</td>\n",
       "      <td>0.217055</td>\n",
       "      <td>0.604776</td>\n",
       "      <td>0.585396</td>\n",
       "      <td>0.212453</td>\n",
       "      <td>0.215183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V87</td>\n",
       "      <td>0.221568</td>\n",
       "      <td>0.213533</td>\n",
       "      <td>0.515480</td>\n",
       "      <td>0.608788</td>\n",
       "      <td>0.196567</td>\n",
       "      <td>0.207535</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          isFraud       V40       V44       V45       V51       V52       V86  \\\n",
       "isFraud  1.000000  0.174672  0.217870  0.235436  0.182007  0.195492  0.222343   \n",
       "V40      0.174672  1.000000  0.225232  0.271469  0.744831  0.745758  0.217055   \n",
       "V44      0.217870  0.225232  1.000000  0.905537  0.257145  0.251881  0.604776   \n",
       "V45      0.235436  0.271469  0.905537  1.000000  0.257400  0.296102  0.585396   \n",
       "V51      0.182007  0.744831  0.257145  0.257400  1.000000  0.954315  0.212453   \n",
       "V52      0.195492  0.745758  0.251881  0.296102  0.954315  1.000000  0.215183   \n",
       "V86      0.222343  0.217055  0.604776  0.585396  0.212453  0.215183  1.000000   \n",
       "V87      0.221568  0.213533  0.515480  0.608788  0.196567  0.207535  0.850021   \n",
       "\n",
       "              V87  \n",
       "isFraud  0.221568  \n",
       "V40      0.213533  \n",
       "V44      0.515480  \n",
       "V45      0.608788  \n",
       "V51      0.196567  \n",
       "V52      0.207535  \n",
       "V86      0.850021  \n",
       "V87      1.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_summary = df_features.groupby('isFraud')\n",
    "fraud_summary.mean().to_csv('/Users/krahman/work/fraud_detection/saved_files/fraud_summary.csv')\n",
    "\n",
    "corr = df_features.corr()\n",
    "corr.to_csv('/Users/krahman/work/fraud_detection/saved_files/corr_matrix.csv')\n",
    "corr2 = corr[corr['isFraud']>.17]\n",
    "corr2 = corr2.loc[:,corr2.index]\n",
    "\n",
    "sns.heatmap(corr2,\n",
    "           xticklabels=corr2.columns.values,\n",
    "           yticklabels=corr2.columns.values)\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.show()\n",
    "corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,4))\n",
    "# sns.barplot(x='V44', y='V44', hue='isFraud', data=df_train)\n",
    "# plt.show()\n",
    "\n",
    "# sns.lmplot(x='addr1', y='addr2', data=df_features,\n",
    "#            fit_reg=False, hue='isFraud')\n",
    "# plt.show()\n",
    "\n",
    "# sns.distplot(df_features.addr1)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(X.card1, y)\n",
    "# sns.regplot(x='card5_237.0', y='isFraud', data=pp.df_train, logistic=True, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create features, target and split the dataframe\n",
    "# X = pp.df_train.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = pp.df_train[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dropping Features On Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# y_test = pp.df_train[col_target] #.rename(columns=['isFraud'])\n",
    "# y_test = pd.Series(y_test)\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing dropping columns\n",
    "# model_lr_results = pd.DataFrame()\n",
    "# model_recall = []\n",
    "# model_precision = []\n",
    "# model_auc_score = []\n",
    "\n",
    "# for col in ['addr1', 'addr2', 'P_emaildomain', 'card1', 'card2', 'card3', 'card5']:\n",
    "#     print('Dropping: ', col)\n",
    "#     X_drop = X.drop(col, axis=1)\n",
    "# #     X_drop = X_drop.loc[:10000,:]\n",
    "#     y_drop = y#[:10001]\n",
    "    \n",
    "#     scaled_X = StandardScaler().fit_transform(X_drop)\n",
    "#     # pca\n",
    "#     pca = PCA()\n",
    "#     pcomponents = pca.fit_transform(scaled_X)\n",
    "#     X_pca = pd.DataFrame(data=pcomponents)\n",
    "#     # split\n",
    "#     X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y_drop, test_size=0.1, random_state=42)\n",
    "#     # smote\n",
    "#     sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "#     X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "#     # model fit\n",
    "#     model_lr_pca = LogisticRegression(random_state=42)\n",
    "#     model_lr_pca.fit(X_train_res, y_train_res)\n",
    "#     # predict\n",
    "#     y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "#     y_pred_class = binarize(y_pred_prob, 0.5)[:,1]\n",
    "#     # scoring\n",
    "#     model_recall.append(recall_score(y_drop, y_pred_class))\n",
    "#     model_precision.append(precision_score(y_drop, y_pred_class))\n",
    "#     model_auc_score.append(roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "#     print(confusion_matrix(y_drop, y_pred_class))\n",
    "#     print(classification_report(y_drop, y_pred_class))\n",
    "#     print('AUC: ', roc_auc_score(y_drop, y_pred_prob[:,1]))\n",
    "\n",
    "# model_recall = pd.Series(model_recall, name='recall')\n",
    "# model_precision = pd.Series(model_precision, name='precision')\n",
    "# model_auc_score = pd.Series(model_auc_score, name='auc_score')\n",
    "# model_results_final = pd.concat([model_auc_score, model_recall, model_precision],axis=1)\n",
    "# model_results_final\n",
    "\n",
    "# # it's apparent that label encoding on some of these don't really matter and if we drop them.. it doesn't really\n",
    "# # matter.. \n",
    "# # dropping these columns has little impact with logistic regression.. \n",
    "\n",
    "# # tune logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DELETE\n",
    "\n",
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final\n",
    "\n",
    "# df_temp = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "# df_temp[0:20]\n",
    "# pd.DataFrame(pca.components_, columns=list(X_temp.columns), index=range(0,225))\n",
    "# pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA (2 components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardizing our data, which is required for PCA.\n",
    "# scaled_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # PCA instantiate and fit \n",
    "# pca = PCA(n_components=2)\n",
    "# pcomponents = pca.fit_transform(scaled_X)\n",
    "# X_pca = pd.DataFrame(data = pcomponents, columns=['PC1','PC2'])\n",
    "# print(X_pca.shape)\n",
    "# X_pca.head()\n",
    "\n",
    "# # two principal components scatter plot\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_pca['PC1'], X_pca['PC2'], c=y['isFraud'])\n",
    "# plt.xlabel('First principal component')\n",
    "# plt.ylabel('Second principal component')\n",
    "\n",
    "# # explaining vaariance\n",
    "# print('Variance ratio:')\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # interpreting principal components\n",
    "# print('\\nPrincipal components explained:')\n",
    "# pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA to all features (all components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca2 = PCA().fit(scaled_X)\n",
    "# plt.plot(np.cumsum(pca2.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Variance (%)')\n",
    "# plt.title('Credit Card Fraud Explained Variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model w/ SMOTE only - base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# # Apply SMOTE\n",
    "# sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train1, y_train1)\n",
    "\n",
    "# model_lr = LogisticRegression(random_state=42)\n",
    "# model_lr.fit(X_train_res, y_train_res) \n",
    "\n",
    "# # predicting on test set w/o PCA\n",
    "# print(\"Predicting using only SMOTE (and w/o PCA)\\n\")\n",
    "# y_pred_test1 = model_lr.predict(X_test1)\n",
    "# print(\"Test set:\")\n",
    "# print(\"Validation results\")\n",
    "# print(model_lr.score(X_test1, y_test1))\n",
    "# print(recall_score(y_test1, y_pred_test1))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y_test1, y_pred_test1))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y_test1, y_pred_test1))\n",
    "\n",
    "# # predicting on original dataset\n",
    "# print(\"Whole dataset:\")\n",
    "# y_pred = model_lr.predict(X)\n",
    "# print(\"\\nTest Results\")\n",
    "# print(model_lr.score(X, y))\n",
    "# print(recall_score(y, y_pred))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/PCA  w/SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(x='addr1', y='addr2', data=df_features,\n",
    "#            fit_reg=False, hue='isFraud')\n",
    "# plt.title('addr1 versus addr2')\n",
    "# plt.show()\n",
    "\n",
    "# sns.distplot(df_features.addr1)\n",
    "# plt.title('Addr1 Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EDA Email Class\n",
    "# # P_emaildomain\n",
    "# # list(df_train.columns)\n",
    "# # df_train.P_emaildomain.unique()\n",
    "# list_perc = []\n",
    "# list_fraud_count = []\n",
    "# list_non_fraud_count = []\n",
    "# for val in df_train.P_emaildomain.unique():\n",
    "#     non_fraud_count = df_train[(df_train.P_emaildomain==val) & (df_train.isFraud==0)].shape[0]\n",
    "#     fraud_count = df_train[(df_train.P_emaildomain==val) & (df_train.isFraud==1)].shape[0]\n",
    "    \n",
    "#     list_perc.append(fraud_count/non_fraud_count)\n",
    "    \n",
    "#     list_fraud_count.append(fraud_count)\n",
    "#     list_non_fraud_count.append(non_fraud_count)\n",
    "    \n",
    "# col_email = pd.Series(df_train.P_emaildomain.unique(), name='email')\n",
    "# col_perc = pd.Series(list_perc, name='fraud_perc')\n",
    "# col_fraud_count = pd.Series(list_fraud_count, name='fraud_count')\n",
    "# col_non_fraud_count = pd.Series(list_non_fraud_count, name='non_fraud_count')\n",
    "\n",
    "# # col_perc\n",
    "# df_email_fe = pd.concat([col_email, col_perc, col_fraud_count, col_non_fraud_count], axis=1)\n",
    "# # df_email_fe\n",
    "# # df_train[(df_train.P_emaildomain=='outlook.com') & (df_train.isFraud==1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: less occurring email domains are more likely to have higher fraud_perc because they are \n",
    "# perhaps an email provider with less resources. The card that the holder has attached to certain email\n",
    "# addresses puts them at higher risk for fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.dtypes\n",
    "# df_features = df_\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features['P_emaildomain_copy'] = df_train['P_emaildomain']\n",
    "class FeatureEngineering():\n",
    "    '''create engineered features for columns without ordinal values'''\n",
    "    def __init__(self):\n",
    "        self.list_fraud_perc = []\n",
    "        self.df_feat = df_features.copy()\n",
    "        self.df_raw = df_raw.copy()\n",
    "        self.len_df_feat = self.df_feat.shape[0]\n",
    "        self.list_new_feat = []\n",
    "        self.dict_all_feat = {}\n",
    "        self.new_col = []\n",
    "        \n",
    "    def create_feature(self, col):\n",
    "        '''creates correllated ratio to target column'''\n",
    "#         self.df_feat = df_features.copy()\n",
    "        df_feat = df_features.copy()\n",
    "#         print(col)\n",
    "        df_feat = self._calculate_fraud_perc(col, df_feat) #DONE\n",
    "#         print(self.df_feat.columns)\n",
    "        self.list_new_feat.append(col + '_fe') #DONE\n",
    "#         print(self.df_feat.columns)\n",
    "        df_feat = self._map_col(col, df_feat)  # NEXT, CONT.\n",
    "#         print(self.df_feat.columns)\n",
    "        df_feat = self._create_ratio(df_feat)  \n",
    "#         print(self.df_feat.columns)\n",
    "        df_feat = df_feat.drop(col, axis=1)\n",
    "#         print(self.df_feat.columns)\n",
    "        return df_feat\n",
    "\n",
    "    def aggregate_features(self, list_col, val_aggreg):\n",
    "        for col in list_col:\n",
    "            df_groupby = self.df_raw.groupby(col).mean()\n",
    "            dict_aggreg_col = df_groupby[[val_aggreg]].to_dict()\n",
    "            self.df_feat[col + '_fe'] = self.df_raw[col].map(dict_aggreg_col['TransactionAmt'])\n",
    "            col_mode = self.df_feat[col + '_fe'].mode()[0]\n",
    "            self.df_feat[col + '_fe'] = self.df_feat[col + '_fe'].fillna(col_mode)\n",
    "\n",
    "    def _calculate_fraud_perc(self, col_val, df_feat):\n",
    "        '''calculate fraud percentage for each column'''\n",
    "        list_perc = []\n",
    "        dict_feat = {}\n",
    "        unique_col_values = df_feat[col_val].unique()\n",
    "        for val in unique_col_values:\n",
    "            fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==1)].shape[0]\n",
    "            non_fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==0)].shape[0]\n",
    "            if (non_fraud_total==0):\n",
    "                list_perc.append(0)\n",
    "            else: \n",
    "                list_perc.append(fraud_total/non_fraud_total)\n",
    "        self._create_dict(col_val, list_perc, unique_col_values)\n",
    "        return df_feat\n",
    "\n",
    "    def _create_dict(self, col_val, list_perc, unique_col_values):\n",
    "        '''create dictionary for original values to new fraud percent values'''\n",
    "        col_name = col_val + '_fraud_perc'\n",
    "        series_perc = pd.Series(list_perc, name=col_name)\n",
    "        series_col = pd.Series(unique_col_values, name=col_val)\n",
    "        df_feat = pd.concat([series_col, series_perc], axis=1)\n",
    "        df_feat = df_feat.sort_values(col_name, ascending=False) \n",
    "        dict_feat = df_feat.set_index(col_val).to_dict()\n",
    "        self.dict_all_feat.update(dict_feat)\n",
    "\n",
    "    def _map_col(self, col, df_feat):\n",
    "        '''map dictionary values to new features'''\n",
    "        dict_keys = self.dict_all_feat.keys()\n",
    "        for val in dict_keys:\n",
    "            df_feat[col + '_fe'] = df_feat[col].map(self.dict_all_feat[val])\n",
    "            self.new_col.append(col + '_fe') \n",
    "        return df_feat\n",
    "            \n",
    "    def _create_ratio(self, df_feat):\n",
    "        '''finalize new features with ranking values'''\n",
    "\n",
    "        for val in self.new_col:\n",
    "            col_min_val = df_feat[df_feat[val] > 0][val].min()\n",
    "            df_feat[val] = df_feat[val]/col_min_val\n",
    "        self.new_col = []\n",
    "        return df_feat\n",
    "#             \n",
    "\n",
    "# list_col = ['addr1', 'addr2', 'card1', 'card2', 'card3', 'card5', 'P_emaildomain_copy']\n",
    "# list_col = ['addr1'] # needed for instantiating fe class.\n",
    "fe = FeatureEngineering()\n",
    "\n",
    "# NEXT. fix what's happening after _map, which is happening within _ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_col = ['TransactionAmt']#, 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new feature from  addr1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       100\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.50      0.46      0.48       100\n",
      "weighted avg       1.00      0.92      0.96       100\n",
      "\n",
      "printing df_scores...\n",
      "    feat_tested    recall  precision   fn     fp     tp    tn  time (s)\n",
      "38       addr2  0.692308   0.089109    8    184    790    18  0.105582\n",
      "39       addr1  0.615385   0.108844   10    131    843    16  0.079705\n",
      "40       addr2  0.692308   0.089109    8    184    790    18  0.096269\n",
      "41       addr1  0.824087   0.055005  371  29859  27086  1738  2.479661\n",
      "0        addr1  0.000000   0.000000    0      8     92     0  0.006549\n"
     ]
    }
   ],
   "source": [
    "class Model():\n",
    "    def __init__(self, bool_smote):\n",
    "        X_train, self.X_test, y_train, self.y_test = self._create_dataframe(df_features)\n",
    "        self.X_train, self.y_train = self._apply_smote(bool_smote, X_train, y_train)\n",
    "        self.col_fe = []\n",
    "        \n",
    "    def _create_dataframe(self, df_feat):\n",
    "        X = df_feat.drop(col_target, axis=1)\n",
    "        X = X.drop(col_id, axis=1)\n",
    "        y = df_feat[col_target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.1, \n",
    "                                                            random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def feature_testing(self, list_feat):\n",
    "        if list_feat:      # feature testing and scoring. \n",
    "            for col in list_feat:\n",
    "                self.col_fe = col\n",
    "                df_feat = fe.create_feature(col)\n",
    "                df_feat = df_feat[0:1000] ### DELETE\n",
    "                X_train, X_test, y_train, y_test = self._create_dataframe(df_feat) \n",
    "                X_train, y_train = self._apply_smote(True, X_train, y_train)\n",
    "                model = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "                self.add_model(model, X_train, y_train, X_test, y_test) # also handles scoring \n",
    "            self.col_fe = []\n",
    "\n",
    "    def _apply_smote(self, bool_smote, X_train, y_train):\n",
    "        '''applying smote to split training set'''\n",
    "        if bool_smote:\n",
    "            sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "            X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            return X_train_res, y_train_res\n",
    "        else:\n",
    "            return X_train, y_train\n",
    "        \n",
    "    def add_model(self, temp_model, X_train, y_train, X_test, y_test):        \n",
    "        '''adding and scoring model'''\n",
    "        model = temp_model\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        self._score_model(y_pred, y_test, elapsed_time)\n",
    "        \n",
    "    def _score_model(self, y_pred, y_test, elapsed_time):      \n",
    "        '''creating dataframe with score results'''\n",
    "        col_recall = pd.Series(recall_score(y_test, y_pred), name='recall')\n",
    "        col_precision = pd.Series(precision_score(y_test, y_pred), name='precision')\n",
    "        col_time = pd.Series(elapsed_time, name='time (s)')\n",
    "        df_conf_matrix = self._confusion_matrix(y_test, y_pred)\n",
    "        df_temp = pd.concat([col_recall, col_precision, df_conf_matrix, col_time], axis=1)\n",
    "        \n",
    "        if self.col_fe:\n",
    "            print(\"Creating new feature from\", self.col_fe)\n",
    "            col_fe = pd.Series(self.col_fe, name='feat_tested')\n",
    "            df_temp = pd.concat([col_fe, df_temp], axis=1)\n",
    "            \n",
    "        df_scores = self._read_create_file(df_temp)\n",
    "        self._save_results(df_scores, df_temp, y_test, y_pred)\n",
    "        \n",
    "    def _read_create_file(self, df_temp):\n",
    "        '''reading or creating df_scores file'''\n",
    "        try: \n",
    "            df_scores = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv') # comment out\n",
    "            df_scores = df_scores.drop('Unnamed: 0', axis=1) # comment out when recreating file\n",
    "        except:\n",
    "            print(\"\\nCreating df_scores.csv file.\") \n",
    "            df_scores = df_temp\n",
    "        return df_scores\n",
    "            \n",
    "    def _save_results(self, df_scores, df_temp, y_test, y_pred):\n",
    "        '''printing scores for new features'''            \n",
    "        df_scores = pd.concat([df_scores, df_temp], axis=0)\n",
    "        df_scores.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "        val_classif_rep = classification_report(y_test, y_pred)\n",
    "        print(val_classif_rep)\n",
    "        print('printing df_scores...\\n', df_scores.tail(5))\n",
    "        file_summary = open('/Users/krahman/work/fraud_detection/saved_files/df_scores_summary.txt', \"r+\")\n",
    "        file_summary.write(self.col_fe)\n",
    "        file_summary.write(val_classif_rep)\n",
    "        file_summary.close()\n",
    "        \n",
    "    def _confusion_matrix(self, y_test, y_pred):\n",
    "        '''creating confusion matrix dataframe'''\n",
    "        df_conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "        val_tp = pd.Series(df_conf_matrix[0][0], name='tp')\n",
    "        val_fn = pd.Series(df_conf_matrix[0][1], name='fn')\n",
    "        val_fp = pd.Series(df_conf_matrix[1][0], name='fp')\n",
    "        val_tn = pd.Series(df_conf_matrix[1][1], name='tn')\n",
    "        return pd.concat([val_fn, val_fp, val_tp, val_tn], axis=1)\n",
    "        \n",
    "model = Model(bool_smote=True)      \n",
    "# model.add_model(LogisticRegression(random_state=42, n_jobs=-1))\n",
    "# model.feature_testing(list_feat=['addr1', 'addr2,', 'card2'])\n",
    "model.feature_testing(list_feat=['addr1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it reads the csv, creates a dataframe, then appends the results, then saves over the old version and keeps\n",
    "# a record of all columns in the tested dataframe.\n",
    "\n",
    "X = df_features.drop(col_target, axis=1)\n",
    "X = X.drop(col_id, axis=1)\n",
    "y = df_features[col_target]\n",
    "\n",
    "# we want to test a feature for feature engineering... we must \n",
    "\n",
    "for col_original, col_new in zip(list_col, fe.list_new_feat):\n",
    "    print(col_original, col_new)\n",
    "    X[col_new] = fe.df_feat[col_new]\n",
    "    X = X.drop(col_original, axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    start_time = time.time()\n",
    "\n",
    "    sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1) # logistic regression\n",
    "    model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    y_pred = model_lr_pca_sm.predict(X_test)     # predict\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     X = X.drop(col, axis=1)\n",
    "    print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.list_new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = fe.df_feat[['TransactionDT']]\n",
    "# df_temp['time_delta'] = 0\n",
    "# len_df_temp = df_temp.shape[0]\n",
    "# for i in range(1,len(df_temp['TransactionDT'][0:len_df_temp])):\n",
    "#     val_time_1 = df_temp.loc[i - 1, 'TransactionDT']\n",
    "#     val_time_2 = df_temp.loc[i, 'TransactionDT']\n",
    "#     val_time_delta = val_time_2 - val_time_1\n",
    "#     df_temp.loc[i, 'time_delta'] = val_time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat = fe.df_feat.drop(['time_delta_fe'],axis=1)\n",
    "# df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat['time_delta'] = df_temp['time_delta']\n",
    "# fe.df_feat['time_delta_week'] = df_temp['time_delta']/7\n",
    "# fe.df_feat['time_delta_month'] = df_temp['time_delta']/30\n",
    "# fe.df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat = fe.df_feat.drop(['time_delta_fe','time_delta_fe_week'],axis=1)\n",
    "# list(fe.df_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat['TransactionAmt'] = df_features['TransactionAmt']\n",
    "# list(fe.df_feat)\n",
    "fe.list_new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat = fe.df_feat.drop(['time_delta_fe','addr1_fe'],axis=1)\n",
    "# list(fe.df_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# val_aggreg = 'TransactionAmt'\n",
    "# list_col = ['card2', 'C4', 'C1', 'V317', 'ProductCD', 'V294', 'V279', 'C14', 'card6', 'V306', 'V69']\n",
    "# fe.aggregate_features(list_col, val_aggreg)\n",
    "# list(fe.df_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete\n",
    "# df_temp = fe.df_feat[['TransactionDT']]\n",
    "# df_temp['time_delta'] = 0\n",
    "# len_df_temp = df_temp.shape[0]\n",
    "# for i in range(1,len(df_temp['TransactionDT'][0:len_df_temp])):\n",
    "#     val_time_1 = df_temp.loc[i - 1, 'TransactionDT']\n",
    "#     val_time_2 = df_temp.loc[i, 'TransactionDT']\n",
    "#     val_time_delta = val_time_2 - val_time_1\n",
    "#     df_temp.loc[i, 'time_delta'] = val_time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_temp.time_delta.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp[df_temp.time_delta==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp[(df_temp.time_delta==1) & (df_temp.isFraud==0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp[(df_temp.time_delta==1) & (df_temp.isFraud==1)].shape[0]/df_temp[(df_temp.time_delta==1) & (df_temp.isFraud==0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp\n",
    "# we need to figure out how to bin transactions based \n",
    "# Is there a relationship between isFraud and time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp['TransactionAmt'] = fe.df_feat['TransactionAmt']\n",
    "# df_temp['isFraud'] = fe.df_feat['isFraud']\n",
    "# df_temp[df_temp.isFraud==1][0:60] # how do we bin these? \n",
    "# df_temp2 = fe.df_feat.copy()\n",
    "# df_temp2['time_delta'] = df_temp['time_delta']\n",
    "# df_temp2.groupby('isFraud').mean()\n",
    "# # we can do based on transactionamt and time delta as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through the aggregation of date, every 7 days, calculate the possibility of fraud for the average week\n",
    "# then do the average month\n",
    "# this would mean that we need to order the dataset dates in some kind of order of 7\n",
    "# create column that expresses days lapsed... just focus on that for a bit. Days lapsed between each transaction\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list_timedelta_isfraud = ['D1','D2','D3','D4','D10','D11','D15','isFraud']#, 'TransactionAmt']\n",
    "# df_temp = fe.df_feat[list_timedelta_isfraud].groupby('isFraud').mean()\n",
    "# df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into this strategy \n",
    "# so we take each D and aggregate it to C4 for only columns that have fraud, then for columns that dont have\n",
    "# fraud, then we map those values to each that dont have fraud and those that do... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # append D1 value based on if its fraud or not. Create a dictionary for each column, then map it\n",
    "# for col in ['D1']:\n",
    "#     df_temp = fe.df_feat[[col, 'isFraud']].groupby('isFraud').mean()\n",
    "#     dict_col = df_temp.to_dict()\n",
    "#     fe.df_feat[col + '_fe'] = \n",
    "#     # this might not work because we cant assign some random value that has a row that is fraud and expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_test = ['C2', 'D1',\n",
    "#  'D2',\n",
    "#  'D3',\n",
    "#  'D4',\n",
    "#  'D10',\n",
    "#  'D11',\n",
    "#  'D15', 'isFraud']\n",
    "# df_not_fraud = fe.df_feat[fe.df_feat.isFraud==0]\n",
    "# df_fraud = fe.df_feat[fe.df_feat.isFraud==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate based on D1 and isFraud\n",
    "# df_test = fe.df_feat[['C2', 'isFraud']].groupby('C2').mean()\n",
    "# # df_test = fraud_summ.mean() \n",
    "# # df_test.isFraud.value_counts()\n",
    "# dict_c2_isfraud = df_test.to_dict()['isFraud']\n",
    "# fe.df_feat['C2_fe'] = fe.df_feat['C2'].map(dict_c2_isfraud)\n",
    "# ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "\n",
    "# X = fe.df_feat.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# ### PCA + SMOTE testing algorithm ###\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# # applying PCA\n",
    "# scaled_X = StandardScaler().fit_transform(X)\n",
    "# # pca = PCA(n_components=250)\n",
    "# pca = PCA()\n",
    "# pcomponents = pca.fit_transform(scaled_X)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)\n",
    "\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# # applying SMOTE\n",
    "# sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "\n",
    "# # fit logistic regression\n",
    "# model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# # predict\n",
    "# y_pred = model_lr_pca_sm.predict(X_train2)\n",
    "# # time\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print('\\nTime elapsed:', elapsed_time / 60)\n",
    "# print(confusion_matrix(y_train2, y_pred))\n",
    "# print(classification_report(y_train2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.drop(col_target, axis=1)\n",
    "X = X.drop(col_id, axis=1)\n",
    "y = df_features[col_target]\n",
    "list(X.columns[275:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X = fe.df_feat.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# list(X.columnsa[275:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NEXT, get our score working properly again... What did we do to score \n",
    "# LogisticRegression Base\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "start_time = time.time()\n",
    "# applying SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "# fit logistic regression\n",
    "model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# predict\n",
    "y_pred = model_lr_pca_sm.predict(X_test)\n",
    "# time\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# BASE SCORE\n",
    "# Time elapsed: 4.251827295621236\n",
    "# [[33985 22960]\n",
    "#  [  601  1508]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.60      0.74     56945\n",
    "#            1       0.06      0.72      0.11      2109\n",
    "\n",
    "#     accuracy                           0.60     59054\n",
    "#    macro avg       0.52      0.66      0.43     59054\n",
    "# weighted avg       0.95      0.60      0.72     59054\n",
    "\n",
    "# RESULT: addr1_fe only\n",
    "\n",
    "# RESULT: time_delta_fe, addr1\n",
    "# Time elapsed: 5.163579479853312\n",
    "# [[26551 30394]\n",
    "#  [  363  1746]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.47      0.63     56945\n",
    "#            1       0.05      0.83      0.10      2109\n",
    "\n",
    "#     accuracy                           0.48     59054\n",
    "#    macro avg       0.52      0.65      0.37     59054\n",
    "# weighted avg       0.95      0.48      0.61     59054\n",
    "\n",
    "# RESULT: time_delta_fe, time_delta_week_fe\n",
    "# Time elapsed: 3.142271514733632\n",
    "# [[32540 24405]\n",
    "#  [  560  1549]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.57      0.72     56945\n",
    "#            1       0.06      0.73      0.11      2109\n",
    "\n",
    "#     accuracy                           0.58     59054\n",
    "#    macro avg       0.52      0.65      0.42     59054\n",
    "# weighted avg       0.95      0.58      0.70     59054\n",
    "\n",
    "\n",
    "# RESULT: created time_delta_fe\n",
    "# Time elapsed: 3.1532896359761557\n",
    "# [[26260 30685]\n",
    "#  [  359  1750]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.46      0.63     56945\n",
    "#            1       0.05      0.83      0.10      2109\n",
    "\n",
    "#     accuracy                           0.47     59054\n",
    "#    macro avg       0.52      0.65      0.36     59054\n",
    "# weighted avg       0.95      0.47      0.61     59054\n",
    "\n",
    "# RESULT: time_delta_fe only. dropping addr1_fe\n",
    "# Time elapsed: 6.11533077955246\n",
    "# [[26893 30052]\n",
    "#  [  365  1744]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.47      0.64     56945\n",
    "#            1       0.05      0.83      0.10      2109\n",
    "\n",
    "#     accuracy                           0.48     59054\n",
    "#    macro avg       0.52      0.65      0.37     59054\n",
    "# weighted avg       0.95      0.48      0.62     59054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.list_new_feat\n",
    "'TransactionAmt_fe', 'card1_fe', 'card2_fe','card3_fe', 'card5_fe', 'addr1_fe', 'addr2_fe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression feature testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.drop(col_target, axis=1)\n",
    "X = X.drop(col_id, axis=1)\n",
    "y = df_features[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 'TransactionAmt_fe', 'card1_fe', 'card2_fe','card3_fe', 'card5_fe', 'addr1_fe', 'addr2_fe'\n",
    "# Good: addr2_fe, \n",
    "# Bad: TransactionAmt_fe, card1_fe, card2_fe, card3_fe, card5_fe\n",
    "# build a function that after a feature is created, you then have it test the feature and provide you results.. \n",
    "\n",
    "for col_original, col_new in zip(list_col, fe.list_new_feat):\n",
    "    print(col_original, col_new)\n",
    "    X[col_new] = fe.df_feat[col_new]\n",
    "    X = X.drop(col_original, axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    start_time = time.time()\n",
    "    sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1) # logistic regression\n",
    "    model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "    y_pred = model_lr_pca_sm.predict(X_test)     # predict\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "#     X = X.drop(col, axis=1)\n",
    "    print(list(X.columns))\n",
    "# RESULTS: Base: 601\n",
    "\n",
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # NEXT, TESTING groupby on C2 and isFraud...test other columns too.. specifically, D1, D2, etc. \n",
    "# # consider creating a rolling 7 day period for time between transactions and aggregating isFraud to estimate\n",
    "# # the amount spent that week against the probability that fraud happened.\n",
    "\n",
    "# # good fe: C2_fe, card2_fe, V294_fe, V317_fe, V279_fe, V306_fe\n",
    "# # bad fe: C1_fe, ProductCD_fe, V294_fe, C14_fe, card6_fe, V69_fe\n",
    "# for col in list_col_fe:\n",
    "#     X[col] = fe.df_feat[col]    \n",
    "#     print(col)\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "#     start_time = time.time()\n",
    "#     sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "#     X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "#     model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1) # logistic regression\n",
    "#     model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "#     y_pred = model_lr_pca_sm.predict(X_test)     # predict\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     print('\\nTime elapsed:', elapsed_time / 60)\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     X = X.drop(col, axis=1)\n",
    "    \n",
    "# # good: C2_fe, \n",
    "# # not good: card2_fe, C4_fe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior information \n",
    "# C2_fe\n",
    "\n",
    "# Time elapsed: 6.717598664760589\n",
    "# [[33735 23210]\n",
    "#  [  595  1514]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.59      0.74     56945\n",
    "#            1       0.06      0.72      0.11      2109\n",
    "\n",
    "#     accuracy                           0.60     59054\n",
    "#    macro avg       0.52      0.66      0.43     59054\n",
    "# weighted avg       0.95      0.60      0.72     59054\n",
    "\n",
    "# card2_fe\n",
    "\n",
    "# Time elapsed: 7.546754765510559\n",
    "# [[35200 21745]\n",
    "#  [  649  1460]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.62      0.76     56945\n",
    "#            1       0.06      0.69      0.12      2109\n",
    "\n",
    "#     accuracy                           0.62     59054\n",
    "#    macro avg       0.52      0.66      0.44     59054\n",
    "# weighted avg       0.95      0.62      0.74     59054\n",
    "\n",
    "# C4_fe\n",
    "\n",
    "# Time elapsed: 5.034457282225291\n",
    "# [[36521 20424]\n",
    "#  [  602  1507]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.64      0.78     56945\n",
    "#            1       0.07      0.71      0.13      2109\n",
    "\n",
    "#     accuracy                           0.64     59054\n",
    "#    macro avg       0.53      0.68      0.45     59054\n",
    "# weighted avg       0.95      0.64      0.75     59054\n",
    "\n",
    "# C1_fe\n",
    "\n",
    "# Time elapsed: 5.847904968261719\n",
    "# [[40085 16860]\n",
    "#  [  701  1408]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.70      0.82     56945\n",
    "#            1       0.08      0.67      0.14      2109\n",
    "\n",
    "#     accuracy                           0.70     59054\n",
    "#    macro avg       0.53      0.69      0.48     59054\n",
    "# weighted avg       0.95      0.70      0.80     59054\n",
    "\n",
    "# V317_fe\n",
    "\n",
    "# Time elapsed: 4.49170538187027\n",
    "# [[33842 23103]\n",
    "#  [  583  1526]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.59      0.74     56945\n",
    "#            1       0.06      0.72      0.11      2109\n",
    "\n",
    "#     accuracy                           0.60     59054\n",
    "#    macro avg       0.52      0.66      0.43     59054\n",
    "# weighted avg       0.95      0.60      0.72     59054\n",
    "\n",
    "# ProductCD_fe\n",
    "\n",
    "# Time elapsed: 5.576840949058533\n",
    "# [[41354 15591]\n",
    "#  [  737  1372]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.73      0.84     56945\n",
    "#            1       0.08      0.65      0.14      2109\n",
    "\n",
    "#     accuracy                           0.72     59054\n",
    "#    macro avg       0.53      0.69      0.49     59054\n",
    "# weighted avg       0.95      0.72      0.81     59054\n",
    "\n",
    "# V294_fe\n",
    "\n",
    "# Time elapsed: 4.640570282936096\n",
    "# [[37922 19023]\n",
    "#  [  698  1411]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.67      0.79     56945\n",
    "#            1       0.07      0.67      0.13      2109\n",
    "\n",
    "#     accuracy                           0.67     59054\n",
    "#    macro avg       0.53      0.67      0.46     59054\n",
    "# weighted avg       0.95      0.67      0.77     59054\n",
    "\n",
    "# V279_fe\n",
    "\n",
    "# Time elapsed: 3.3400171319643657\n",
    "# [[33443 23502]\n",
    "#  [  577  1532]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.59      0.74     56945\n",
    "#            1       0.06      0.73      0.11      2109\n",
    "\n",
    "#     accuracy                           0.59     59054\n",
    "#    macro avg       0.52      0.66      0.42     59054\n",
    "# weighted avg       0.95      0.59      0.71     59054\n",
    "\n",
    "# C14_fe\n",
    "\n",
    "# Time elapsed: 6.974740914503733\n",
    "# [[35823 21122]\n",
    "#  [  599  1510]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.63      0.77     56945\n",
    "#            1       0.07      0.72      0.12      2109\n",
    "\n",
    "#     accuracy                           0.63     59054\n",
    "#    macro avg       0.53      0.67      0.44     59054\n",
    "# weighted avg       0.95      0.63      0.74     59054\n",
    "\n",
    "# card6_fe\n",
    "\n",
    "# Time elapsed: 3.6439321478207907\n",
    "# [[36705 20240]\n",
    "#  [  640  1469]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.64      0.78     56945\n",
    "#            1       0.07      0.70      0.12      2109\n",
    "\n",
    "#     accuracy                           0.65     59054\n",
    "#    macro avg       0.53      0.67      0.45     59054\n",
    "# weighted avg       0.95      0.65      0.76     59054\n",
    "\n",
    "# V306_fe\n",
    "\n",
    "# Time elapsed: 2.792719868818919\n",
    "# [[34359 22586]\n",
    "#  [  592  1517]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.60      0.75     56945\n",
    "#            1       0.06      0.72      0.12      2109\n",
    "\n",
    "#     accuracy                           0.61     59054\n",
    "#    macro avg       0.52      0.66      0.43     59054\n",
    "# weighted avg       0.95      0.61      0.73     59054\n",
    "\n",
    "# V69_fe\n",
    "\n",
    "# Time elapsed: 5.5301028688748675\n",
    "# [[39833 17112]\n",
    "#  [  694  1415]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.70      0.82     56945\n",
    "#            1       0.08      0.67      0.14      2109\n",
    "\n",
    "#     accuracy                           0.70     59054\n",
    "#    macro avg       0.53      0.69      0.48     59054\n",
    "# weighted avg       0.95      0.70      0.79     59054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.drop('card2_fe',axis=1)\n",
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "for col in list_col_fe:\n",
    "    X[col] = fe.df_feat[col]    \n",
    "    print(col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    start_time = time.time()\n",
    "    # SMOTE\n",
    "    sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    # fit decision tree\n",
    "    model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "    model_dt_pca_smote.fit(X_train_res, y_train_res)\n",
    "    # predict\n",
    "    y_pred = model_dt_pca_smote.predict(X_train)\n",
    "    # time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    X = X.drop(col, axis=1)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decision tree\n",
    "for col in list_col_fe:\n",
    "    X[col] = fe.df_feat[col]    \n",
    "    print(col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    start_time = time.time()\n",
    "    # applying SMOTE\n",
    "    sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    # fit decision tree\n",
    "    model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "    model_dt_pca_smote.fit(X_train_res, y_train_res)\n",
    "    # predict\n",
    "    y_pred = model_dt_pca_smote.predict(X_test)\n",
    "    # time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    X = X.drop(col, axis=1)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('\\nTime elapsed:', elapsed_time / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree Base\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "start_time = time.time()\n",
    "# applying SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "# fit decision tree\n",
    "model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "model_dt_pca_smote.fit(X_train_res, y_train_res)\n",
    "# predict\n",
    "y_pred = model_dt_pca_smote.predict(X_test)\n",
    "# time\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)\n",
    "\n",
    "# [[55791  1154]\n",
    "#  [  941  1168]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.98      0.98      0.98     56945\n",
    "#            1       0.50      0.55      0.53      2109\n",
    "\n",
    "#     accuracy                           0.96     59054\n",
    "#    macro avg       0.74      0.77      0.75     59054\n",
    "# weighted avg       0.97      0.96      0.97     59054\n",
    "\n",
    "\n",
    "# Time elapsed: 8.485406096776327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit logistic regression\n",
    "# model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "# # predict\n",
    "# y_pred = model_lr_pca_sm.predict(X_train2)\n",
    "# # time\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print('\\nTime elapsed:', elapsed_time / 60)\n",
    "# print(confusion_matrix(y_train2, y_pred))\n",
    "# print(classification_report(y_train2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT: C4_fe with TransactionAmt\n",
    "\n",
    "# RESULT: Base Score, smote only, no pca\n",
    "# [[245073 267859]\n",
    "#  [  3447  15107]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.48      0.64    512932\n",
    "#            1       0.05      0.81      0.10     18554\n",
    "\n",
    "#     accuracy                           0.49    531486\n",
    "#    macro avg       0.52      0.65      0.37    531486\n",
    "# weighted avg       0.95      0.49      0.62    531486\n",
    "\n",
    "# Scores below reflect _fe's\n",
    "# RESULTS: drop card6_fe\n",
    "# Time elapsed: 3.520830734570821\n",
    "# [[361114 151818]\n",
    "#  [  4823  13731]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.70      0.82    512932\n",
    "#            1       0.08      0.74      0.15     18554\n",
    "\n",
    "#     accuracy                           0.71    531486\n",
    "#    macro avg       0.53      0.72      0.49    531486\n",
    "# weighted avg       0.96      0.71      0.80    531486\n",
    "\n",
    "# RESULTS: card6_fe \n",
    "# Time elapsed: 3.569287049770355\n",
    "# [[362083 150849]\n",
    "#  [  4745  13809]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.71      0.82    512932\n",
    "#            1       0.08      0.74      0.15     18554\n",
    "\n",
    "#     accuracy                           0.71    531486\n",
    "#    macro avg       0.54      0.73      0.49    531486\n",
    "# weighted avg       0.96      0.71      0.80    531486\n",
    "\n",
    "# RESULTS: dropped features and used PCA, worstened score\n",
    "# [[413977  98955]\n",
    "#  [  4747  13807]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.81      0.89    512932\n",
    "#            1       0.12      0.74      0.21     18554\n",
    "\n",
    "#     accuracy                           0.80    531486\n",
    "#    macro avg       0.56      0.78      0.55    531486\n",
    "# weighted avg       0.96      0.80      0.87    531486\n",
    "\n",
    "# RESULTS: drop low ranking features found in decision tree\n",
    "# [[329812 183120]\n",
    "#  [  4810  13744]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.64      0.78    512932\n",
    "#            1       0.07      0.74      0.13     18554\n",
    "\n",
    "#     accuracy                           0.65    531486\n",
    "#    macro avg       0.53      0.69      0.45    531486\n",
    "# weighted avg       0.95      0.65      0.76    531486\n",
    "\n",
    "# RESULTS: without PCA\n",
    "# [[298715 214217]\n",
    "#  [  3509  15045]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.58      0.73    512932\n",
    "#            1       0.07      0.81      0.12     18554\n",
    "\n",
    "#     accuracy                           0.59    531486\n",
    "#    macro avg       0.53      0.70      0.43    531486\n",
    "# weighted avg       0.96      0.59      0.71    531486\n",
    "\n",
    "# RESULTS: keep ohe for email and create p email feature calculate email domain fraud feature\n",
    "# [[434421  78511]\n",
    "#  [  4429  14125]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.84    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.84      0.89    531486\n",
    "\n",
    "# RESULTS: with new feature, try dropping ohe p_email and test\n",
    "# [[435499  77433]\n",
    "#  [  4539  14015]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.85    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.85      0.89    531486\n",
    "\n",
    "# RESULTS: cut FP rate quite a bit. fixing fraud perc calculation by doing fraud/non fraud for each value in each column\n",
    "# [[434423  78509]\n",
    "#  [  4430  14124]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.85      0.91    512932\n",
    "#            1       0.15      0.76      0.25     18554\n",
    "\n",
    "#     accuracy                           0.84    531486\n",
    "#    macro avg       0.57      0.80      0.58    531486\n",
    "# weighted avg       0.96      0.84      0.89    531486\n",
    "\n",
    "\n",
    "# RESULTS: adding 60 for one hot encoding\n",
    "# [[403339 109593]\n",
    "#  [  4393  14161]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.79      0.88    512932\n",
    "#            1       0.11      0.76      0.20     18554\n",
    "\n",
    "#     accuracy                           0.79    531486\n",
    "#    macro avg       0.55      0.77      0.54    531486\n",
    "# weighted avg       0.96      0.79      0.85    531486\n",
    "\n",
    "# RESULTS: fe for 'addr1', 'addr2', 'card1', 'card2', 'card3', 'card5'\n",
    "# [[397116 115816]\n",
    "#  [  4490  14064]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.77      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: only addr6. Creating ratio ranking of higher risk areas for fraud. \n",
    "# [[396866 116066]\n",
    "#  [  4534  14020]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# NEXT, test creating iqr range instead of percentage values, instead of what we did\n",
    "# with addr5, then create one for addr2, then create an addr7 based on interaction with addr2. \n",
    "\n",
    "\n",
    "\n",
    "# RESULTS: only addr5. testing mapping percentage values transformed..\n",
    "# [[396898 116034]\n",
    "#  [  4535  14019]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with only addr3, addr4\n",
    "# [[396868 116064]\n",
    "#  [  4544  14010]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: w/o addr1, addr2, addr3, addr4\n",
    "# [[396629 116303]\n",
    "#  [  4545  14009]]\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.76      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with only addr1, addr2\n",
    "# [[396803 116129]\n",
    "#  [  4555  13999]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.75      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486\n",
    "\n",
    "# RESULTS: with addr1, addf2, addr3, addr4\n",
    "# [[396877 116055]\n",
    "#  [  4549  14005]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.77      0.87    512932\n",
    "#            1       0.11      0.75      0.19     18554\n",
    "\n",
    "#     accuracy                           0.77    531486\n",
    "#    macro avg       0.55      0.76      0.53    531486\n",
    "# weighted avg       0.96      0.77      0.84    531486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - SMOTE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = fe.df_feat.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree smote only (pca commented out)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# # applying PCA\n",
    "# scaled_X = StandardScaler().fit_transform(X)\n",
    "# # pca = PCA(n_components=250)\n",
    "# pca = PCA()\n",
    "# pcomponents = pca.fit_transform(scaled_X)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "# applying SMOTE\n",
    "sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train2, y_train2)\n",
    "\n",
    "model_dt_pca_smote = DecisionTreeClassifier(random_state=42)\n",
    "model_dt_pca_smote.fit(X_train_res, y_train_res)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\nTime elapsed:', elapsed_time / 60)\n",
    "\n",
    "# Feature importance\n",
    "col_name = pd.Series(X.columns, name='col')\n",
    "col_feat_rank = pd.Series(model_dt_pca_smote.feature_importances_, name='feat_rank')\n",
    "df_feat_rank = pd.concat([col_name, col_feat_rank], axis=1).sort_values('feat_rank', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(5,6))\n",
    "sns.barplot(df_feat_rank.feat_rank[0:10], df_feat_rank.col[0:10], palette='Blues_d')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "df_feat_rank[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EDA on C4 and TransactionAmt\n",
    "# df_temp = fe.df_feat\n",
    "\n",
    "# sns.lineplot(x='C4', y='TransactionAmt', data=df_temp)\n",
    "# plt.title('C4 versus Transaction Amount')\n",
    "# plt.show()\n",
    "\n",
    "# sns.scatterplot(x='C4', y='TransactionAmt', hue='isFraud', data=df_temp)\n",
    "# plt.title('C4 versus Transaction Amount')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the type of card is highly correllated with debit, credit, etc. figure out which feature to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_lr_pca_sm.predict(X_pca)\n",
    "\n",
    "# cross validation\n",
    "# result = cross_val_score(model_lr_pca_sm, X_train2, y_train2, cv=10, n_jobs=-1, scoring='recall_score')\n",
    "# result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes, do we need to visualize our results?? We need to look at our confusion matrix\n",
    "# and decide how to improve our results from here... Test and see what happens if we \n",
    "# increase our principal components... test removing columns, adding columns, imputing\n",
    "# certain columns all together. \n",
    "\n",
    "# test imputing 500 or less for ohe. Then test dropping card1 due to its number\n",
    "# of unique values that would make our data very high dimensional. We can try only ohe for\n",
    "# the entire dataset to see how our model performs over all... Though.. it will likely run \n",
    "# out of memory and crash. \n",
    "\n",
    "# ALSO test probabilities on logisticregression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.rcParams['font.size'] = 14\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "# plt.hist(y_pred_prob[:,1], bins=8)\n",
    "# plt.xlim(0,1)\n",
    "# plt.title(\"Histogram of Probability of Fraud\")\n",
    "# plt.xlabel(\"Predicted probability of Fraud\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "# y_pred_class = binarize(y_pred_prob, 0.5)[:,1]\n",
    "\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:,1])\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.title(\"ROC curve for fraud detection classifier\")\n",
    "# plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "# plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_threshold(threshold):\n",
    "#     print(\"Sensitivity:\", tpr[thresholds > threshold][-1])\n",
    "#     print(\"Specificity:\", 1 - fpr[thresholds > threshold][-1],'\\n')\n",
    "    \n",
    "# evaluate_threshold(.5)\n",
    "# evaluate_threshold(.2)\n",
    "# evaluate_threshold(.1)\n",
    "\n",
    "# print(roc_auc_score(y, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_test2) # actual prediction test data set\n",
    "# print('y_pred_actual on test set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_actual))\n",
    "# print(classification_report(y_test2, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_test2)\n",
    "# y_pred_class = binarize(y_pred_proba, 0.5)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_class))\n",
    "# print(classification_report(y_test2, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_pca) # actual full data set\n",
    "# print('Logistic Regression')\n",
    "# print('y_pred_actual full data set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y, y_pred_actual))\n",
    "# print(classification_report(y, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba full data set\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_pca)#[:,1]#[0:10]\n",
    "# y_pred_class = binarize(y_pred_proba, 0.2)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # predicting on test set w/o PCA\n",
    "# print(\"Predicting using Logistic Regression, PCA, SMOTE\\n\")\n",
    "# y_pred_pca_sm = model_lr_pca.predict(X_test2)\n",
    "# # y_pred_test1 = model_lr.predict(X_test1)\n",
    "# print(\"Test set:\")\n",
    "# print(\"Validation results\")\n",
    "# print(model_lr_pca.score(X_test2, y_test2))\n",
    "# print(recall_score(y_test2, y_pred_pca_sm))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y_test2, y_pred_pca_sm))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y_test2, y_pred_pca_sm))\n",
    "\n",
    "# # predicting on original dataset\n",
    "# print(\"Whole dataset:\")\n",
    "# y_pred_pca_sm_whole = model_lr_pca.predict(X_pca)\n",
    "# print(\"\\nTest Results\")\n",
    "# print(model_lr_pca.score(X_pca, y))\n",
    "# print(recall_score(y, y_pred_pca_sm_whole))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred_pca_sm_whole))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred_pca_sm_whole))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('dataframe length: ' + str(df_features.shape[0]))\n",
    "# print('TransactionDT unique: ' + str(len(df_features.TransactionDT.unique())))\n",
    "# print('is not fraud: ' + str(df_features[df_features.isFraud==0].shape[0]))\n",
    "# print('is fraud: ' + str(df_features[df_features.isFraud==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # KEEP\n",
    "# fig = plt.figure(figsize=(15,4))\n",
    "# df_temp = df_features\n",
    "# sns.kdeplot(df_temp.loc[(df_temp.isFraud==0), 'TransactionDT'], color='b', shade=True, label='Not Fraud')\n",
    "# sns.kdeplot(df_temp.loc[(df_temp.isFraud==1), 'TransactionDT'], color='r', shade=True, label='Fraud')\n",
    "# plt.title('Transaction Date Versus Fraud')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(x='TransactionDT', y='TransactionDT', hue='isFraud', data=df_features)\n",
    "# plt.title(\"Transaction Date Versus Fraud\")\n",
    "# plt.show()\n",
    "\n",
    "# we want to figure out how to create more features from TransactionDT.. \n",
    "# are certain transactions more likely. in general, to be fraudualant around a certain day? \n",
    "# we have transactionID yet we dont have the specific card of who it belongs to.. \n",
    "# if card1 is the unique identifier... and we did a groupby on fraud.. \n",
    "# we have average spent per day as an option... \n",
    "# create a feature that is average spent per day for non fraud versus average spent per day for fraud.. \n",
    "# what would the describe method reveal for us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction date is the most highly correllated.. another correlation is transaction amount..\n",
    "# our goal is to create another feature that is highly correllated almost or more than TransactionDT. \n",
    "# we want to increase the strength.. What feature can we create that would boost the accuracy...\n",
    "# what feature can we create that exposes more truth in transactionDT? Would it be a specific date?\n",
    "# would it be the average amount over each week or month... ? what about a specific dates transaction\n",
    "# would help us...? We know taking average amounts spent can help us... So if we take monthly spending..\n",
    "# we can tell the algorithm that during certain months the transaction likelihood of fraud is higher.. \n",
    "# we must think up a solution before implementing it, we want to think out the best use of our time.. \n",
    "# We might perform EDA to see if specific time chunks do in fact have higher fraud counts.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_drop_col_1 = list(df_feat_rank[df_feat_rank.feat_rank < .0015]['col'])\n",
    "# list_drop_col_2 = list(df_feat_rank[df_feat_rank.feat_rank < .0015]['col'])\n",
    "# list_drop_col_3 = list(df_feat_rank[df_feat_rank.feat_rank < .01]['col'])\n",
    "\n",
    "# print(len(list_drop_col_1))\n",
    "# print(len(list_drop_col_2))\n",
    "# print(len(list_drop_col_3))\n",
    "\n",
    "# list_drop_col_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predicting on test set w/o PCA\n",
    "# print(\"Predicting using PCA\\n\")\n",
    "# y_pred_pca = model_dt_pca_smote.predict(X_test2)\n",
    "# # y_pred_test1 = model_lr.predict(X_test1)\n",
    "# print(\"Test set:\")\n",
    "# print(\"Validation results\")\n",
    "# print(model_dt_pca_smote.score(X_test2, y_test2))\n",
    "# print(recall_score(y_test2, y_pred_pca))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y_test2, y_pred_pca))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y_test2, y_pred_pca))\n",
    "\n",
    "# # predicting on original dataset\n",
    "# print(\"Whole dataset:\")\n",
    "# y_pred_pca = model_dt_pca_smote.predict(X_pca)\n",
    "# print(\"\\nTest Results\")\n",
    "# print(model_dt_pca_smote.score(X_pca, y))\n",
    "# print(recall_score(y, y_pred_pca))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred_pca))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Variance ratio:\")\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(\"\\nPrincipal components explained:\")\n",
    "# pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explaining variance\n",
    "# print('Variance ratio:')\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # interpreting principal components\n",
    "# print('\\nPrincipal components explained:')\n",
    "# pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1', 'PC2'))\n",
    "\n",
    "# # # predicting on original dataset\n",
    "# # y_pred = clf_lr.predict(X)\n",
    "# # print(\"\\nTest Results\")\n",
    "# # print(clf_lr.score(X, y))\n",
    "# # print(recall_score(y, y_pred))\n",
    "# # print(\"\\nConfusion Matrix\")\n",
    "# # print(confusion_matrix(y, y_pred))\n",
    "# # print('\\nClassification Report:\\n')\n",
    "# # print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X2 = StandardScaler().fit_transform(X)\n",
    "# pca3 = PCA(n_components=275)\n",
    "# pcomponents = pca3.fit_transform(scaled_X2)\n",
    "# X_pca = pd.DataFrame(data=pcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # applying SMOTE to train set to correct class imbalance\n",
    "# sm = SMOTE(random_state=42, ratio = 1.0, n_jobs=-1)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "# # fitting to residuals created by SMOTE\n",
    "# clf_lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "# clf_lr.fit(X_train_res, y_train_res);\n",
    "\n",
    "# # predicting on test set\n",
    "# y_test_pred = clf_lr.predict(X_test)\n",
    "# print(\"Validation results\")\n",
    "# print(clf_lr.score(X_test, y_test))\n",
    "# print(recall_score(y_test, y_test_pred))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y_test, y_test_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# # predicting on original dataset\n",
    "# y_pred = clf_lr.predict(X)\n",
    "# print(\"\\nTest Results\")\n",
    "# print(clf_lr.score(X, y))\n",
    "# print(recall_score(y, y_pred))\n",
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nConfusion Matrix\")\n",
    "# print(confusion_matrix(y, y_pred))\n",
    "# print('\\nClassification Report:\\n')\n",
    "# print(classification_report(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
