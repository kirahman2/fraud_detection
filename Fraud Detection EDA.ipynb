{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#import your libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/krahman/work/fraud_detection/data/train_identity.csv')\n",
    "# merging dataframes \n",
    "df_raw = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "df_train = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning columns to specific lists (cat, num, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 220\n"
     ]
    }
   ],
   "source": [
    "# dropping columns with more than 50% missing data\n",
    "length_df = df_train.shape[0]/2\n",
    "list_temp = []\n",
    "for val in df_train.columns:\n",
    "    if np.sum(df_train[val].isnull()) > length_df:\n",
    "        list_temp.append(val)   \n",
    "df_train = df_train.drop(list_temp, axis=1)\n",
    "\n",
    "###################################\n",
    "# c is num, ex, how many addresses associated with card\n",
    "col_c = [c for c in df_train.columns if c.startswith('C') and (len(c)==2 or len(c)==3)]\n",
    "# d is num, time/days between transactions\n",
    "col_d = [d for d in df_train.columns if d.startswith('D') and (len(d)==2 or len(d)==3)]\n",
    "# m is date of transaction\n",
    "col_m = [m for m in df_train.columns if m.startswith('M') and (len(m)==2 or len(m)==3)]\n",
    "# v is num, features created by vesta such as ranking, counting. entity relationships, etc. \n",
    "col_v = [v for v in df_train.columns if v.startswith('V') and (len(v)==2 or len(v)==3 or len(v)==4)]\n",
    "# i is identity information like network and digital signature associated with transaction\n",
    "col_i = [i for i in df_train.columns if i.startswith('id_') and len(i)==5]\n",
    "# ca is cat, card information such as card type, etc. \n",
    "col_card = [ca for ca in df_train.columns if ca.startswith('card')]\n",
    "\n",
    "# column id and target\n",
    "col_id = ['TransactionID']\n",
    "col_target = 'isFraud'\n",
    "\n",
    "# converting categorical columns with numerical values to string types.\n",
    "col_cat_to_obj = ['addr1','addr2','card1','card2', 'card3', 'card5']\n",
    "for val in col_cat_to_obj:\n",
    "    df_train[val] = df_train[val].astype(str)\n",
    "\n",
    "# categorical columns\n",
    "col_cat = ['addr1','addr2','ProductCD',\"P_emaildomain\"] + col_card + col_m\n",
    "\n",
    "# C counter, D is time elapsed between transactions, V feature engineered variables by firm\n",
    "col_num = ['TransactionAmt'] + col_c + col_d + col_v \n",
    "col_num.append(col_target)\n",
    "\n",
    "# figure out how to handle this. What do these dates mean? Do certain dates have more fraud occurences?\n",
    "col_date = ['TransactionDT'] \n",
    "\n",
    "# confirming all columns are accounted for\n",
    "print('Total columns: ' + str(len(col_cat + col_num + col_date + col_id + col_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        '''initialize variables and column names for null dataframe'''\n",
    "        self.df_train = df_train.copy()\n",
    "        self.list_col = []\n",
    "        self.list_total = []\n",
    "        self.dict_unique = {}\n",
    "        self.list_datatype = []\n",
    "        self.list_unique_val = []\n",
    "        self.list_mode_count = []\n",
    "        self.list_mode_value = []\n",
    "        self.list_mode_count_perc = []\n",
    "        self.list_unique_total = []\n",
    "        self.list_unique_first_10 = []\n",
    "        self.column_names = ['col_name', 'total_null', 'datatype', 'total_unique',\n",
    "                             'mode_value', 'mode_count', 'mode_percentage']\n",
    "\n",
    "    def missing_values(self):\n",
    "        '''check for null values and add to null dataframe if more than 0 nulls exist'''\n",
    "        for val in df_train.columns:\n",
    "            total_null = np.sum(df_train[val].isnull())\n",
    "            if total_null > 0:\n",
    "                self.list_col.append(val)\n",
    "                self.list_total.append(total_null)\n",
    "                self.list_datatype.append(df_train[val].dtype)\n",
    "                self.list_unique_total.append(len(df_train[val].unique()))\n",
    "                self.list_unique_val.append(df_train[val].unique())\n",
    "                self.list_mode_value.append(df_train[val].mode()[0])\n",
    "                val_counts = max(df_train[val].value_counts())\n",
    "                self.list_mode_count.append(val_counts)\n",
    "                self.list_mode_count_perc.append(val_counts/len(df_train))\n",
    "                val_unique = df_train[val].unique()\n",
    "                self._create_dict(val_unique, df_train, val)\n",
    "        df_null_info = self._create_dataframe()\n",
    "        df_null_info = self._create_df_unique(df_null_info)\n",
    "        self._summary(df_null_info)\n",
    "        self._fillna(df_null_info)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _create_dict(self, val_unique, df_train, val):\n",
    "        '''create dictionary of unique values for each column'''\n",
    "        if (len(val_unique) > 99) and isinstance(df_train[val], object):  \n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if (len(val_unique) > 99) and not isinstance(df_train[val], object):\n",
    "            self.dict_unique.update([(val,0)])\n",
    "        if len(val_unique) < 100:\n",
    "            self.dict_unique.update([(val, val_unique)])\n",
    "\n",
    "    def _create_dataframe(self):\n",
    "        '''create main dataframe'''\n",
    "        df_null_info = pd.DataFrame()\n",
    "        counter = -1\n",
    "        for list_val in [self.list_col, self.list_total, self.list_datatype, self.list_unique_total,\n",
    "                        self.list_mode_value, self.list_mode_count, self.list_mode_count_perc]:\n",
    "            counter = counter + 1\n",
    "            col_title = self.column_names[counter]\n",
    "            df = pd.DataFrame(list_val, columns=[col_title])\n",
    "            df_null_info = pd.concat([df_null_info, df], axis=1)\n",
    "        return df_null_info\n",
    "    \n",
    "    def _summary(self, df_null_info):\n",
    "        val = df_null_info.shape[0]\n",
    "        print('There were ' + str(val) + ' columns with null values.')\n",
    "    \n",
    "    def _create_df_unique(self, df_null_info):\n",
    "        '''create unique values dataframe'''\n",
    "        series_unique = pd.Series(self.dict_unique)\n",
    "        df_unique = pd.DataFrame(series_unique).reset_index()\n",
    "        df_unique = df_unique.rename(columns={'index':'col_name', 0:'unique'})\n",
    "        df_null_info = df_null_info.merge(df_unique, how='left', \n",
    "                                          left_on='col_name', right_on='col_name')\n",
    "        df_null_info.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_null_info.csv')\n",
    "        return df_null_info\n",
    "    \n",
    "    def _fillna(self, df_null_info):\n",
    "        '''fill null values of df_train with mode'''\n",
    "        total_null_columns = sum(np.sum(self.df_train.isnull()))\n",
    "        if total_null_columns > 0:\n",
    "            for val in df_null_info.col_name:\n",
    "                val_mode = self.df_train[val].mode()[0]\n",
    "                self.df_train[val] = self.df_train[val].fillna(val_mode)\n",
    "    \n",
    "    def impute_features(self):\n",
    "        df_temp = pp.df_train\n",
    "        for val in col_cat:\n",
    "            total_unique_val = pp.df_train[val].unique().shape[0]\n",
    "            if len(df_temp[val].unique()) < 60:\n",
    "                print('dummies encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "                df_dumm = pd.get_dummies(df_temp[val], prefix=val, drop_first=True)\n",
    "                df_temp = df_temp.drop(val,axis=1)\n",
    "                df_temp = pd.concat([df_temp, df_dumm], axis=1)\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df_temp[val] = le.fit_transform(df_temp[val])\n",
    "                print('label encoded: ' + str(val) + ' unique ' + str(total_unique_val))\n",
    "        print('new dataframe shape:' + str(df_temp.shape))\n",
    "        return df_temp\n",
    "\n",
    "pp = Preprocessing()\n",
    "df_null_info = pp.missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.df_train = pp.impute_features()\n",
    "pp.df_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_imputed.csv')\n",
    "df_features = df_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create features, target and split the dataframe\n",
    "# X = pp.df_train.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = pp.df_train[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after running final_features, run create_final_df.\n",
      "after running final_features, run create_final_df.\n",
      "keeping original feature card5\n",
      "keeping original feature V317\n",
      "keeping original feature V69\n",
      "keeping original feature D1\n",
      "keeping original feature D3\n",
      "keeping original feature D4\n",
      "keeping original feature D11\n",
      "dropping columns:  ['addr1', 'addr2', 'card2', 'card3', 'C1', 'V294', 'V279', 'C14', 'V306', 'D2', 'D10', 'C4']\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "bool_apply_pca set to false.\n",
      "bool_apply_pca set to false.\n",
      "creating tuning dataframe...\n",
      "downsampling applied.\n",
      "smote applied.\n",
      "final dataframe created.\n",
      "To test a model use the mod.create_df_score_model(model_current)\n",
      "method where, for example, model_current=LogisticRegression().\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineering():\n",
    "    '''create new features for columns without ordinal values'''\n",
    "    def __init__(self):\n",
    "        self.list_fraud_perc = []\n",
    "        self.df_feat = df_features.copy()\n",
    "        self.df_raw = df_raw.copy()\n",
    "        \n",
    "        self.len_df_feat = self.df_feat.shape[0]\n",
    "        self.dict_all_feat = {}\n",
    "        self.new_col = []\n",
    "        \n",
    "        self.col = []\n",
    "        self.col_fe = []\n",
    "        self.df_new_feat = pd.DataFrame()\n",
    "        self.list_drop_col = []\n",
    "        self.str_list_col_fe = []\n",
    "        self.list_feat = []\n",
    "\n",
    "    def feature_testing(self, bool_drop_col, list_feat):\n",
    "        '''testing and scoring new potential features'''\n",
    "        print(\"While running feature_testing, do not run final_features.\")            \n",
    "        if list_feat:\n",
    "            for col in list_feat:\n",
    "                self.col_fe = col\n",
    "                bool_predict_proba = False\n",
    "                if col in df_features.columns:\n",
    "                    df_feat = self.create_test_feature(bool_drop_col, col)\n",
    "                    if df_feat_1000:\n",
    "                        df_feat = df_feat[0:1000]\n",
    "                    df_feat = df_feat.drop(self.list_drop_col[-1], axis=1)\n",
    "                    self._apply_df_transform(df_feat)\n",
    "                    model_lr = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "                    self._convert_list_to_string(list_feat)\n",
    "                    mod.create_df_score_model(model_lr)\n",
    "                else:\n",
    "                    print(\"\\nColumn\", col, \"does not exist in dataframe.\\n\")\n",
    "            self.col_fe = []\n",
    "        self.list_drop_col = []\n",
    "            \n",
    "    def final_features(self, bool_drop_col, list_feat):\n",
    "        '''creates final new features'''\n",
    "        print('after running final_features, run create_final_df.')\n",
    "        self.list_feat = list_feat\n",
    "        df_feat = self.create_feature(bool_drop_col, list_feat)  \n",
    "        if df_feat_1000:\n",
    "            df_feat = df_feat[0:1000]\n",
    "        for col in list_feat:\n",
    "            col_fe = self._append_col_lists(col)\n",
    "            df_feat[col] = self._fill_na(df_feat, col_fe)\n",
    "            self._concat_df_new_feat(df_feat, col_fe)\n",
    "        self._convert_list_to_string(list_feat)\n",
    "        return df_feat ### delete?\n",
    "    \n",
    "    def _append_col_lists(self, col):\n",
    "        '''appending columns and new feature column names'''\n",
    "        col_fe = col + '_fe'\n",
    "        self.col.append(col)\n",
    "        self.col_fe.append(col_fe)\n",
    "        return col_fe\n",
    "\n",
    "    def _fill_na(self, df_feat, col_fe):\n",
    "        '''fill na values for new features'''\n",
    "        col_mode = df_feat[col_fe].mode()[0]\n",
    "        return df_feat[col_fe].fillna(col_mode)\n",
    "        \n",
    "    def _concat_df_new_feat(self, df_feat, col_fe):\n",
    "        '''adding new feauture columns to one dataframe'''  \n",
    "        df_temp = df_feat[col_fe]\n",
    "        self.df_new_feat = pd.concat([self.df_new_feat, df_temp], axis=1)\n",
    "    \n",
    "    def _convert_list_to_string(self, list_feat):\n",
    "        '''convert list to string to print later'''\n",
    "        str_temp = ''\n",
    "        for val in list_feat:\n",
    "            str_temp = str_temp + val + ' '\n",
    "        self.str_list_col_fe = str_temp\n",
    "\n",
    "    def create_final_df(self):\n",
    "        '''creates final dataframe after creating final_features'''\n",
    "        df_feat = pd.concat([df_features, self.df_new_feat], axis=1)\n",
    "        if df_feat_1000:\n",
    "            df_feat = pd.concat([df_features[0:1000], self.df_new_feat], axis=1)\n",
    "        print('dropping columns: ', self.list_drop_col)\n",
    "        df_feat = df_feat.drop(self.list_drop_col, axis=1)\n",
    "        \n",
    "        self._apply_df_transform(df_feat)\n",
    "        \n",
    "        self._create_tuning_df(df_feat)\n",
    "        self.list_drop_col = []\n",
    "        self._final_df_summary()\n",
    "        \n",
    "    def _shuffle_df(self, X, y):\n",
    "        '''shuffle dataframe'''\n",
    "        y = pd.Series(y)\n",
    "        X = pd.DataFrame(X)\n",
    "        df_temp = pd.concat([X, y], keys=['features','target'], axis=1)\n",
    "        df_temp = shuffle(df_temp).reset_index(drop=True)\n",
    "        X = df_temp.features\n",
    "        y = df_temp.target\n",
    "        return X, y\n",
    "        \n",
    "    def _final_df_summary(self):\n",
    "        print(\"final dataframe created.\")\n",
    "        print(\"To test a model use the mod.create_df_score_model(model_current)\")\n",
    "        print(\"method where, for example, model_current=LogisticRegression().\")\n",
    "\n",
    "    def _apply_df_transform(self, df_feat):\n",
    "        '''create dataframe, apply pca, apply smote'''\n",
    "        self.df_feat = df_feat\n",
    "        X, y = self._drop_col_id_target(df_feat)\n",
    "        X_train, X_test, y_train, y_test = self._split_dataframe(X, y)\n",
    "        X_train, y_train = self._apply_downsampling(X_train, y_train) # apply only train set\n",
    "        X_train, y_train = self._apply_smote(X_train, y_train)        # apply only train set\n",
    "        \n",
    "        X_train, y_train = self._shuffle_df(X_train, y_train)\n",
    "        \n",
    "        mod.X_train, mod.y_train = self._apply_pca(X_train, y_train)          # apply to train set\n",
    "        mod.X_test , mod.y_test = self._apply_pca(X_test, y_test) \n",
    "        \n",
    "#         X_train, y_train = self._apply_pca(X_train, y_train)          # apply to train set\n",
    "#         X_test, y_test = self._apply_pca(X_test, y_test)              # apply to test set\n",
    "#         mod.X_train, mod.y_train = X_train, y_train\n",
    "#         mod.X_test , mod.y_test = X_test, y_test\n",
    "\n",
    "    def _create_tuning_df(self, df_feat):\n",
    "        '''whole dataframe used for model tuning'''\n",
    "        if bool_create_tuning_df:\n",
    "            print(\"creating tuning dataframe...\")\n",
    "            X, y = self._drop_col_id_target(df_feat)\n",
    "            X, y = self._apply_downsampling(X, y)\n",
    "            X, y = self._apply_smote(X, y)\n",
    "            mod.X_features, mod.y_target = self._shuffle_df(X, y)\n",
    "        else:\n",
    "            print('bool_create_tuning_df set to false.')\n",
    "\n",
    "    def _split_dataframe(self, X, y):\n",
    "        '''splitting dataframe into training and test set'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size=0.1, \n",
    "                                                            random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def _drop_col_id_target(self, df_feat):\n",
    "        '''dropping col id and target from features and creating target dataframe'''\n",
    "        X = df_feat.drop(col_target, axis=1)\n",
    "        X = X.drop(col_id, axis=1)\n",
    "        y = df_feat[col_target]\n",
    "        return X, y\n",
    "    \n",
    "    def _apply_downsampling(self, X, y):\n",
    "        '''down sampling majority class'''\n",
    "        if bool_apply_downsampling:\n",
    "            len_y_one = len(y[y==1])\n",
    "            sampler = RandomUnderSampler(random_state=42, ratio={0:95000, 1:len_y_one})\n",
    "            X, y = sampler.fit_sample(X, y)\n",
    "            print(\"downsampling applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_downsampling set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_smote(self, X, y):\n",
    "        '''applying smote to split training set'''\n",
    "        if bool_apply_smote:\n",
    "            sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "            X, y = sm.fit_sample(X, y)\n",
    "            print(\"smote applied.\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_smote set to false.\")\n",
    "            return X, y\n",
    "            \n",
    "    def _apply_pca(self, X, y):\n",
    "        '''applying PCA and creating train and test set'''\n",
    "        if bool_apply_pca:\n",
    "            X = self._pca(X)\n",
    "            print('pca applied to training set, then test set.')\n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"bool_apply_pca set to false.\")\n",
    "            return X, y\n",
    "\n",
    "    def _pca(self, X):\n",
    "        '''applying pca features dataframe'''\n",
    "        scaled_X = StandardScaler().fit_transform(X)\n",
    "        pca = PCA(n_components=250) #set value\n",
    "        pcomponents = pca.fit_transform(scaled_X)\n",
    "        X_pca = pd.DataFrame(data=pcomponents)\n",
    "        return X_pca\n",
    "        \n",
    "    def create_test_feature(self, bool_drop_col, col):\n",
    "        '''creates correllated ratio to target column'''\n",
    "        df_feat = df_features.copy()        \n",
    "        df_feat = self._calculate_target_perc(col, df_feat) \n",
    "        df_feat = self._map_col(col, df_feat)\n",
    "        df_feat = self._create_ratio(df_feat)\n",
    "        df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat\n",
    "    \n",
    "    def create_feature(self, bool_drop_col, list_col):\n",
    "        '''creating new feature'''\n",
    "        df_feat = self.df_feat       \n",
    "        for col in list_col:\n",
    "            df_feat = self._check_col_exist(col, df_feat)\n",
    "            df_feat = self._calculate_target_perc(col, df_feat) \n",
    "            df_feat = self._map_col(col, df_feat)\n",
    "            df_feat = self._create_ratio(df_feat)\n",
    "            df_feat = self._drop_column(bool_drop_col, col, df_feat)\n",
    "        return df_feat \n",
    "    \n",
    "    def _check_col_exist(self, col, df_feat):\n",
    "        '''recreates original column from original dataframe'''\n",
    "        if col not in df_feat.columns:\n",
    "            df_feat[col] = df_raw[col]\n",
    "            df_feat[col] = self._fill_na(df_feat, col)\n",
    "            df_feat[col] = self._label_encode(df_feat, col)\n",
    "        return df_feat\n",
    "    \n",
    "    def _label_encode(self, df_feat, col):\n",
    "        '''label encoding columns pulled from original df_raw'''\n",
    "        le = LabelEncoder()\n",
    "        df_feat[col] = le.fit_transform(df_feat[col])\n",
    "        return df_feat[col]\n",
    "    \n",
    "    def _drop_column(self, bool_drop_col, col, df_feat):\n",
    "        '''dropping or keeping columns'''\n",
    "        if bool_drop_col:\n",
    "            if (col in df_features.columns):    \n",
    "                self.list_drop_col.append(col) \n",
    "        else:\n",
    "            print(\"keeping original feature\", col)\n",
    "        return df_feat\n",
    "\n",
    "    def aggregate_features(self, list_col, val_aggreg):\n",
    "        for col in list_col:\n",
    "            df_groupby = self.df_raw.groupby(col).mean()\n",
    "            dict_aggreg_col = df_groupby[[val_aggreg]].to_dict()\n",
    "            self.df_feat[col + '_fe'] = self.df_raw[col].map(dict_aggreg_col['TransactionAmt'])\n",
    "            col_mode = self.df_feat[col + '_fe'].mode()[0]\n",
    "            self.df_feat[col + '_fe'] = self.df_feat[col + '_fe'].fillna(col_mode)\n",
    "\n",
    "    def _calculate_target_perc(self, col_val, df_feat):\n",
    "        '''calculate fraud percentage for each column'''\n",
    "        list_perc = []\n",
    "        dict_feat = {}\n",
    "        unique_col_values = df_feat[col_val].unique()\n",
    "        for val in unique_col_values:\n",
    "            list_perc = self._append_fraud_percentage(df_feat, col_val, val, list_perc)    \n",
    "        self._create_dict(col_val, list_perc, unique_col_values)\n",
    "        return df_feat\n",
    "    \n",
    "    def _append_fraud_percentage(self, df_feat, col_val, val, list_perc):\n",
    "        '''calculating fraud percentage and adding to list'''\n",
    "        fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                            & (df_feat[col_target]==1)].shape[0]\n",
    "        non_fraud_total = df_feat[(df_feat[col_val]==val) \n",
    "                                & (df_feat[col_target]==0)].shape[0]\n",
    "        if (non_fraud_total==0):\n",
    "            list_perc.append(0)\n",
    "        else: \n",
    "            list_perc.append(fraud_total/non_fraud_total)\n",
    "        return list_perc\n",
    "\n",
    "    def _create_dict(self, col_val, list_perc, unique_col_values):\n",
    "        '''create dictionary for original values to new fraud percent values'''\n",
    "        col_name = col_val + '_fraud_perc'\n",
    "        series_perc = pd.Series(list_perc, name=col_name)\n",
    "        series_col = pd.Series(unique_col_values, name=col_val)\n",
    "        df_feat = pd.concat([series_col, series_perc], axis=1)\n",
    "        df_feat = df_feat.sort_values(col_name, ascending=False) \n",
    "        dict_feat = df_feat.set_index(col_val).to_dict()\n",
    "        self.dict_all_feat.update(dict_feat)\n",
    "\n",
    "    def _map_col(self, col, df_feat):\n",
    "        '''map dictionary values to new features'''\n",
    "        dict_keys = self.dict_all_feat.keys()\n",
    "        for val in dict_keys:\n",
    "            df_feat[col + '_fe'] = df_feat[col].map(self.dict_all_feat[val])\n",
    "            self.new_col.append(col + '_fe')\n",
    "        return df_feat\n",
    "            \n",
    "    def _create_ratio(self, df_feat):\n",
    "        '''finalize new features with ranking values'''\n",
    "        for val in self.new_col:\n",
    "            col_min_val = df_feat[df_feat[val] > 0][val].min()\n",
    "            df_feat[val] = df_feat[val]/col_min_val\n",
    "        self.new_col = []\n",
    "        return df_feat\n",
    "    \n",
    "mod = Model()\n",
    "fe = FeatureEngineering()\n",
    "\n",
    "bool_predict_proba = False\n",
    "\n",
    "bool_apply_pca = False\n",
    "bool_apply_smote = True\n",
    "bool_apply_downsampling = True\n",
    "\n",
    "bool_create_tuning_df = True\n",
    "bool_drop_col = True\n",
    "df_feat_1000 = False\n",
    "fe.final_features(bool_drop_col, list_feat=['addr1','addr2','card2','card3','C1','P_emaildomain', \n",
    "                                            'card6', 'V294','V279','C14','V306','D2','D10'])\n",
    "bool_drop_col = False\n",
    "fe.final_features(bool_drop_col, list_feat=['card5', 'V317', 'V69', 'D1','D3','D4','D11'])\n",
    "fe.list_drop_col.append('C4')\n",
    "fe.create_final_df()\n",
    "\n",
    "# fe.feature_testing(bool_drop_col, list_feat=['addr1'])\n",
    "\n",
    "# FN = 540, FP = 12598.0, pca, smote, downsampling\n",
    "# FN = 534.0, FP = 12535.0, pca, smote \n",
    "# FN = 577.0, FP = 22223.0, smote and downsampling only\n",
    "# FN = 360, FP = 30387.0, smote only\n",
    "# FN = 2103.0, FP = 36.0, undersampling only \n",
    "# FN = 508.0, FP = 8724.0, full, pca, smote\n",
    "# FN = 508, FP = 8745, full, pca, smote, undersample, 50,000 samples\n",
    "# FN = 510, FP = 8701, full, pca, smote, undersample, 25,000 samples\n",
    "# FN = 1093, FP = 734, full, pca, smote, undersample, 50,000 samples, correction\n",
    "# FN = 360, FP = 30387, smote\n",
    "# FN = 354, FP = 31090, smote, undersample\n",
    "# FN = 572, FP = 30741, smote, undersample, pca\n",
    "# FN = 485, FP = 30076, smote, undersample, pca, 100000\n",
    "# FN = , FP = , smote, undersample, pca, 95000, full\n",
    "# FN = 360, FP = 30387, no downsample, no pca, smote addr1\n",
    "# FN = 529, FP = 18023, smote, full\n",
    "# FN = 429, FP = 23106, smote, downsample, full\n",
    "# FN = 549, FP = 30123, smote, downsample, pca, full\n",
    "# FN = 545, FP = 30227, smote, downsample, pca, 290, full\n",
    "\n",
    "# NEXT, we need to fix our tuning dataframe. method, decide how we are going to tune our model, then \n",
    "# we also need to decide how this effects our results. Should we tune on the hold outset? If we applied\n",
    "# smote and undersampling, etc. how does this effect our outcome?\n",
    "\n",
    "# the tuning dataframe we define must upsample and downsample from whole dataset, we use that to \n",
    "# tune our data since during the process of tuning, we will have our hold out set in the CV. \n",
    "# Also, check time elap for non pca. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (190000, 291)\n",
      "y_train (190000, 1)\n",
      "X_test (59054, 291)\n",
      "y_test (59054,)\n",
      "y_train==1 0    95000\n",
      "dtype: int64\n",
      "y_train==0 0    95000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('X_train', mod.X_train.shape)\n",
    "print('y_train',mod.y_train.shape)\n",
    "print('X_test',mod.X_test.shape)\n",
    "print('y_test',mod.y_test.shape)\n",
    "print('y_train==1',np.sum(mod.y_train[mod.y_train==0].isnull()))\n",
    "print('y_train==0',np.sum(mod.y_train[mod.y_train==1].isnull()))\n",
    "# we need to test tuning on the entire dataframe and also the split version we have created. \n",
    "# lets test using logisticregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_features (190000, 291)\n",
      "y_target (190000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "...    ..\n",
       "189995  1\n",
       "189996  1\n",
       "189997  0\n",
       "189998  1\n",
       "189999  0\n",
       "\n",
       "[190000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X_features', mod.X_features.shape)\n",
    "print('y_target',mod.y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2', random_state=42,\n",
      "                   solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "roc score: 0.6803990865550728\n",
      "\n",
      "The following new features have been created: ['addr1_fe', 'addr2_fe', 'card2_fe', 'card3_fe', 'C1_fe', 'P_emaildomain_fe', 'card6_fe', 'V294_fe', 'V279_fe', 'C14_fe', 'V306_fe', 'D2_fe', 'D10_fe', 'card5_fe', 'V317_fe', 'V69_fe', 'D1_fe', 'D3_fe', 'D4_fe', 'D11_fe'] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.53      0.69     56945\n",
      "           1       0.06      0.83      0.11      2109\n",
      "\n",
      "    accuracy                           0.55     59054\n",
      "   macro avg       0.52      0.68      0.40     59054\n",
      "weighted avg       0.96      0.55      0.67     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "375  model score  429.0  23106.0   0.067780  0.796586            0.623481   \n",
      "376  model score  545.0  30227.0   0.049196  0.741584            1.639055   \n",
      "377  model score  549.0  30123.0   0.049238  0.739687            0.431291   \n",
      "378  model score  429.0  23106.0   0.067780  0.796586            0.578866   \n",
      "0    model score  367.0  26490.0   0.061703  0.825984            0.790139   \n",
      "\n",
      "         tn       tp  \n",
      "375  1680.0  33839.0  \n",
      "376  1564.0  26718.0  \n",
      "377  1560.0  26822.0  \n",
      "378  1680.0  33839.0  \n",
      "0    1742.0  30455.0  \n",
      "\n",
      "model does not have _feature_importance attribute.\n"
     ]
    }
   ],
   "source": [
    "# Testing split, then finalized and moving onto tuning.\n",
    "bool_predict_proba = False\n",
    "model_current = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr_temp = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "# model_lr_temp.fit(mod.X_train, mod.y_train)\n",
    "# pred_lr = model_lr_temp.predict(mod.X_test)\n",
    "# print(confusion_matrix(mod.y_test, pred_lr))\n",
    "# print(roc_auc_score(mod.y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.X_train = pd.DataFrame(mod.X_train)\n",
    "mod.y_train = pd.DataFrame(mod.y_train)\n",
    "mod.X_test = pd.DataFrame(mod.X_test)\n",
    "mod.y_test = pd.DataFrame(mod.y_test)\n",
    "mod.X_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv')\n",
    "mod.y_train.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv')\n",
    "mod.X_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv')\n",
    "mod.y_test.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv')\n",
    "mod.X_features = pd.DataFrame(mod.X_features)\n",
    "mod.y_target = pd.DataFrame(mod.y_target)\n",
    "mod.X_features.to_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv')\n",
    "mod.y_target.to_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.X_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_train.csv')\n",
    "mod.y_train = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_train.csv')\n",
    "mod.X_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_test.csv')\n",
    "mod.y_test = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_test.csv')\n",
    "mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv')\n",
    "mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.X_features = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/X_features.csv')\n",
    "mod.y_target = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/y_target.csv')\n",
    "mod.X_features = mod.X_features.drop('Unnamed: 0', axis=1)\n",
    "mod.y_target = mod.y_target.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.X_features.info(memory_usage='deep')\n",
    "# mod.y_target.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "threshold:  0.1\n",
      "roc auc score: 0.5199051050440433\n",
      "confusion matrix:\n",
      " [[ 2483 54462]\n",
      " [    8  2101]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.5423389076188869\n",
      "confusion matrix:\n",
      " [[ 5443 51502]\n",
      " [   23  2086]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.5630077660970813\n",
      "confusion matrix:\n",
      " [[ 8472 48473]\n",
      " [   48  2061]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.5831585725222707\n",
      "confusion matrix:\n",
      " [[11469 45476]\n",
      " [   74  2035]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.6004644287340888\n",
      "confusion matrix:\n",
      " [[14466 42479]\n",
      " [  112  1997]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.6195614620031532\n",
      "confusion matrix:\n",
      " [[17748 39197]\n",
      " [  153  1956]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.6379032890953442\n",
      "confusion matrix:\n",
      " [[21241 35704]\n",
      " [  205  1904]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.6541727580966735\n",
      "confusion matrix:\n",
      " [[25092 31853]\n",
      " [  279  1830]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.6980820420958874\n",
      "confusion matrix:\n",
      " [[34980 21965]\n",
      " [  460  1649]]\n",
      "roc auc score: 0.6980820420958874\n",
      "\n",
      "The following new features have been created: ['addr1_fe', 'addr2_fe', 'card2_fe', 'card3_fe', 'C1_fe', 'P_emaildomain_fe', 'card6_fe', 'V294_fe', 'V279_fe', 'C14_fe', 'V306_fe', 'D2_fe', 'D10_fe', 'card5_fe', 'V317_fe', 'V69_fe', 'D1_fe', 'D3_fe', 'D4_fe', 'D11_fe'] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76     56945\n",
      "           1       0.07      0.78      0.13      2109\n",
      "\n",
      "    accuracy                           0.62     59054\n",
      "   macro avg       0.53      0.70      0.44     59054\n",
      "weighted avg       0.95      0.62      0.73     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "376  model score  545.0  30227.0   0.049196  0.741584            1.639055   \n",
      "377  model score  549.0  30123.0   0.049238  0.739687            0.431291   \n",
      "378  model score  429.0  23106.0   0.067780  0.796586            0.578866   \n",
      "379  model score  367.0  26490.0   0.061703  0.825984            0.790139   \n",
      "0    model score  460.0  21965.0   0.069831  0.781887            1.149289   \n",
      "\n",
      "         tn       tp  \n",
      "376  1564.0  26718.0  \n",
      "377  1560.0  26822.0  \n",
      "378  1680.0  33839.0  \n",
      "379  1742.0  30455.0  \n",
      "0    1649.0  34980.0  \n",
      "\n",
      "model does not have _feature_importance attribute.\n"
     ]
    }
   ],
   "source": [
    "bool_predict_proba = True\n",
    "model_current = LogisticRegression(random_state=42)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model:\n",
      " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "threshold:  0.1\n",
      "roc auc score: 0.7049145438722639\n",
      "confusion matrix:\n",
      " [[31384 25561]\n",
      " [  298  1811]]\n",
      "threshold:  0.15\n",
      "roc auc score: 0.751985055747227\n",
      "confusion matrix:\n",
      " [[40714 16231]\n",
      " [  445  1664]]\n",
      "threshold:  0.2\n",
      "roc auc score: 0.751985055747227\n",
      "confusion matrix:\n",
      " [[40714 16231]\n",
      " [  445  1664]]\n",
      "threshold:  0.25\n",
      "roc auc score: 0.751985055747227\n",
      "confusion matrix:\n",
      " [[40714 16231]\n",
      " [  445  1664]]\n",
      "threshold:  0.3\n",
      "roc auc score: 0.7669717450489294\n",
      "confusion matrix:\n",
      " [[47146  9799]\n",
      " [  620  1489]]\n",
      "threshold:  0.35\n",
      "roc auc score: 0.7587608783416373\n",
      "confusion matrix:\n",
      " [[50072  6873]\n",
      " [  763  1346]]\n",
      "threshold:  0.4\n",
      "roc auc score: 0.7487766201996462\n",
      "confusion matrix:\n",
      " [[51986  4959]\n",
      " [  876  1233]]\n",
      "threshold:  0.45\n",
      "roc auc score: 0.7487766201996462\n",
      "confusion matrix:\n",
      " [[51986  4959]\n",
      " [  876  1233]]\n",
      "threshold:  0.5\n",
      "roc auc score: 0.7487766201996462\n",
      "confusion matrix:\n",
      " [[51986  4959]\n",
      " [  876  1233]]\n",
      "roc auc score: 0.7487766201996462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95     56945\n",
      "           1       0.20      0.58      0.30      2109\n",
      "\n",
      "    accuracy                           0.90     59054\n",
      "   macro avg       0.59      0.75      0.62     59054\n",
      "weighted avg       0.96      0.90      0.92     59054\n",
      "\n",
      "\n",
      "Printing df_scores...\n",
      "\n",
      "      feat_tested     fn       fp  precision    recall  time_elapsed (min)  \\\n",
      "379  model score  367.0  26490.0   0.061703  0.825984            0.790139   \n",
      "380  model score  460.0  21965.0   0.069831  0.781887            1.149289   \n",
      "381          NaN  920.0   6053.0   0.164181  0.563774            0.090905   \n",
      "382          NaN  968.0   5471.0   0.172565  0.541015            0.135564   \n",
      "0            NaN  876.0   4959.0   0.199128  0.584637            0.141384   \n",
      "\n",
      "         tn       tp  \n",
      "379  1742.0  30455.0  \n",
      "380  1649.0  34980.0  \n",
      "381  1189.0  50892.0  \n",
      "382  1141.0  51474.0  \n",
      "0    1233.0  51986.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                col  feat_rank\n",
      "0       ProductCD_R   0.445391\n",
      "1          card2_fe   0.200286\n",
      "2            D10_fe   0.185126\n",
      "3           V279_fe   0.054042\n",
      "4           V294_fe   0.045456\n",
      "5          card6_fe   0.025921\n",
      "6  P_emaildomain_fe   0.018616\n",
      "7  card4_mastercard   0.014196\n",
      "8              V280   0.010655\n",
      "9              V100   0.000310\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_current = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
    "mod.create_df_score_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# model_current = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# model_current = LogisticRegression(random_state=42)\n",
    "# mod.create_df_score_model(model_current) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.X_train = pd.DataFrame()\n",
    "        self.y_train = pd.DataFrame()\n",
    "        self.X_test = pd.DataFrame()\n",
    "        self.y_test = pd.DataFrame()\n",
    "        self.X_features = pd.DataFrame()\n",
    "        self.y_target = pd.DataFrame()\n",
    "        \n",
    "    def create_df_score_model(self, model_current):\n",
    "        '''scores model'''\n",
    "        print(\"Fitting model:\\n\", model_current)\n",
    "        y_pred, elapsed_time = self.add_model(model_current) \n",
    "        df_scores, df_temp, y_pred = self._score_model(y_pred, \n",
    "                                                       elapsed_time)\n",
    "        self._save_results(df_scores, df_temp, y_pred)\n",
    "        self._feature_importance(model_current)\n",
    "        fe.col_fe = []\n",
    "        \n",
    "    def add_model(self, model):        \n",
    "        '''fitting model and calculating time elapsed'''\n",
    "        start_time = time.time()\n",
    "        model.fit(mod.X_train, mod.y_train)\n",
    "        y_pred = self._predict(model)\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        return y_pred, elapsed_time\n",
    "    \n",
    "    def _predict(self, model):\n",
    "        '''make prediction'''\n",
    "        if bool_predict_proba:\n",
    "            y_pred = self._predict_proba(model)\n",
    "            return y_pred \n",
    "        else:\n",
    "            y_pred = model.predict(mod.X_test)\n",
    "            return y_pred\n",
    "        \n",
    "    def _predict_proba(self, model):\n",
    "        try:\n",
    "            y_pred_prob = model.predict_proba(mod.X_test)\n",
    "            y_pred_class = self._predict_proba_threshold(y_pred_prob)\n",
    "            return y_pred_class\n",
    "        except:\n",
    "            print(\"Model does not have predict_proba attribute.\")\n",
    "            \n",
    "    def _predict_proba_threshold(self, y_pred_prob):\n",
    "        for threshold in [.1, .15, .2, .25, .3, .35, .4, .45, .5]:\n",
    "            print('threshold: ', threshold)\n",
    "            y_pred_class = binarize(y_pred_prob, threshold)[:,1]\n",
    "            print('roc auc score:', roc_auc_score(mod.y_test, y_pred_class))\n",
    "            print('confusion matrix:\\n', confusion_matrix(mod.y_test, y_pred_class))\n",
    "        return y_pred_class\n",
    "            \n",
    "#     def _create_roc_curve(self, y_pred_class):\n",
    "#         fpr, tpr, thresholds = roc_curve(mod.y_test, y_pred_class)\n",
    "#         plt.plot(fpr, tpr)\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.0])\n",
    "#         plt.title(\"ROC curve for classifier\")\n",
    "#         plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "#         plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "        \n",
    "    def _score_model(self, y_pred, elapsed_time):      \n",
    "        '''creating dataframe with score results'''\n",
    "        col_recall, col_precision, col_time = self._calc_scores(y_pred, \n",
    "                                                                  elapsed_time)        \n",
    "        df_conf_matrix = self._confusion_matrix(y_pred)\n",
    "        df_temp = pd.concat([col_recall, col_precision, df_conf_matrix, col_time], axis=1)\n",
    "        if fe.col_fe:\n",
    "            df_temp = self._concat_new_feat(df_temp)\n",
    "        df_scores = self._read_create_score_file(df_temp)\n",
    "        return df_scores, df_temp, y_pred\n",
    "\n",
    "    def _calc_scores(self, y_pred, elapsed_time):\n",
    "        '''calculating recall, precision and elapsed time'''\n",
    "        col_recall = pd.Series(recall_score(mod.y_test, y_pred), name='recall')\n",
    "        col_precision = pd.Series(precision_score(mod.y_test, y_pred), name='precision')\n",
    "        col_time = pd.Series(elapsed_time, name='time_elapsed (min)')\n",
    "        print(\"roc auc score:\", roc_auc_score(mod.y_test, y_pred))\n",
    "        return col_recall, col_precision, col_time\n",
    "    \n",
    "    def _confusion_matrix(self, y_pred):\n",
    "        '''creating confusion matrix dataframe'''\n",
    "        df_conf_matrix = pd.DataFrame(confusion_matrix(mod.y_test, y_pred))\n",
    "        val_tp = pd.Series(df_conf_matrix[0][0], name='tp')\n",
    "        val_fn = pd.Series(df_conf_matrix[0][1], name='fn')\n",
    "        val_fp = pd.Series(df_conf_matrix[1][0], name='fp')\n",
    "        val_tn = pd.Series(df_conf_matrix[1][1], name='tn')\n",
    "        return pd.concat([val_fn, val_fp, val_tp, val_tn], axis=1)\n",
    "\n",
    "    def _concat_new_feat(self, df_temp):\n",
    "        '''concatenate scoring results'''        \n",
    "        print(\"\\nThe following new features have been created:\", fe.col_fe, '\\n')\n",
    "        if len(fe.col_fe) > 1: \n",
    "            fe.col_fe = \"model score\"\n",
    "        col_fe = pd.Series(fe.col_fe, name='feat_tested')\n",
    "        return pd.concat([col_fe, df_temp], axis=1)\n",
    "    \n",
    "    def _read_create_score_file(self, df_temp):\n",
    "        '''reading or creating df_scores file'''\n",
    "        try: \n",
    "            df_scores = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "            df_scores = df_scores.drop('Unnamed: 0', axis=1)\n",
    "        except:\n",
    "            print(\"\\nCreating df_scores.csv file.\") \n",
    "            df_scores = df_temp\n",
    "        return df_scores\n",
    "            \n",
    "    def _save_results(self, df_scores, df_temp, y_pred):\n",
    "        '''printing scores for new features'''            \n",
    "        df_scores = pd.concat([df_scores, df_temp], axis=0)\n",
    "        df_scores.to_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "        classif_report = classification_report(mod.y_test, y_pred)\n",
    "        self._print_summary(classif_report, df_scores)\n",
    "        self._save_summary(classif_report)\n",
    "\n",
    "    def _print_summary(self, classif_report, df_scores):\n",
    "        '''print last 5 rows of previous score results'''\n",
    "        print(classif_report)\n",
    "        print('\\nPrinting df_scores...\\n\\n', df_scores.tail(5))\n",
    "    \n",
    "    def _save_summary(self, classif_report):\n",
    "        '''save score result summary to text file'''\n",
    "        file_summary = open('/Users/krahman/work/fraud_detection/saved_files/df_scores_summary.txt', \"a\")\n",
    "        file_summary.write('New features created from: ' \n",
    "                           + fe.str_list_col_fe \n",
    "                           + '\\n')\n",
    "        file_summary.write(classif_report)\n",
    "        file_summary.close()\n",
    "        \n",
    "    def _feature_importance(self, model):\n",
    "        '''create feature importance dataframe and bar plot'''\n",
    "        try:\n",
    "            df_feat_rank = self._feat_import_create_df(model)\n",
    "            self._feat_import_create_plot(df_feat_rank)\n",
    "            print(df_feat_rank[0:10].reset_index(drop=True))\n",
    "        except:\n",
    "            print(\"\\nmodel does not have _feature_importance attribute.\")\n",
    "        \n",
    "    def _feat_import_create_df(self, model):\n",
    "        '''creating dataframe of important features'''\n",
    "        col_name = pd.Series(fe.df_feat.columns, name='col')\n",
    "        col_feat_rank = pd.Series(model.feature_importances_, \n",
    "                                  name='feat_rank')\n",
    "        df_feat_rank = pd.concat([col_name, col_feat_rank], axis=1)\n",
    "        df_feat_rank = df_feat_rank.sort_values('feat_rank', ascending=False)\n",
    "        return df_feat_rank\n",
    "    \n",
    "    def _feat_import_create_plot(self, df_feat_rank):\n",
    "        '''create feature importance bar plot'''\n",
    "        plt.figure(figsize=(5,6))\n",
    "        sns.barplot(df_feat_rank.feat_rank[0:10],\n",
    "                    df_feat_rank.col[0:10],\n",
    "                    palette='Blues_d')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "        \n",
    "mod = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82     56954\n",
      "           1       0.84      0.77      0.80     57022\n",
      "\n",
      "    accuracy                           0.81    113976\n",
      "   macro avg       0.81      0.81      0.81    113976\n",
      "weighted avg       0.81      0.81      0.81    113976\n",
      "\n",
      "[[48297  8657]\n",
      " [12938 44084]]\n"
     ]
    }
   ],
   "source": [
    "# model_xgbc = XGBClassifier()\n",
    "# model_xgbc.fit(mod.X_train, mod.y_train)\n",
    "# y_pred_xgbc = model_xgbc.predict(mod.X_test)\n",
    "# print(classification_report(mod.y_test, y_pred_xgbc))\n",
    "# print(confusion_matrix(mod.y_test, y_pred_xgbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cbc = CatBoostClassifier()\n",
    "# X_test\n",
    "# model_cbc.fit(mod.X_train, mod.y_train)\n",
    "# # y_pred_cbc = model_cbc.predict(mod.y_test)\n",
    "# # print(classification_report(mod.y_test, y_pred_cbc))\n",
    "# # print(confusion_matrix(mod.y_test, y_pred_cbc))\n",
    "# test_data = Pool(mod.X_test)\n",
    "# y_pred_cbc = model_cbc.predict(test_data)\n",
    "# print(classification_report(mod.y_test, y_pred_cbc))\n",
    "# print(confusion_matrix(mod.y_test, y_pred_cbc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48301  8653]\n",
      " [13502 43520]]\n",
      "0.8056422853051062\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "model_lr_temp = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "model_lr_temp.fit(mod.X_train, mod.y_train)\n",
    "pred_lr = model_lr_temp.predict(mod.X_test)\n",
    "print(confusion_matrix(mod.y_test, pred_lr))\n",
    "print(roc_auc_score(mod.y_test, pred_lr))\n",
    "# NEXT, we fixed our downsample and upsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48301  8653]\n",
      " [13502 43520]]\n",
      "0.8056422853051062\n"
     ]
    }
   ],
   "source": [
    "model_lr_temp.fit(mod.X_train, mod.y_train)\n",
    "pred_lr = model_lr_temp.predict(mod.X_test)\n",
    "print(confusion_matrix(mod.y_test, pred_lr))\n",
    "print(roc_auc_score(mod.y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Tuning - XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.X_train, mod.X_test, mod.y_train, mod.y_test = train_test_split(mod.X_features[0:10000], \n",
    "#                                                                     mod.y_target[0:10000], \n",
    "#                                                                     test_size=0.1, \n",
    "#                                                                     random_state=42)\n",
    "mod.X_train, mod.X_test, mod.y_train, mod.y_test = train_test_split(mod.X_features, \n",
    "                                                                    mod.y_target, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning xgbc\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 7.5min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.986, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 14.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.985, total= 6.9min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 28.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 34.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 40.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 46.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 53.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.987, total= 6.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 59.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=3, reg_lambda=5, reg_alpha=5, max_depth=9, colsample_bytree=0.5, colsample_bynode=0.7, colsample_bylevel=0.9, booster=dart, base_score=0.3, score=0.988, total= 6.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 3.1min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.965, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.963, total= 2.8min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.966, total= 3.0min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.964, total= 2.6min\n",
      "[CV] subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4 \n",
      "[CV]  subsample=0.7, scale_pos_weight=1, reg_lambda=7, reg_alpha=7, max_depth=11, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0, booster=dart, base_score=0.4, score=0.967, total= 2.6min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 2.1min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 2.2min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.966, total= 2.0min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.961, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.963, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.962, total= 1.9min\n",
      "[CV] subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3 \n",
      "[CV]  subsample=0.1, scale_pos_weight=7, reg_lambda=3, reg_alpha=7, max_depth=2, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=1, booster=dart, base_score=0.3, score=0.964, total= 2.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.800, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.796, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.799, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.805, total= 1.6min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.802, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.804, total= 1.8min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.798, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5 \n",
      "[CV]  subsample=0.5, scale_pos_weight=7, reg_lambda=1, reg_alpha=5, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.7, colsample_bylevel=1, booster=gblinear, base_score=0.5, score=0.807, total= 1.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.988, total= 3.4min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.2min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.990, total= 3.0min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.1min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.3min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 3.7min\n",
      "[CV] subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4 \n",
      "[CV]  subsample=0.5, scale_pos_weight=5, reg_lambda=5, reg_alpha=3, max_depth=13, colsample_bytree=0.7, colsample_bynode=0.5, colsample_bylevel=0.3, booster=gbtree, base_score=0.4, score=0.989, total= 4.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.985, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n",
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.986, total= 2.5min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3, score=0.984, total= 2.4min\n",
      "[CV] subsample=0.7, scale_pos_weight=3, reg_lambda=1, reg_alpha=7, max_depth=9, colsample_bytree=0.7, colsample_bynode=0.1, colsample_bylevel=0.9, booster=gbtree, base_score=0.3 \n"
     ]
    }
   ],
   "source": [
    "### Tuning XGBClassifier READY ###\n",
    "print('tuning xgbc')\n",
    "xgbc = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "# learning_rate = [0,.1,.3,.5,.7,.9]\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "subsample = [.1,.3,.5,.7]\n",
    "colsample_bytree = [.1,.3,.5,.7]\n",
    "colsample_bylevel = [0,.1,.3,.5,.7,.9,1]\n",
    "colsample_bynode = [.1,.3,.5,.7]\n",
    "reg_alpha = [0,1,3,5,7]\n",
    "reg_lambda = [1,3,5,7]\n",
    "scale_pos_weight = [1,3,5,7]\n",
    "base_score = [.1,.2,.3,.4,.5]\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "#                        learning_rate=learning_rate, \n",
    "                       booster=booster, \n",
    "                       subsample=subsample, \n",
    "                       colsample_bytree=colsample_bytree, colsample_bylevel=colsample_bylevel, \n",
    "                       colsample_bynode=colsample_bynode, reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                       scale_pos_weight=scale_pos_weight,\n",
    "                       base_score=base_score\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(xgbc, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best booster:', best_model.best_estimator_.get_params()['booster'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n",
    "print('Best colsample_bytree:', best_model.best_estimator_.get_params()['colsample_bytree'])\n",
    "print('Best colsample_bylevel:', best_model.best_estimator_.get_params()['colsample_bylevel'])\n",
    "print('Best colsample_bynode:', best_model.best_estimator_.get_params()['colsample_bynode'])\n",
    "print('Best reg_alpha:', best_model.best_estimator_.get_params()['reg_alpha'])\n",
    "print('Best reg_lambda:', best_model.best_estimator_.get_params()['reg_lambda'])\n",
    "print('Best scale_pos_weight:', best_model.best_estimator_.get_params()['scale_pos_weight'])\n",
    "print('Best base_score:', best_model.best_estimator_.get_params()['base_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_estimator_)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### testing manual tuning ######\n",
    "### manual xbgc tuning\n",
    "max_depth = [3,5,7,9,11,13]\n",
    "max_depth = [1,3,5]\n",
    "list_time_elapsed = []\n",
    "list_roc_auc_score = []\n",
    "for val in max_depth:\n",
    "    model_xgbc = XGBClassifier(max_depth=val, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_xgbc.fit(mod.X_train_test, mod.y_train)\n",
    "    y_pred_xgbc = model_xgbc.predict(mod.X_test)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    score_roc_auc = roc_auc_score(mod.y_test, y_pred_xgbc)\n",
    "    print(score_roc_auc) #delete\n",
    "    list_time_elapsed.append(elapsed_time)\n",
    "    list_roc_auc_score.append(score_roc_auc)\n",
    "    print('max depth: ', val)\n",
    "    print(confusion_matrix(mod.y_test, y_pred_xgbc))\n",
    "\n",
    "col_time_elapsed = pd.Series(list_time_elapsed)\n",
    "col_roc_score = pd.Series(list_roc_auc_score)\n",
    "col_max_depth = pd.Series(max_depth)\n",
    "df_results_xgbc = pd.concat([col_max_depth, col_roc_score,col_time_elapsed], \n",
    "                            keys=['max_depth', 'roc_auc_score', 'time_elap'], \n",
    "                            axis=1)\n",
    "print(df_results_xgbc)\n",
    "\n",
    "sns.lineplot(x='max_depth', y='roc_auc_score', data=df_results_xgbc)\n",
    "plt.title(\"XGBC manual tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Tuning RandomForestClassifier READY ####\n",
    "rfc = RandomForestClassifier(oob_score=False, n_jobs=1, random_state=42, verbose=1)\n",
    "\n",
    "n_estimators = [50,75,100,125,150,200]\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [3,5,7,9,11,13,15]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,2,4,6,8,10]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "max_leaf_nodes = [2,3,5,7,9,None]\n",
    "min_impurity_decrease = [0,.1,.3,.5,.7,.9]\n",
    "\n",
    "# n_estimators = [50,75,100,125]\n",
    "# criterion = ['gini']\n",
    "# max_depth = [2,3,4,5,6,7,None]\n",
    "# min_samples_split = [6,7,8,9]\n",
    "# min_samples_leaf = [1,2]\n",
    "# min_weight_fraction_leaf = [0]\n",
    "# max_features = ['auto', 'sqrt', 'log2', None]\n",
    "# max_leaf_nodes = [None]\n",
    "# min_impurity_decrease = [0]\n",
    "\n",
    "hyperparameters = dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split,\n",
    "                       min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                       min_impurity_decrease=min_impurity_decrease\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(rfc, hyperparameters, random_state=42, cv=5, verbose=5, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best max_leaf_nodes:', best_model.best_estimator_.get_params()['max_leaf_nodes'])\n",
    "print('Best min_impurity_decrease:', best_model.best_estimator_.get_params()['min_impurity_decrease'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-5c0baab4b667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### testing manual tuning ######\n",
    "### manual lr tuning\n",
    "\n",
    "penalty = ['l1', 'l2', 'elasticnet','none']\n",
    "\n",
    "list_time_elapsed = []\n",
    "list_roc_auc_score = []\n",
    "for val in penalty:\n",
    "    model = LogisticRegression(penalty=val, n_jobs=1, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(mod.X_train, mod.y_train)\n",
    "    y_pred = model.predict(mod.X_test)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    score_roc_auc = roc_auc_score(mod.y_test, y_pred)\n",
    "    print(score_roc_auc) #delete\n",
    "    list_time_elapsed.append(elapsed_time)\n",
    "    list_roc_auc_score.append(score_roc_auc)\n",
    "    print('penalty: ', val)\n",
    "    print(confusion_matrix(mod.y_test, y_pred))\n",
    "\n",
    "col_time_elapsed = pd.Series(list_time_elapsed)\n",
    "col_roc_score = pd.Series(list_roc_auc_score)\n",
    "col_penalty = pd.Series(penalty) ### UPDATE\n",
    "df_results = pd.concat([col_max_depth, col_roc_score,col_time_elapsed], \n",
    "                            keys=['penalty', 'roc_auc_score', 'time_elap'], \n",
    "                            axis=1)\n",
    "print(df_results)\n",
    "\n",
    "sns.lineplot(x='penalty', y='roc_auc_score', data=df_results_xgbc)\n",
    "plt.title(\"lr manual tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 291)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mod.X_features_test.shape)\n",
    "print(mod.y_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.878, total=  51.7s\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.882, total=  44.6s\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.904, total=  21.5s\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.910, total= 1.1min\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.912, total= 2.0min\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.913, total=  57.1s\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  6.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.914, total= 1.1min\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  7.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.913, total= 1.1min\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  8.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ............. penalty=l1, max_iter=50, score=0.910, total= 1.0min\n",
      "[CV] penalty=l1, max_iter=50 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  9.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-a05421809e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### LR Tuning ####\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "penalty = ['l1', 'l2', 'elasticnet','none']\n",
    "tol = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "C = [1e-1,.2,.3,.5,.7,1]\n",
    "fit_intercept = [True,False]\n",
    "intercept_scaling = [1,.1,.01,.001]\n",
    "class_weight = ['balanced', None]\n",
    "solver = ['newton-cg', 'lbfgs', 'sag']#, 'liblinear','saga']\n",
    "max_iter = [50,75,100,150,200]\n",
    "multi_class = ['auto', 'ovr', 'multinomial']\n",
    "l1_ratio = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7]\n",
    "\n",
    "\n",
    "hyperparameters = dict(penalty=penalty, \n",
    "#                        tol=tol, \n",
    "#                        C=C, \n",
    "#                        fit_intercept=fit_intercept,\n",
    "#                        intercept_scaling=intercept_scaling, \n",
    "#                        class_weight=class_weight,\n",
    "#                        solver=solver, \n",
    "                       max_iter=max_iter\n",
    "#                        multi_class=multi_class, \n",
    "#                        l1_ratio=l1_ratio\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best tol:', best_model.best_estimator_.get_params()['tol'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best fit_intercept:', best_model.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best intercept_scaling:', best_model.best_estimator_.get_params()['intercept_scaling'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n",
    "print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "print('Best max_iter:', best_model.best_estimator_.get_params()['max_iter'])\n",
    "print('Best multi_class:', best_model.best_estimator_.get_params()['multi_class'])\n",
    "print('Best l1_ratio:', best_model.best_estimator_.get_params()['l1_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(mod.y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(mod.y_target[mod.y_target==0].isnull())\n",
    "df_temp = pd.concat([mod.X_features, mod.y_target], keys=['features','target'],axis=1).sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.X_features_test = df_temp.features\n",
    "mod.y_target_test = df_temp.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.1750s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done  27 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0297s.) Setting batch_size=26.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best criterion: entropy\n",
      "Best splitter: best\n",
      "Best max_depth: 11\n",
      "Best min_samples_split: 5\n",
      "Best min_samples_leaf: 9\n",
      "Best min_weight_fraction_leaf: 0\n",
      "Best max_features: None\n",
      "Best class_weight: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "#### Tuning DTC READY ####\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_depth = [3,5,7,9,11]\n",
    "min_samples_split = [2,3,5,7,9]\n",
    "min_samples_leaf = [1,3,5,7,9]\n",
    "min_weight_fraction_leaf = [0,.1,.2,.3,.4,.5]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "hyperparameters = dict(criterion=criterion, splitter=splitter, max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                       min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                       max_features=max_features, class_weight=class_weight\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(dt, hyperparameters, random_state=42, cv=10, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "# best_model = clf.fit(mod.X_features_test, mod.y_target_test) #testing\n",
    " \n",
    "# best hyper parameters\n",
    "print('Best criterion:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best splitter:', best_model.best_estimator_.get_params()['splitter'])\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_weight_fraction_leaf:', best_model.best_estimator_.get_params()['min_weight_fraction_leaf'])\n",
    "print('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\n",
    "print('Best class_weight:', best_model.best_estimator_.get_params()['class_weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 267, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 890, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 846, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 789, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 846, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/pickle.py\", line 510, in save\n",
      "    rv = reduce(obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/_memmapping_reducer.py\", line 340, in __call__\n",
      "    os.chmod(dumped_filename, FILE_PERMISSIONS)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/folders/tg/r19qpg554vq4rcwypdqzh8v00000gn/T/joblib_memmapping_folder_6333_789436622/6333-125992266960-2da1dd81dca74d229094788816a18463.pkl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/backend/queues.py\", line 175, in _feed\n",
      "    onerror(e, obj)\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 310, in _on_queue_feeder_error\n",
      "    self.thread_wakeup.wakeup()\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 155, in wakeup\n",
      "    self._writer.send_bytes(b\"\")\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 183, in send_bytes\n",
      "    self._check_closed()\n",
      "  File \"/Users/krahman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
      "    raise OSError(\"handle is closed\")\n",
      "OSError: handle is closed\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 13.5min\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/options/bootstrap_options.cpp:5: Taken fraction should in in (0,1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 516, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\", line 3844, in fit\n    silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\", line 1714, in _fit\n    save_snapshot, snapshot_file, snapshot_interval, init_model\n  File \"/Users/krahman/opt/anaconda3/lib/python3.7/site-packages/catboost/core.py\", line 1642, in _prepare_train_params\n    _check_train_params(params)\n  File \"_catboost.pyx\", line 5161, in _catboost._check_train_params\n  File \"_catboost.pyx\", line 5173, in _catboost._check_train_params\n_catboost.CatBoostError: catboost/private/libs/options/bootstrap_options.cpp:5: Taken fraction should in in (0,1]\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-483-cda861767c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# best hyper parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/private/libs/options/bootstrap_options.cpp:5: Taken fraction should in in (0,1]"
     ]
    }
   ],
   "source": [
    "### Tuning CatBoost READY ###\n",
    "# Tune learning rate manually.\n",
    "cbc = CatBoostClassifier(random_state=42)\n",
    "\n",
    "max_depth = [2,3,5,7,9,11,13]\n",
    "learning_rate = [.1,.3,.5,.7,.9]\n",
    "# bagging_temperature = []\n",
    "subsample = [1,3,5,7]\n",
    "n_estimators = [50,75,100,150]\n",
    "depth = [2,4,6,8,10]\n",
    "grow_policy = ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "                       learning_rate=learning_rate, \n",
    "                       n_estimators=n_estimators,\n",
    "                       subsample=subsample,\n",
    "                       depth=depth,\n",
    "                       grow_policy=grow_policy\n",
    "                      )\n",
    "\n",
    "clf = RandomizedSearchCV(cbc, hyperparameters, random_state=42, cv=5, verbose=10, n_jobs=1, scoring='roc_auc')\n",
    "best_model = clf.fit(mod.X_features, mod.y_target)\n",
    "\n",
    "# best hyper parameters\n",
    "print('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best learning_rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "print('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best subsample:', best_model.best_estimator_.get_params()['subsample'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbc.fit(x)\n",
    "mod.X_train, mod.X_test, mod.y_train, mod.y_test = train_test_split(mod.X_features, \n",
    "                                                                    mod.y_target, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbc.fit(mod.X_train, mod.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc = cbc.predict(mod.X_test)\n",
    "confusion_matrix(y_test, pred_cbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_predict_proba = False\n",
    "# # tuned model\n",
    "# current_model = RandomForestClassifier(\n",
    "#                        max_depth=3, max_features='log2',\n",
    "#                        min_impurity_decrease=0.0, \n",
    "#                        min_samples_leaf=1, min_samples_split=7,\n",
    "#                        min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "#                        n_jobs=-1, oob_score=False, random_state=42,\n",
    "#                        verbose=0, warm_start=False)\n",
    "# mod.create_df_score_model(model_current)\n",
    "\n",
    "# # base model\n",
    "# current_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# mod.create_df_score_model(model_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.create_df_score_model(df_feat)\n",
    "# 369.0 all col\n",
    "# 398.0 remove P_emaildomain\n",
    "# 410.0 remove card6\n",
    "# 395.0 drop C4\n",
    "# 415.0 add all back in\n",
    "# 368.0 test again with C14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create feature from TransactionAmt. Add more EDA information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Tuning\n",
    "# 2. Finished - Features\n",
    "# 3. more EDA\n",
    "# 4. Finished - move pca and smote into fe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_read = pd.read_csv('/Users/krahman/work/fraud_detection/saved_files/df_scores.csv')\n",
    "df_temp_read[len(df_temp_read)-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it reads the csv, creates a dataframe, then appends the results, then saves over the old version and keeps\n",
    "# # a record of all columns in the tested dataframe.\n",
    "\n",
    "# X = df_features.drop(col_target, axis=1)\n",
    "# X = X.drop(col_id, axis=1)\n",
    "# y = df_features[col_target]\n",
    "\n",
    "# # we want to test a feature for feature engineering... we must \n",
    "\n",
    "# for col_original, col_new in zip(list_col, fe.list_new_feat):\n",
    "#     print(col_original, col_new)\n",
    "#     X[col_new] = fe.df_feat[col_new]\n",
    "#     X = X.drop(col_original, axis=1)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     sm = SMOTE(random_state=42, ratio=1.0, n_jobs=-1) # SMOTE\n",
    "#     X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "#     model_lr_pca_sm = LogisticRegression(random_state=42, n_jobs=-1) # logistic regression\n",
    "#     model_lr_pca_sm.fit(X_train_res, y_train_res)\n",
    "    \n",
    "#     y_pred = model_lr_pca_sm.predict(X_test)     # predict\n",
    "    \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     print('\\nTime elapsed:', elapsed_time / 60)\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "# #     X = X.drop(col, axis=1)\n",
    "#     print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.df_feat['time_delta'] = df_temp['time_delta']\n",
    "# fe.df_feat['time_delta_week'] = df_temp['time_delta']/7\n",
    "# fe.df_feat['time_delta_month'] = df_temp['time_delta']/30\n",
    "# fe.df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### implement into feature engineering class. days lapsed\n",
    "df_temp = fe.df_feat[['TransactionDT']]\n",
    "df_temp['time_delta'] = 0\n",
    "len_df_temp = df_temp.shape[0]\n",
    "for i in range(1,len(df_temp['TransactionDT'][0:len_df_temp])):\n",
    "    val_time_1 = df_temp.loc[i - 1, 'TransactionDT']\n",
    "    val_time_2 = df_temp.loc[i, 'TransactionDT']\n",
    "    val_time_delta = val_time_2 - val_time_1\n",
    "    df_temp.loc[i, 'time_delta'] = val_time_delta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP\n",
    "# class Results():\n",
    "#     def __init__(self):\n",
    "#         self.model_lr_results = pd.DataFrame()\n",
    "#         self.model_recall = []\n",
    "#         self.model_precision = []\n",
    "# #         self.model_auc_score = []\n",
    "        \n",
    "#     def _score(self, y_true, y_pred):\n",
    "#         self.model_recall.append(recall_score(y_true, y_pred))\n",
    "#         self.model_precision.append(precision_score(y_true, y_pred))\n",
    "#         self._create_series()\n",
    "# #         self.model_auc_score.append(roc_auc_score(y_true, y_pred[:,1]))\n",
    "\n",
    "#     def _create_series(self):\n",
    "#         model_recall = pd.Series(self.model_recall, name='recall')\n",
    "#         model_precision = pd.Series(self.model_precision, name='precision')\n",
    "# #         model_auc_score = pd.Series(self.model_auc_score, name='auc_score')\n",
    "#         model_results_final = pd.concat([model_recall, model_precision],axis=1)\n",
    "#         return model_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes, do we need to visualize our results?? We need to look at our confusion matrix\n",
    "# and decide how to improve our results from here... Test and see what happens if we \n",
    "# increase our principal components... test removing columns, adding columns, imputing\n",
    "# certain columns all together. \n",
    "\n",
    "# test imputing 500 or less for ohe. Then test dropping card1 due to its number\n",
    "# of unique values that would make our data very high dimensional. We can try only ohe for\n",
    "# the entire dataset to see how our model performs over all... Though.. it will likely run \n",
    "# out of memory and crash. \n",
    "\n",
    "# ALSO test probabilities on logisticregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.rcParams['font.size'] = 14\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "# plt.hist(y_pred_prob[:,1], bins=8)\n",
    "# plt.xlim(0,1)\n",
    "# plt.title(\"Histogram of Probability of Fraud\")\n",
    "# plt.xlabel(\"Predicted probability of Fraud\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# y_pred_prob = model_lr_pca.predict_proba(X_pca)\n",
    "e\n",
    "\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:,1])\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.title(\"ROC curve for fraud detection classifier\")\n",
    "# plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "# plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_threshold(threshold):\n",
    "#     print(\"Sensitivity:\", tpr[thresholds > threshold][-1])\n",
    "#     print(\"Specificity:\", 1 - fpr[thresholds > threshold][-1],'\\n')\n",
    "    \n",
    "# evaluate_threshold(.5)\n",
    "# evaluate_threshold(.2)\n",
    "# evaluate_threshold(.1)\n",
    "\n",
    "# print(roc_auc_score(y, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_test2) # actual prediction test data set\n",
    "# print('y_pred_actual on test set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_actual))\n",
    "# print(classification_report(y_test2, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_test2)\n",
    "# y_pred_class = binarize(y_pred_proba, 0.5)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y_test2, y_pred_class))\n",
    "# print(classification_report(y_test2, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_actual = model_lr_pca.predict(X_pca) # actual full data set\n",
    "# print('Logistic Regression')\n",
    "# print('y_pred_actual full data set\\n')\n",
    "# print(y_pred_actual[0:10])\n",
    "# print(confusion_matrix(y, y_pred_actual))\n",
    "# print(classification_report(y, y_pred_actual))\n",
    "\n",
    "# print('y_pred_proba full data set\\n')\n",
    "# y_pred_proba = model_lr_pca.predict_proba(X_pca)#[:,1]#[0:10]\n",
    "# y_pred_class = binarize(y_pred_proba, 0.2)[:,1]\n",
    "# print(y_pred_class[0:10])\n",
    "# print(confusion_matrix(y, y_pred_class))\n",
    "# print(classification_report(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(x='TransactionDT', y='TransactionDT', hue='isFraud', data=df_features)\n",
    "# plt.title(\"Transaction Date Versus Fraud\")\n",
    "# plt.show()\n",
    "\n",
    "# we want to figure out how to create more features from TransactionDT.. \n",
    "# are certain transactions more likely. in general, to be fraudualant around a certain day? \n",
    "# we have transactionID yet we dont have the specific card of who it belongs to.. \n",
    "# if card1 is the unique identifier... and we did a groupby on fraud.. \n",
    "# we have average spent per day as an option... \n",
    "# create a feature that is average spent per day for non fraud versus average spent per day for fraud.. \n",
    "# what would the describe method reveal for us? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
